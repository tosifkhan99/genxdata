# Testing and Development Workflow

GenXData follows comprehensive testing practices with unit tests, integration tests, and performance benchmarks.

## Testing Structure

### Test Organization
```
tests/
├── unit/                    # Unit tests for individual components
│   ├── test_strategies/     # Strategy-specific tests
│   ├── test_generators/     # Generator tests
│   ├── test_utils/         # Utility function tests
│   └── test_exceptions/    # Exception handling tests
├── integration/            # Integration tests
│   ├── test_orchestrator/  # End-to-end orchestrator tests
│   ├── test_api/          # API endpoint tests
│   └── test_cli/          # CLI command tests
├── performance/           # Performance benchmarks
│   ├── test_large_datasets/
│   └── test_streaming/
└── fixtures/              # Test data and configurations
    ├── configs/
    └── expected_outputs/
```

## Testing Patterns

### Strategy Testing
```python
# tests/unit/test_strategies/test_number_range_strategy.py
import pytest
import pandas as pd
from unittest.mock import MagicMock
from core.strategies.number_range_strategy import NumberRangeStrategy
from exceptions.param_exceptions import InvalidConfigParamException

class TestNumberRangeStrategy:
    def setup_method(self):
        self.logger = MagicMock()
        self.base_kwargs = {
            'logger': self.logger,
            'col_name': 'test_column',
            'rows': 100,
            'params': {'min': 1, 'max': 100}
        }

    def test_valid_parameters(self):
        strategy = NumberRangeStrategy(**self.base_kwargs)
        assert strategy.params['min'] == 1
        assert strategy.params['max'] == 100

    def test_missing_required_parameters(self):
        invalid_kwargs = self.base_kwargs.copy()
        invalid_kwargs['params'] = {'min': 1}  # Missing 'max'

        with pytest.raises(InvalidConfigParamException):
            NumberRangeStrategy(**invalid_kwargs)

    def test_data_generation(self):
        strategy = NumberRangeStrategy(**self.base_kwargs)
        data = strategy.generate_data(10)

        assert isinstance(data, pd.Series)
        assert len(data) == 10
        assert all(1 <= x <= 100 for x in data)

    def test_unique_constraint(self):
        kwargs = self.base_kwargs.copy()
        kwargs['unique'] = True
        kwargs['params'] = {'min': 1, 'max': 5}

        strategy = NumberRangeStrategy(**kwargs)
        data = strategy.generate_data(5)

        assert len(data.unique()) == 5

    def test_precision_parameter(self):
        kwargs = self.base_kwargs.copy()
        kwargs['params'] = {'min': 1, 'max': 10, 'precision': 2}

        strategy = NumberRangeStrategy(**kwargs)
        data = strategy.generate_data(10)

        # Check that all values have at most 2 decimal places
        for value in data:
            assert len(str(value).split('.')[-1]) <= 2
```

### Generator Testing
```python
# tests/unit/test_generators/test_generator_utils.py
import pytest
from utils.generator_utils import (
    load_all_generators,
    get_generator_info,
    validate_generator_config
)

class TestGeneratorUtils:
    def test_load_all_generators(self):
        generators = load_all_generators()
        assert len(generators) == 175
        assert 'FULL_NAME' in generators
        assert 'EMAIL_PATTERN' in generators

    def test_get_generator_info(self):
        info = get_generator_info('FULL_NAME')
        assert info is not None
        assert info['strategy'] == 'RANDOM_NAME_STRATEGY'
        assert 'params' in info

    def test_invalid_generator_name(self):
        info = get_generator_info('INVALID_GENERATOR')
        assert info is None

    def test_validate_generator_config(self):
        config = {
            'columns': {
                'name': {'generator': 'FULL_NAME'},
                'age': {'generator': 'PERSON_AGE'}
            }
        }

        # Should not raise exception
        validate_generator_config(config)

    def test_invalid_generator_config(self):
        config = {
            'columns': {
                'name': {'generator': 'INVALID_GENERATOR'}
            }
        }

        with pytest.raises(Exception):
            validate_generator_config(config)
```

### Integration Testing
```python
# tests/integration/test_orchestrator/test_full_pipeline.py
import pytest
import tempfile
import os
from core.orchestrator import DataOrchestrator

class TestFullPipeline:
    def test_simple_generation(self):
        config = {
            'name': 'Test Generation',
            'rows': 100,
            'columns': {
                'id': {
                    'strategy': 'SERIES_STRATEGY',
                    'params': {'start': 1, 'step': 1}
                },
                'name': {
                    'generator': 'FULL_NAME'
                }
            },
            'output': [
                {'format': 'csv', 'output_path': 'test_output.csv'}
            ]
        }

        orchestrator = DataOrchestrator(config)
        result = orchestrator.run()

        assert result is not None
        assert 'df' in result
        assert len(result['df']) == 100

    def test_dependent_columns(self):
        config = {
            'name': 'Dependent Columns Test',
            'rows': 50,
            'columns': {
                'first_name': {
                    'strategy': 'RANDOM_NAME_STRATEGY',
                    'params': {'name_type': 'first'}
                },
                'last_name': {
                    'strategy': 'RANDOM_NAME_STRATEGY',
                    'params': {'name_type': 'last'}
                },
                'email': {
                    'strategy': 'CONCAT_STRATEGY',
                    'params': {
                        'columns': ['first_name', 'last_name'],
                        'format': '{0}.{1}@example.com'
                    }
                }
            },
            'output': [
                {'format': 'json', 'output_path': 'dependent_test.json'}
            ]
        }

        orchestrator = DataOrchestrator(config)
        result = orchestrator.run()

        assert result is not None
        data = result['df']

        # Verify email format
        for row in data:
            email = row['email']
            first_name = row['first_name']
            last_name = row['last_name']
            expected_email = f"{first_name}.{last_name}@example.com"
            assert email == expected_email
```

### API Testing
```python
# tests/integration/test_api/test_endpoints.py
import pytest
from fastapi.testclient import TestClient
from api import app

client = TestClient(app)

class TestAPIEndpoints:
    def test_health_check(self):
        response = client.get("/ping")
        assert response.status_code == 200
        assert response.json() == {"status": "healthy"}

    def test_list_generators(self):
        response = client.get("/generators")
        assert response.status_code == 200
        data = response.json()
        assert len(data) == 175

    def test_filter_generators(self):
        response = client.get("/generators?filter=NAME")
        assert response.status_code == 200
        data = response.json()
        assert all('NAME' in gen['name'] for gen in data)

    def test_generate_data_endpoint(self):
        config = {
            "name": "API Test",
            "rows": 10,
            "columns": {
                "name": {"generator": "FULL_NAME"}
            },
            "output": [{"format": "json", "output_path": "api_test.json"}]
        }

        response = client.post("/generate", json=config)
        assert response.status_code == 200
        data = response.json()
        assert data["success"] is True
        assert len(data["data"]) == 10

    def test_invalid_config(self):
        invalid_config = {
            "name": "Invalid Test",
            "rows": 10,
            "columns": {
                "name": {"generator": "INVALID_GENERATOR"}
            },
            "output": [{"format": "json", "output_path": "invalid.json"}]
        }

        response = client.post("/generate", json=invalid_config)
        assert response.status_code == 400
```

### Performance Testing
```python
# tests/performance/test_large_datasets.py
import pytest
import time
from core.orchestrator import DataOrchestrator

class TestPerformance:
    @pytest.mark.slow
    def test_large_dataset_generation(self):
        config = {
            'name': 'Large Dataset Performance Test',
            'rows': 100000,
            'columns': {
                'id': {'generator': 'UUID'},
                'name': {'generator': 'FULL_NAME'},
                'email': {'generator': 'EMAIL_PATTERN'},
                'age': {'generator': 'PERSON_AGE'},
                'salary': {'generator': 'SALARY'}
            },
            'output': [
                {'format': 'csv', 'output_path': 'large_dataset.csv'}
            ]
        }

        start_time = time.time()
        orchestrator = DataOrchestrator(config, perf_report=True)
        result = orchestrator.run()
        end_time = time.time()

        assert result is not None
        assert len(result['df']) == 100000
        assert end_time - start_time < 60  # Should complete within 60 seconds

    def test_memory_usage(self):
        import psutil
        import os

        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss

        config = {
            'name': 'Memory Test',
            'rows': 50000,
            'columns': {
                'data': {'generator': 'RANDOM_TEXT'}
            },
            'output': [
                {'format': 'json', 'output_path': 'memory_test.json'}
            ]
        }

        orchestrator = DataOrchestrator(config)
        result = orchestrator.run()

        final_memory = process.memory_info().rss
        memory_increase = final_memory - initial_memory

        # Memory increase should be reasonable (less than 500MB)
        assert memory_increase < 500 * 1024 * 1024
```

## Development Workflow

### Pre-commit Hooks
```bash
# Install pre-commit hooks
poetry add --group dev pre-commit
pre-commit install

# .pre-commit-config.yaml
repos:
  - repo: https://github.com/psf/black
    rev: 22.3.0
    hooks:
      - id: black
        language_version: python3

  - repo: https://github.com/pycqa/flake8
    rev: 4.0.1
    hooks:
      - id: flake8

  - repo: https://github.com/pycqa/isort
    rev: 5.10.1
    hooks:
      - id: isort
```

### Test Commands
```bash
# Run all tests
pytest

# Run specific test categories
pytest tests/unit/                    # Unit tests only
pytest tests/integration/             # Integration tests only
pytest tests/performance/ -m slow     # Performance tests

# Run with coverage
pytest --cov=core --cov=utils --cov=exceptions

# Run with verbose output
pytest -v

# Run specific test file
pytest tests/unit/test_strategies/test_number_range_strategy.py

# Run tests matching pattern
pytest -k "test_generator"
```

### Continuous Integration
```yaml
# .github/workflows/test.yml
name: Test Suite
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        poetry install --with dev

    - name: Run tests
      run: |
        pytest --cov=core --cov=utils --cov=exceptions

    - name: Upload coverage
      uses: codecov/codecov-action@v3
```

## Test Data Management

### Fixtures
```python
# tests/fixtures/config_fixtures.py
import pytest

@pytest.fixture
def simple_config():
    return {
        'name': 'Simple Test Config',
        'rows': 10,
        'columns': {
            'id': {'generator': 'UUID'},
            'name': {'generator': 'FULL_NAME'}
        },
        'output': [
            {'format': 'json', 'output_path': 'simple_test.json'}
        ]
    }

@pytest.fixture
def complex_config():
    return {
        'name': 'Complex Test Config',
        'rows': 100,
        'columns': {
            'user_id': {
                'strategy': 'SERIES_STRATEGY',
                'params': {'start': 1, 'step': 1}
            },
            'age': {
                'strategy': 'DISTRIBUTED_NUMBER_RANGE_STRATEGY',
                'params': {
                    'distribution': [
                        {'range': [18, 30], 'weight': 0.4},
                        {'range': [31, 50], 'weight': 0.35},
                        {'range': [51, 70], 'weight': 0.25}
                    ]
                }
            }
        },
        'output': [
            {'format': 'csv', 'output_path': 'complex_test.csv'}
        ]
    }
```

### Mock Data
```python
# tests/fixtures/mock_data.py
MOCK_GENERATORS = {
    'TEST_GENERATOR': {
        'strategy': 'PATTERN_STRATEGY',
        'params': {'pattern': 'TEST-[0-9]{3}'}
    }
}

EXPECTED_OUTPUTS = {
    'simple_generation': [
        {'id': 1, 'name': 'John Doe'},
        {'id': 2, 'name': 'Jane Smith'}
    ]
}
```

## Best Practices

1. **Test Isolation**: Each test should be independent and not rely on others
2. **Comprehensive Coverage**: Test both success and failure scenarios
3. **Performance Benchmarks**: Include performance tests for critical paths
4. **Mock External Dependencies**: Use mocks for file I/O and external services
5. **Parameterized Tests**: Use pytest.mark.parametrize for testing multiple scenarios
6. **Clear Test Names**: Test names should describe what they're testing
7. **Setup/Teardown**: Use fixtures for common setup and cleanup
8. **Continuous Integration**: Run tests on every commit and pull request
description:
globs:
alwaysApply: false
---
