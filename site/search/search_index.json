{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GenXData Documentation","text":"<p>GenXData is a flexible, high\u2011performance synthetic data generator supporting batch and streaming modes, rich strategies, and pluggable writers.</p> <ul> <li>Install and run with Poetry</li> <li>Configure generation via YAML</li> <li>Extend with new strategies, processors, and writers</li> </ul> <p>Get started with the quickstarts below, or dive into the explanations and reference sections.</p> <p>Quick links:</p> <ul> <li>Quickstart (Batch)</li> <li>Quickstart (Streaming)</li> <li>Quickstart (Frontend) EOF</li> </ul>"},{"location":"explanations/architecture-mermaid/","title":"Architecture (Mermaid)","text":"<pre><code>%% GenXData - High-level Architecture Overview (Mermaid)\n%% This diagram shows the main runtime flow and module boundaries.\n\ngraph TD\n  C[\"Frontend / CLI / Tools\"] --&gt;|HTTP JSON| API[\"FastAPI app (api.py)\"]\n\n  API --&gt;|validate| VAL[\"utils.generator_utils.validate_generator_config\"]\n  API --&gt;|run()| ORCH[\"DataOrchestrator\"]\n\n  ORCH --&gt;|normal| NPROC[\"NormalConfigProcessor\"]\n  ORCH --&gt;|stream/batch| SPROC[\"StreamingConfigProcessor\"]\n  ORCH --&gt;|_create_writer()| WSEL{{\"Writer selection\"}}\n  WSEL --&gt;|file_writer| FFACT[\"FileWriterFactory\"]\n  WSEL --&gt;|stream| SWR[\"StreamWriter\"]\n  WSEL --&gt;|batch| BWR[\"BatchWriter\"]\n\n  NPROC --&gt;|process()| W[\"BaseWriter\"]\n  SPROC --&gt;|process()| BWR\n\n  NPROC --&gt; SF[\"StrategyFactory\"]\n  SPROC --&gt; SF\n  SF --&gt; SMAP[\"strategy_mapping (get_* functions)\"]\n  SF --&gt; BSTR[\"BaseStrategy + Mixins\"]\n  SF --&gt; BCFG[\"BaseConfig + Configs\"]\n\n  FFACT --&gt; BFW[\"BaseFileWriter\"]\n  BFW --&gt; CSV[\"CsvFileWriter\"]\n  BFW --&gt; JSON[\"JsonFileWriter\"]\n  BFW --&gt; XLSX[\"ExcelFileWriter\"]\n  BFW --&gt; PARQ[\"ParquetFileWriter\"]\n  BFW --&gt; FTHR[\"FeatherFileWriter\"]\n  BFW --&gt; HTML[\"HtmlFileWriter\"]\n  BFW --&gt; SQLITE[\"SqliteFileWriter\"]\n\n  SWR -.-&gt; QF[\"QueueFactory\"]\n  QF --&gt; QCFG[\"QueueConfig\"]\n  QCFG --&gt; AMQPCFG[\"AMQPConfig\"]\n  QCFG --&gt; KAFKACFG[\"KafkaConfig\"]\n  QF --&gt; QPROD[\"QueueProducer\"]\n  QPROD --&gt; AMQPPROD[\"AMQPProducer\"]\n  QPROD --&gt; KAFKAPROD[\"KafkaProducer\"]\n\n  CSV --&gt; OUT1[\"Files in output/ (CSV)\"]\n  JSON --&gt; OUT2[\"Files in output/ (JSON)\"]\n  XLSX --&gt; OUT3[\"Files in output/ (Excel)\"]\n  PARQ --&gt; OUT4[\"Files in output/ (Parquet)\"]\n  FTHR --&gt; OUT5[\"Files in output/ (Feather)\"]\n  HTML --&gt; OUT6[\"Files in output/ (HTML)\"]\n  SQLITE --&gt; OUT7[\"SQLite DB file\"]\n  SWR --&gt; QOUT[\"Message Queues (AMQP/Kafka)\"]\n\n  classDef mod fill:#eef,stroke:#88a,stroke-width:1px;\n  class API,ORCH,NPROC,SPROC,SF,FFACT,SWR,BWR,QF,QCFG,QPROD,SMAP,BSTR,BCFG,VAL mod;\n</code></pre>"},{"location":"explanations/architecture-overview/","title":"Architecture Overview","text":"<p>This page summarizes the core components and how GenXData operates end\u2011to\u2011end.</p>"},{"location":"explanations/architecture-overview/#core-components","title":"Core Components","text":"<ul> <li>Orchestrator (<code>core/orchestrator.py</code>): coordinates config loading, strategy execution, and writing.</li> <li>Strategies (<code>core/strategies/*.py</code>): data generation primitives.</li> <li>Processors (<code>core/processors/*.py</code>): config processing modes (normal vs streaming).</li> <li>Writers (<code>core/writers/*.py</code>): output sinks (CSV, Parquet, Kafka, AMQP, etc.).</li> <li>Messaging (<code>messaging/*</code>): Kafka/AMQP integration.</li> <li>CLI (<code>cli/main_cli.py</code>) and Python API (<code>api.py</code>).</li> </ul>"},{"location":"explanations/architecture-overview/#flow","title":"Flow","text":"<ol> <li>Load YAML config and validate against strategy configuration (<code>core/strategy_config.py</code>).</li> <li>Initialize orchestrator with selected mode (batch/streaming).</li> <li>Create appropriate processor and strategy instances.</li> <li>Generate records and write via configured writer(s).</li> <li>Handle errors with rich exception classes.</li> </ol> <p>Refer to diagrams in <code>dev-docs/</code> for class and sequence views. EOF</p>"},{"location":"explanations/class-diagram/","title":"Class Diagram (LLD)","text":"<pre><code>%% GenXData - Core Classes and Relationships (Mermaid)\n%% Class diagram of main modules: core, strategies, writers, messaging\n\nclassDiagram\n  %% Orchestrator and Processors\n  class DataOrchestrator {\n    - config: dict\n    - perf_report: bool\n    - stream: any\n    - batch: any\n    + run() dict\n    + _create_writer(config, stream_config) BaseWriter\n  }\n\n  class BaseConfigProcessor {\n    - config: dict\n    - writer: BaseWriter\n    - column_names: list\n    - rows: int\n    - configs: list\n    + validate_config() bool\n    + create_base_dataframe(size) DataFrame\n    + process_column_strategies(df, strategy_state, mode) DataFrame\n    + process() dict\n  }\n  class NormalConfigProcessor {\n    + process() dict\n  }\n  class StreamingConfigProcessor {\n    - batch_size: int\n    - chunk_size: int\n    - strategy_state: dict\n    + process() dict\n    + _process_chunk(chunk_size) DataFrame\n  }\n\n  DataOrchestrator --&gt; BaseConfigProcessor : uses\n  BaseConfigProcessor &lt;|-- NormalConfigProcessor\n  BaseConfigProcessor &lt;|-- StreamingConfigProcessor\n  DataOrchestrator --&gt; BaseWriter : creates\n\n  %% Strategy Factory and Mapping\n  class StrategyFactory {\n    + create_strategy(mode, name, **kwargs) BaseStrategy\n    + execute_strategy(strategy, mode) (DataFrame, dict)\n  }\n  class StrategyMapping {\n    + get_strategy_class(name)\n    + get_config_class(name)\n  }\n\n  BaseConfigProcessor --&gt; StrategyFactory : composes\n  StrategyFactory ..&gt; StrategyMapping : resolves\n  StrategyFactory --&gt; BaseStrategy : creates\n  StrategyFactory --&gt; BaseConfig : validates\n\n  %% Base Strategy and Mixins\n  class BaseStrategy {\n    - df: DataFrame\n    - col_name: str\n    - rows: int\n    - is_intermediate: bool\n    - params: dict\n    - strategy_state: dict\n    + generate_data(count) Series\n    + generate_chunk(count) Series\n    + apply_to_dataframe(df, column_name, mask) DataFrame\n    + reset_state()\n    + get_current_state() dict\n    + sync_state(result) dict\n  }\n  class SeedMixin\n  class StatefulMixin\n  class ValidationMixin\n\n  %% Representative Strategies\n  class NumberRangeStrategy\n  class DateGeneratorStrategy\n  class PatternStrategy\n  class SeriesStrategy\n  class DistributedChoiceStrategy\n  class TimeRangeStrategy\n  class DistributedTimeRangeStrategy\n  class DistributedNumberRangeStrategy\n  class DistributedDateRangeStrategy\n  class ReplacementStrategy\n  class ConcatStrategy\n  class RandomNameStrategy\n  class DeleteStrategy\n\n  BaseStrategy &lt;|-- NumberRangeStrategy\n  BaseStrategy &lt;|-- DateGeneratorStrategy\n  BaseStrategy &lt;|-- PatternStrategy\n  BaseStrategy &lt;|-- SeriesStrategy\n  BaseStrategy &lt;|-- DistributedChoiceStrategy\n  BaseStrategy &lt;|-- TimeRangeStrategy\n  BaseStrategy &lt;|-- DistributedTimeRangeStrategy\n  BaseStrategy &lt;|-- DistributedNumberRangeStrategy\n  BaseStrategy &lt;|-- DistributedDateRangeStrategy\n  BaseStrategy &lt;|-- ReplacementStrategy\n  BaseStrategy &lt;|-- ConcatStrategy\n  BaseStrategy &lt;|-- RandomNameStrategy\n  BaseStrategy &lt;|-- DeleteStrategy\n\n  %% Configs for Strategies\n  class BaseConfig {\n    + from_dict(d) BaseConfig\n    + to_dict() dict\n    + validate() void\n  }\n  class NumberRangeConfig\n  class DistributedNumberRangeConfig\n  class DateRangeConfig\n  class DistributedDateRangeConfig\n  class TimeRangeConfig\n  class DistributedTimeRangeConfig\n  class PatternConfig\n  class SeriesConfig\n  class DistributedChoiceConfig\n  class ReplacementConfig\n  class ConcatConfig\n  class DeleteConfig\n  class RandomNameConfig\n\n  BaseConfig &lt;|-- NumberRangeConfig\n  BaseConfig &lt;|-- DistributedNumberRangeConfig\n  BaseConfig &lt;|-- DateRangeConfig\n  BaseConfig &lt;|-- DistributedDateRangeConfig\n  BaseConfig &lt;|-- TimeRangeConfig\n  BaseConfig &lt;|-- DistributedTimeRangeConfig\n  BaseConfig &lt;|-- PatternConfig\n  BaseConfig &lt;|-- SeriesConfig\n  BaseConfig &lt;|-- DistributedChoiceConfig\n  BaseConfig &lt;|-- ReplacementConfig\n  BaseConfig &lt;|-- ConcatConfig\n  BaseConfig &lt;|-- DeleteConfig\n  BaseConfig &lt;|-- RandomNameConfig\n\n  %% Writers\n  class BaseWriter {\n    + write(df, metadata) dict\n    + finalize() dict\n    + validate_config() bool\n  }\n  class BaseFileWriter {\n    - params: dict\n    - base_output_path: str\n    - output_path: str\n    + get_expected_extensions() list\n    + get_default_params() dict\n    + write(df, metadata) dict\n    + finalize() dict\n  }\n  class CsvFileWriter\n  class JsonFileWriter\n  class ExcelFileWriter\n  class ParquetFileWriter\n  class FeatherFileWriter\n  class HtmlFileWriter\n  class SqliteFileWriter\n  class FileWriterFactory {\n    + create_writer(type, params) BaseFileWriter\n    + create_multiple_writers(configs) list\n  }\n  class StreamWriter {\n    - queue_producer\n    + write(df, metadata) dict\n    + finalize() dict\n  }\n  class BatchWriter {\n    - writer_implementation: BaseWriter\n    + write(df, metadata) dict\n    + finalize() dict\n  }\n\n  BaseWriter &lt;|-- BaseFileWriter\n  BaseWriter &lt;|-- StreamWriter\n  BaseWriter &lt;|-- BatchWriter\n  BaseFileWriter &lt;|-- CsvFileWriter\n  BaseFileWriter &lt;|-- JsonFileWriter\n  BaseFileWriter &lt;|-- ExcelFileWriter\n  BaseFileWriter &lt;|-- ParquetFileWriter\n  BaseFileWriter &lt;|-- FeatherFileWriter\n  BaseFileWriter &lt;|-- HtmlFileWriter\n  BaseFileWriter &lt;|-- SqliteFileWriter\n  FileWriterFactory ..&gt; BaseFileWriter : creates\n  BatchWriter o--&gt; BaseWriter : delegates\n\n  %% Messaging\n  class QueueConfig {\n    + validate_config() void\n    + queue_type str\n    + get_producer_config() dict\n  }\n  class AMQPConfig\n  class KafkaConfig\n  class QueueProducer {\n    + connect() void\n    + disconnect() void\n    + send_dataframe(df, batch_info) void\n    + send_message(message) void\n  }\n  class AMQPProducer\n  class KafkaProducer\n  class QueueFactory {\n    + create_config(type, data) QueueConfig\n    + create_producer(config) QueueProducer\n    + create_from_config(stream_config) QueueProducer\n  }\n\n  QueueConfig &lt;|-- AMQPConfig\n  QueueConfig &lt;|-- KafkaConfig\n  QueueProducer &lt;|-- AMQPProducer\n  QueueProducer &lt;|-- KafkaProducer\n  QueueFactory ..&gt; QueueConfig : creates\n  QueueFactory ..&gt; QueueProducer : creates\n  StreamWriter ..&gt; QueueFactory : uses\n  StreamWriter ..&gt; QueueProducer : sends\n\n  %% Associations\n  DataOrchestrator o--&gt; BaseWriter : owns\n  DataOrchestrator o--&gt; BaseConfigProcessor : owns\n  BaseStrategy ..&gt; SeedMixin\n  BaseStrategy ..&gt; StatefulMixin\n  BaseStrategy ..&gt; ValidationMixin\n</code></pre>"},{"location":"how-to/configs/write-batch-configs/","title":"Write Batch Configs","text":"<p>Learn how to author batch YAML configs, including generators, strategies, and writers.</p> <ul> <li>Use <code>examples/batch_configs/batch_example.yaml</code> as a starting point</li> <li>Validate fields against <code>core/strategy_config.py</code></li> <li>Prefer explicit types and defaults in YAML</li> </ul>"},{"location":"how-to/configs/write-stream-configs/","title":"Write Streaming Configs","text":"<p>Write streaming YAML configs that produce continuous data to messaging systems.</p> <ul> <li>Choose <code>mode: streaming</code></li> <li>Configure topics/queues and message formats</li> <li>Tune rate limits and ordering constraints</li> </ul>"},{"location":"how-to/deployment/docker/","title":"Docker Deployment","text":"<p>Run GenXData with Docker and Compose for reproducible environments.</p> <pre><code>docker compose up -d\n</code></pre> <ul> <li>Mount configs and <code>output/</code> as volumes</li> <li>Use environment variables for broker endpoints</li> </ul>"},{"location":"how-to/extending/new-processor/","title":"Add a New Processor","text":"<p>Steps:</p> <ol> <li>Implement a processor in <code>core/processors/</code></li> <li>Integrate with <code>core/orchestrator.py</code></li> <li>Document config parameters</li> <li>Add tests</li> </ol>"},{"location":"how-to/extending/new-strategy/","title":"Add a New Strategy","text":"<p>Steps:</p> <ol> <li>Create a new module in <code>core/strategies/</code></li> <li>Implement the required interface from <code>core/base_strategy.py</code></li> <li>Register it in <code>core/strategy_mapping.py</code></li> <li>Add tests under <code>tests/strategies/</code></li> </ol>"},{"location":"how-to/extending/new-writer/","title":"Add a New Writer","text":"<p>Steps:</p> <ol> <li>Create a writer in <code>core/writers/</code></li> <li>Implement the base interface (<code>core/writers/base_writer.py</code>)</li> <li>Update factory if applicable</li> <li>Add integration tests</li> </ol>"},{"location":"how-to/messaging/amqp/","title":"AMQP Messaging","text":"<p>Configure AMQP producers and verify messages.</p> <ul> <li>Provide host, port, user, vhost, and queue</li> <li>Use <code>tools/verify_queue_messages.py</code> and <code>tools/check_artemis_console.py</code></li> <li>See <code>messaging/amqp_producer.py</code> and <code>messaging/amqp_config.py</code></li> </ul>"},{"location":"how-to/messaging/kafka/","title":"Kafka Messaging","text":"<p>Configure Kafka producers and verify messages.</p> <ul> <li>Provide broker endpoints and topic</li> <li>Use <code>tools/check_queue_roundtrip.py</code> to validate</li> <li>See <code>messaging/kafka_producer.py</code> for integration details</li> </ul>"},{"location":"reference/cli/","title":"CLI Reference","text":"<p>Command-line usage and options for GenXData.</p>"},{"location":"reference/cli/#entry-points","title":"Entry points","text":"<ul> <li>Poetry script: </li> <li>Module: </li> </ul>"},{"location":"reference/cli/#global-options","title":"Global options","text":"<ul> <li>Set logging level. One of: , , , . Default: .</li> <li>Show help.</li> </ul>"},{"location":"reference/cli/#commands","title":"Commands","text":""},{"location":"reference/cli/#list-generators","title":"list-generators","text":"<p>List available generators.</p> <p>Options:</p> <ul> <li>Filter generators by name pattern</li> <li>Show strategy/domain stats summary</li> </ul> <p>Examples:</p>"},{"location":"reference/cli/#show-generator","title":"show-generator","text":"<p>Show details for a specific generator.</p> <p>Usage:</p>"},{"location":"reference/cli/#by-strategy","title":"by-strategy","text":"<p>List generators using a given strategy.</p> <p>Usage:</p>"},{"location":"reference/cli/#create-config","title":"create-config","text":"<p>Create a configuration from a generator mapping.</p> <p>Options:</p> <ul> <li>Mapping string like </li> <li>Path to JSON/YAML mapping file</li> <li>Output file path (required; .yaml/.yml or .json)</li> <li>Number of rows (default: 100)</li> <li>Config name</li> <li>Config description</li> </ul> <p>Examples:</p>"},{"location":"reference/cli/#create-domain-configs","title":"create-domain-configs","text":"<p>Generate example configurations for multiple domains into .</p>"},{"location":"reference/cli/#generate","title":"generate","text":"<p>Generate data from a configuration file.</p> <p>Usage:</p> <p>Options:</p> <ul> <li>Use streaming mode with a separate config</li> <li>Use batch file mode with a separate config</li> </ul> <p>Examples:</p>"},{"location":"reference/cli/#stats","title":"stats","text":"<p>Show generator statistics: counts per strategy/domain and available strategies.</p>"},{"location":"reference/cli/#notes","title":"Notes","text":"<ul> <li>Config format can be YAML or JSON. Output writer is chosen by config contents.</li> <li>See also:  for required/optional fields.</li> </ul>"},{"location":"reference/config-schema/","title":"Config Schema","text":"<p>This page describes the YAML fields, their types, and defaults.</p> <ul> <li>Backed by <code>core/strategy_config.py</code></li> <li>Examples pulled from <code>examples/</code></li> <li>(Automated schema generation to be added)</li> </ul>"},{"location":"reference/api/base_strategy/","title":"BaseStrategy","text":""},{"location":"reference/api/base_strategy/#core.base_strategy.BaseStrategy","title":"<code>core.base_strategy.BaseStrategy</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all data generation strategies. All strategies must implement the stateful generation pattern.</p> Source code in <code>core/base_strategy.py</code> <pre><code>class BaseStrategy(ABC):\n    \"\"\"\n    Base class for all data generation strategies.\n    All strategies must implement the stateful generation pattern.\n    \"\"\"\n\n    def __init__(self, mode: str, logger=None, **kwargs):\n        \"\"\"Initialize the strategy with configuration parameters\"\"\"\n        self.df = kwargs.get(\"df\")\n        self.col_name = kwargs.get(\"col_name\")\n        self.rows = kwargs.get(\"rows\", 100)\n        self.is_intermediate = kwargs.get(\"intermediate\", False)\n        self.params = kwargs.get(\"params\", {})\n        self.debug = kwargs.get(\"debug\", False)\n        self.unique = kwargs.get(\"unique\", False)\n        self.shuffle = kwargs.get(\"shuffle\", False)\n        # Ensure strategy_state is always a dictionary for stateful ops\n        self.strategy_state = kwargs.get(\"strategy_state\") or {}\n        self.mode = mode\n        # Standardized state key\n        self._state_key = f\"{self.__class__.__name__}:{self.col_name}\"\n        # Initialize mixin-expected attributes\n        self._seed = None\n\n        # Create strategy-specific logger\n        if logger is None:\n            strategy_name = self.__class__.__name__.lower().replace(\"strategy\", \"\")\n            logger_name = f\"strategies.{strategy_name}\"\n            self.logger = Logger.get_logger(logger_name)\n        else:\n            self.logger = logger\n\n        # Log strategy initialization\n        self.logger.debug(\n            f\"Initializing {self.__class__.__name__} for column '{self.col_name}'\"\n        )\n\n        # Warn if uniqueness is requested in streaming/batch mode (currently disabled)\n        if self.unique and self.is_streaming_and_batch():\n            self.logger.warning(\n                f\"Unique=True requested for column '{self.col_name}' in {self.__class__.__name__} \"\n                f\"but uniqueness is disabled in STREAM&amp;BATCH mode. It will be ignored.\"\n            )\n\n        # Validation is handled by config classes at factory level\n\n    # def _validate_params(self):\n    #     \"\"\"\n    #     Validate the parameters required by this strategy.\n    #     Raises InvalidConfigParamException if required parameters are missing or\n    #     invalid.\n    #     \"\"\"\n    #     return\n\n    def generate_data(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Default data generation flow used by most strategies.\n        Resets state in non-streaming mode, then delegates to generate_chunk().\n\n        Args:\n            count: Number of values to generate\n\n        Returns:\n            pd.Series: Generated values\n        \"\"\"\n        self.logger.debug(\n            f\"Generating {count} values using unified chunk-based approach\"\n        )\n\n        if not self.is_streaming_and_batch():\n            self.reset_state()\n\n        result = self.generate_chunk(count)\n        return result\n\n    @abstractmethod\n    def generate_chunk(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Generate a chunk of data maintaining internal state.\n        All strategies must implement this method for stateful generation.\n\n        Args:\n            count: Number of values to generate\n        Returns:\n            pd.Series: Generated values\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def reset_state(self):\n        \"\"\"\n        Reset the internal state to initial values.\n        All strategies must implement this method.\n        \"\"\"\n        pass\n\n    def get_current_state(self) -&gt; dict:\n        \"\"\"\n        Get current state information for debugging.\n\n        Returns a safe default if no state was recorded yet.\n\n        Returns:\n            dict: Current state information\n        \"\"\"\n        if self._state_key in self.strategy_state:\n            return self.strategy_state[self._state_key]\n        # Safe default state when not yet set\n        return {\n            \"strategy\": self.__class__.__name__,\n            \"stateful\": True,\n            \"column\": self.col_name,\n            \"seed\": getattr(self, \"_seed\", None),\n            \"dtype\": None,\n            \"step\": None,\n        }\n\n    def sync_state(self, result: pd.Series) -&gt; dict[str, Any]:\n        \"\"\"\n        Sync the state of the strategy.\n        This is used to update the state of the strategy with the result of the strategy.\n        Strategies may override this method to customize state sync.\n\n        Args:\n            result: Result of the strategy\n\n        Returns:\n            dict: Updated state information\n        \"\"\"\n        if self.is_streaming_and_batch():\n            state = self.strategy_state.get(self._state_key, {})\n            state.update(\n                {\n                    \"last_value\": result.iloc[-1]\n                    if len(result) &gt; 0\n                    else state.get(\"last_value\"),\n                    \"last_index\": result.index[-1]\n                    if len(result) &gt; 0\n                    else state.get(\"last_index\"),\n                    \"dtype\": str(result.dtype)\n                    if len(result) &gt; 0\n                    else state.get(\"dtype\"),\n                }\n            )\n\n            # Track uniqueness context if requested (NORMAL mode only)\n            if self.unique and not self.is_streaming_and_batch():\n                prev = state.get(\"unique_values\")\n                if prev is None:\n                    state[\"unique_values\"] = pd.Series(result).dropna()\n                else:\n                    # Concatenate and drop duplicates to maintain a running set\n                    state[\"unique_values\"] = (\n                        pd.concat([pd.Series(prev), pd.Series(result)])\n                        .drop_duplicates()\n                        .reset_index(drop=True)\n                    )\n\n            self.strategy_state[self._state_key] = state\n\n        return self.strategy_state\n\n    def is_streaming_and_batch(self) -&gt; bool:\n        \"\"\"\n        Check if this strategy supports streaming and batch generation.\n        \"\"\"\n        return self.mode == \"STREAM&amp;BATCH\"\n\n    def is_stateful(self) -&gt; bool:\n        \"\"\"\n        Check if this strategy supports stateful generation.\n        All strategies are now stateful by design.\n\n        Returns:\n            bool: Always True since all strategies implement stateful methods\n        \"\"\"\n        return True\n\n    def apply_to_dataframe(\n        self, df: pd.DataFrame, column_name: str, mask: str | None = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Apply strategy to dataframe with optional mask filtering.\n\n        Args:\n            df: Target dataframe\n            column_name: Column to populate\n            mask: Optional pandas query string for filtering rows\n\n        Returns:\n            Updated dataframe\n        \"\"\"\n        self.logger.debug(\n            f\"Applying {self.__class__.__name__} to column '{column_name}' \"\n            f\"with {len(df)} rows\"\n        )\n\n        # In-place mutation path to avoid full copies\n        df_copy = df  # alias for clarity; we mutate original\n\n        # Initialize column with NaN if it doesn't exist\n        if column_name not in df_copy.columns:\n            df_copy[column_name] = np.nan\n\n        if mask and mask.strip():\n            self.logger.debug(f\"Applying mask to column '{column_name}': {mask}\")\n            try:\n                filtered_df = df_copy.query(mask)\n                if len(filtered_df) &gt; 0:\n                    self.logger.debug(\n                        f\"Mask filtered {len(filtered_df)} rows out of \"\n                        f\"{len(df_copy)} total rows\"\n                    )\n                    # Generate data only for filtered rows\n                    values = self.generate_data(len(filtered_df))\n\n                    # Enforce uniqueness only in NORMAL mode (not STREAM&amp;BATCH)\n                    if self.unique and not self.is_streaming_and_batch():\n                        values = self._enforce_uniqueness(values)\n\n                    # Sync state with the result\n                    self.sync_state(values)\n\n                    # Ensure column has compatible dtype before assignment\n                    if (\n                        df_copy[column_name].dtype == \"float64\"\n                        and values.dtype == \"object\"\n                    ):\n                        df_copy[column_name] = df_copy[column_name].astype(\"object\")\n\n                    df_copy.loc[filtered_df.index, column_name] = values.values\n                else:\n                    self.logger.warning(\n                        f\"Mask '{mask}' matched no rows for column '{column_name}'\"\n                    )\n\n            except IndexingError as e:\n                self.logger.warning(\n                    f\"IndexError applying mask to column '{column_name}': {e}. \"\n                    f\"Applying to all rows as fallback.\"\n                )\n                # Fallback: apply to all rows\n                values = self.generate_data(len(df_copy))\n\n                # Sync state with the result\n                if self.unique and not self.is_streaming_and_batch():\n                    values = self._enforce_uniqueness(values)\n                self.sync_state(values)\n\n                # Ensure column has compatible dtype before assignment\n                if df_copy[column_name].dtype == \"float64\" and values.dtype == \"object\":\n                    df_copy[column_name] = df_copy[column_name].astype(\"object\")\n\n                df_copy[column_name] = values.values\n            except Exception as e:\n                self.logger.error(f\"Error applying mask to column '{column_name}': {e}\")\n                raise e\n        else:\n            # No mask: apply to all rows\n            self.logger.debug(f\"No mask specified, applying to all {len(df_copy)} rows\")\n            values = self.generate_data(len(df_copy))\n\n            # Sync state with the result\n            if self.unique and not self.is_streaming_and_batch():\n                values = self._enforce_uniqueness(values)\n            self.sync_state(values)\n\n            # Ensure column has compatible dtype before assignment\n            if df_copy[column_name].dtype == \"float64\" and values.dtype == \"object\":\n                df_copy[column_name] = df_copy[column_name].astype(\"object\")\n\n            df_copy[column_name] = values.values\n\n        self.logger.debug(\n            f\"Successfully applied {self.__class__.__name__} to column '{column_name}'\"\n        )\n        return df_copy\n\n    def _enforce_uniqueness(\n        self, values: pd.Series, max_attempts: int = 10\n    ) -&gt; pd.Series:\n        \"\"\"\n        Ensure the produced values are unique across chunks when unique=True.\n\n        Uses the recorded unique_values in strategy_state as the running set.\n\n        If duplicates are found against the running set or within the new chunk,\n        attempts to sample replacements up to max_attempts times. If still not\n        enough unique values can be produced, raises a ValueError.\n        \"\"\"\n        # Build current seen set from state and this batch\n        state = (\n            self.strategy_state.get(self._state_key, {})\n            if isinstance(self.strategy_state, dict)\n            else {}\n        )\n        prev_series = state.get(\"unique_values\")\n        seen: set[Any] = (\n            set(pd.Series(prev_series).dropna().tolist())\n            if prev_series is not None\n            else set()\n        )\n\n        unique_list: list[Any] = []\n        local_seen: set[Any] = set()\n\n        # First pass: accept non-duplicates\n        for v in values.tolist():\n            if v not in seen and v not in local_seen:\n                unique_list.append(v)\n                local_seen.add(v)\n\n        needed = len(values) - len(unique_list)\n        attempts = 0\n\n        # Try to fill remaining with additional samples\n        while needed &gt; 0 and attempts &lt; max_attempts:\n            attempts += 1\n            candidates = self._sample_more(needed * 2)\n            for v in candidates.tolist():\n                if v not in seen and v not in local_seen:\n                    unique_list.append(v)\n                    local_seen.add(v)\n                    if len(unique_list) == len(values):\n                        break\n            needed = len(values) - len(unique_list)\n\n        if needed &gt; 0:\n            raise ValueError(\n                f\"Unable to generate {needed} additional unique values for column '{self.col_name}'. \"\n                f\"Consider disabling unique or expanding the domain.\"\n            )\n\n        return pd.Series(unique_list)[: len(values)]\n\n    def _sample_more(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Sample more candidate values using the strategy's stateful generator.\n        Default behavior uses generate_chunk which advances state appropriately.\n        Strategies can override for more efficient sampling.\n        \"\"\"\n        return self.generate_chunk(count)\n\n    def validate_mask(self, df: pd.DataFrame, mask: str) -&gt; tuple[bool, str]:\n        \"\"\"\n        Validate if a mask can be executed against the dataframe.\n\n        Args:\n            df: Dataframe to test against\n            mask: Mask expression to validate\n\n        Returns:\n            Tuple of (is_valid, error_message)\n        \"\"\"\n        if not mask or not mask.strip():\n            return True, \"\"\n\n        try:\n            # Test the query on a small sample\n            test_df = df.head(1) if len(df) &gt; 0 else df\n            test_df.query(mask)\n            self.logger.debug(f\"Mask validation successful: {mask}\")\n            return True, \"\"\n        except Exception as e:\n            self.logger.debug(f\"Mask validation failed: {mask} - {str(e)}\")\n            return False, str(e)\n\n    def preview_mask_results(self, df: pd.DataFrame, mask: str) -&gt; dict:\n        \"\"\"\n        Preview how many rows would be affected by a mask.\n\n        Args:\n            df: Dataframe to test against\n            mask: Mask expression\n\n        Returns:\n            Dictionary with preview information\n        \"\"\"\n        if not mask or not mask.strip():\n            return {\n                \"total_rows\": len(df),\n                \"affected_rows\": len(df),\n                \"percentage\": 100.0,\n                \"mask_valid\": True,\n            }\n\n        try:\n            filtered_df = df.query(mask)\n            affected_rows = len(filtered_df)\n            total_rows = len(df)\n            percentage = (affected_rows / total_rows * 100) if total_rows &gt; 0 else 0\n\n            self.logger.debug(\n                f\"Mask preview: {affected_rows}/{total_rows} rows \"\n                f\"({percentage:.2f}%) would be affected\"\n            )\n\n            return {\n                \"total_rows\": total_rows,\n                \"affected_rows\": affected_rows,\n                \"percentage\": round(percentage, 2),\n                \"mask_valid\": True,\n                \"sample_affected_rows\": (\n                    filtered_df.head(3).to_dict(\"records\") if affected_rows &gt; 0 else []\n                ),\n            }\n        except Exception as e:\n            self.logger.debug(f\"Mask preview failed: {str(e)}\")\n            return {\n                \"total_rows\": len(df),\n                \"affected_rows\": 0,\n                \"percentage\": 0.0,\n                \"mask_valid\": False,\n                \"error\": str(e),\n            }\n</code></pre>"},{"location":"reference/api/base_strategy/#core.base_strategy.BaseStrategy.apply_to_dataframe","title":"<code>apply_to_dataframe(df, column_name, mask=None)</code>","text":"<p>Apply strategy to dataframe with optional mask filtering.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Target dataframe</p> required <code>column_name</code> <code>str</code> <p>Column to populate</p> required <code>mask</code> <code>str | None</code> <p>Optional pandas query string for filtering rows</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Updated dataframe</p> Source code in <code>core/base_strategy.py</code> <pre><code>def apply_to_dataframe(\n    self, df: pd.DataFrame, column_name: str, mask: str | None = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply strategy to dataframe with optional mask filtering.\n\n    Args:\n        df: Target dataframe\n        column_name: Column to populate\n        mask: Optional pandas query string for filtering rows\n\n    Returns:\n        Updated dataframe\n    \"\"\"\n    self.logger.debug(\n        f\"Applying {self.__class__.__name__} to column '{column_name}' \"\n        f\"with {len(df)} rows\"\n    )\n\n    # In-place mutation path to avoid full copies\n    df_copy = df  # alias for clarity; we mutate original\n\n    # Initialize column with NaN if it doesn't exist\n    if column_name not in df_copy.columns:\n        df_copy[column_name] = np.nan\n\n    if mask and mask.strip():\n        self.logger.debug(f\"Applying mask to column '{column_name}': {mask}\")\n        try:\n            filtered_df = df_copy.query(mask)\n            if len(filtered_df) &gt; 0:\n                self.logger.debug(\n                    f\"Mask filtered {len(filtered_df)} rows out of \"\n                    f\"{len(df_copy)} total rows\"\n                )\n                # Generate data only for filtered rows\n                values = self.generate_data(len(filtered_df))\n\n                # Enforce uniqueness only in NORMAL mode (not STREAM&amp;BATCH)\n                if self.unique and not self.is_streaming_and_batch():\n                    values = self._enforce_uniqueness(values)\n\n                # Sync state with the result\n                self.sync_state(values)\n\n                # Ensure column has compatible dtype before assignment\n                if (\n                    df_copy[column_name].dtype == \"float64\"\n                    and values.dtype == \"object\"\n                ):\n                    df_copy[column_name] = df_copy[column_name].astype(\"object\")\n\n                df_copy.loc[filtered_df.index, column_name] = values.values\n            else:\n                self.logger.warning(\n                    f\"Mask '{mask}' matched no rows for column '{column_name}'\"\n                )\n\n        except IndexingError as e:\n            self.logger.warning(\n                f\"IndexError applying mask to column '{column_name}': {e}. \"\n                f\"Applying to all rows as fallback.\"\n            )\n            # Fallback: apply to all rows\n            values = self.generate_data(len(df_copy))\n\n            # Sync state with the result\n            if self.unique and not self.is_streaming_and_batch():\n                values = self._enforce_uniqueness(values)\n            self.sync_state(values)\n\n            # Ensure column has compatible dtype before assignment\n            if df_copy[column_name].dtype == \"float64\" and values.dtype == \"object\":\n                df_copy[column_name] = df_copy[column_name].astype(\"object\")\n\n            df_copy[column_name] = values.values\n        except Exception as e:\n            self.logger.error(f\"Error applying mask to column '{column_name}': {e}\")\n            raise e\n    else:\n        # No mask: apply to all rows\n        self.logger.debug(f\"No mask specified, applying to all {len(df_copy)} rows\")\n        values = self.generate_data(len(df_copy))\n\n        # Sync state with the result\n        if self.unique and not self.is_streaming_and_batch():\n            values = self._enforce_uniqueness(values)\n        self.sync_state(values)\n\n        # Ensure column has compatible dtype before assignment\n        if df_copy[column_name].dtype == \"float64\" and values.dtype == \"object\":\n            df_copy[column_name] = df_copy[column_name].astype(\"object\")\n\n        df_copy[column_name] = values.values\n\n    self.logger.debug(\n        f\"Successfully applied {self.__class__.__name__} to column '{column_name}'\"\n    )\n    return df_copy\n</code></pre>"},{"location":"reference/api/base_strategy/#core.base_strategy.BaseStrategy.generate_chunk","title":"<code>generate_chunk(count)</code>  <code>abstractmethod</code>","text":"<p>Generate a chunk of data maintaining internal state. All strategies must implement this method for stateful generation.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of values to generate</p> required <p>Returns:     pd.Series: Generated values</p> Source code in <code>core/base_strategy.py</code> <pre><code>@abstractmethod\ndef generate_chunk(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Generate a chunk of data maintaining internal state.\n    All strategies must implement this method for stateful generation.\n\n    Args:\n        count: Number of values to generate\n    Returns:\n        pd.Series: Generated values\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/base_strategy/#core.base_strategy.BaseStrategy.generate_data","title":"<code>generate_data(count)</code>","text":"<p>Default data generation flow used by most strategies. Resets state in non-streaming mode, then delegates to generate_chunk().</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of values to generate</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Generated values</p> Source code in <code>core/base_strategy.py</code> <pre><code>def generate_data(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Default data generation flow used by most strategies.\n    Resets state in non-streaming mode, then delegates to generate_chunk().\n\n    Args:\n        count: Number of values to generate\n\n    Returns:\n        pd.Series: Generated values\n    \"\"\"\n    self.logger.debug(\n        f\"Generating {count} values using unified chunk-based approach\"\n    )\n\n    if not self.is_streaming_and_batch():\n        self.reset_state()\n\n    result = self.generate_chunk(count)\n    return result\n</code></pre>"},{"location":"reference/api/base_strategy/#core.base_strategy.BaseStrategy.get_current_state","title":"<code>get_current_state()</code>","text":"<p>Get current state information for debugging.</p> <p>Returns a safe default if no state was recorded yet.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Current state information</p> Source code in <code>core/base_strategy.py</code> <pre><code>def get_current_state(self) -&gt; dict:\n    \"\"\"\n    Get current state information for debugging.\n\n    Returns a safe default if no state was recorded yet.\n\n    Returns:\n        dict: Current state information\n    \"\"\"\n    if self._state_key in self.strategy_state:\n        return self.strategy_state[self._state_key]\n    # Safe default state when not yet set\n    return {\n        \"strategy\": self.__class__.__name__,\n        \"stateful\": True,\n        \"column\": self.col_name,\n        \"seed\": getattr(self, \"_seed\", None),\n        \"dtype\": None,\n        \"step\": None,\n    }\n</code></pre>"},{"location":"reference/api/base_strategy/#core.base_strategy.BaseStrategy.is_stateful","title":"<code>is_stateful()</code>","text":"<p>Check if this strategy supports stateful generation. All strategies are now stateful by design.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Always True since all strategies implement stateful methods</p> Source code in <code>core/base_strategy.py</code> <pre><code>def is_stateful(self) -&gt; bool:\n    \"\"\"\n    Check if this strategy supports stateful generation.\n    All strategies are now stateful by design.\n\n    Returns:\n        bool: Always True since all strategies implement stateful methods\n    \"\"\"\n    return True\n</code></pre>"},{"location":"reference/api/base_strategy/#core.base_strategy.BaseStrategy.is_streaming_and_batch","title":"<code>is_streaming_and_batch()</code>","text":"<p>Check if this strategy supports streaming and batch generation.</p> Source code in <code>core/base_strategy.py</code> <pre><code>def is_streaming_and_batch(self) -&gt; bool:\n    \"\"\"\n    Check if this strategy supports streaming and batch generation.\n    \"\"\"\n    return self.mode == \"STREAM&amp;BATCH\"\n</code></pre>"},{"location":"reference/api/base_strategy/#core.base_strategy.BaseStrategy.preview_mask_results","title":"<code>preview_mask_results(df, mask)</code>","text":"<p>Preview how many rows would be affected by a mask.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to test against</p> required <code>mask</code> <code>str</code> <p>Mask expression</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with preview information</p> Source code in <code>core/base_strategy.py</code> <pre><code>def preview_mask_results(self, df: pd.DataFrame, mask: str) -&gt; dict:\n    \"\"\"\n    Preview how many rows would be affected by a mask.\n\n    Args:\n        df: Dataframe to test against\n        mask: Mask expression\n\n    Returns:\n        Dictionary with preview information\n    \"\"\"\n    if not mask or not mask.strip():\n        return {\n            \"total_rows\": len(df),\n            \"affected_rows\": len(df),\n            \"percentage\": 100.0,\n            \"mask_valid\": True,\n        }\n\n    try:\n        filtered_df = df.query(mask)\n        affected_rows = len(filtered_df)\n        total_rows = len(df)\n        percentage = (affected_rows / total_rows * 100) if total_rows &gt; 0 else 0\n\n        self.logger.debug(\n            f\"Mask preview: {affected_rows}/{total_rows} rows \"\n            f\"({percentage:.2f}%) would be affected\"\n        )\n\n        return {\n            \"total_rows\": total_rows,\n            \"affected_rows\": affected_rows,\n            \"percentage\": round(percentage, 2),\n            \"mask_valid\": True,\n            \"sample_affected_rows\": (\n                filtered_df.head(3).to_dict(\"records\") if affected_rows &gt; 0 else []\n            ),\n        }\n    except Exception as e:\n        self.logger.debug(f\"Mask preview failed: {str(e)}\")\n        return {\n            \"total_rows\": len(df),\n            \"affected_rows\": 0,\n            \"percentage\": 0.0,\n            \"mask_valid\": False,\n            \"error\": str(e),\n        }\n</code></pre>"},{"location":"reference/api/base_strategy/#core.base_strategy.BaseStrategy.reset_state","title":"<code>reset_state()</code>  <code>abstractmethod</code>","text":"<p>Reset the internal state to initial values. All strategies must implement this method.</p> Source code in <code>core/base_strategy.py</code> <pre><code>@abstractmethod\ndef reset_state(self):\n    \"\"\"\n    Reset the internal state to initial values.\n    All strategies must implement this method.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/base_strategy/#core.base_strategy.BaseStrategy.sync_state","title":"<code>sync_state(result)</code>","text":"<p>Sync the state of the strategy. This is used to update the state of the strategy with the result of the strategy. Strategies may override this method to customize state sync.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>Series</code> <p>Result of the strategy</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Updated state information</p> Source code in <code>core/base_strategy.py</code> <pre><code>def sync_state(self, result: pd.Series) -&gt; dict[str, Any]:\n    \"\"\"\n    Sync the state of the strategy.\n    This is used to update the state of the strategy with the result of the strategy.\n    Strategies may override this method to customize state sync.\n\n    Args:\n        result: Result of the strategy\n\n    Returns:\n        dict: Updated state information\n    \"\"\"\n    if self.is_streaming_and_batch():\n        state = self.strategy_state.get(self._state_key, {})\n        state.update(\n            {\n                \"last_value\": result.iloc[-1]\n                if len(result) &gt; 0\n                else state.get(\"last_value\"),\n                \"last_index\": result.index[-1]\n                if len(result) &gt; 0\n                else state.get(\"last_index\"),\n                \"dtype\": str(result.dtype)\n                if len(result) &gt; 0\n                else state.get(\"dtype\"),\n            }\n        )\n\n        # Track uniqueness context if requested (NORMAL mode only)\n        if self.unique and not self.is_streaming_and_batch():\n            prev = state.get(\"unique_values\")\n            if prev is None:\n                state[\"unique_values\"] = pd.Series(result).dropna()\n            else:\n                # Concatenate and drop duplicates to maintain a running set\n                state[\"unique_values\"] = (\n                    pd.concat([pd.Series(prev), pd.Series(result)])\n                    .drop_duplicates()\n                    .reset_index(drop=True)\n                )\n\n        self.strategy_state[self._state_key] = state\n\n    return self.strategy_state\n</code></pre>"},{"location":"reference/api/base_strategy/#core.base_strategy.BaseStrategy.validate_mask","title":"<code>validate_mask(df, mask)</code>","text":"<p>Validate if a mask can be executed against the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to test against</p> required <code>mask</code> <code>str</code> <p>Mask expression to validate</p> required <p>Returns:</p> Type Description <code>tuple[bool, str]</code> <p>Tuple of (is_valid, error_message)</p> Source code in <code>core/base_strategy.py</code> <pre><code>def validate_mask(self, df: pd.DataFrame, mask: str) -&gt; tuple[bool, str]:\n    \"\"\"\n    Validate if a mask can be executed against the dataframe.\n\n    Args:\n        df: Dataframe to test against\n        mask: Mask expression to validate\n\n    Returns:\n        Tuple of (is_valid, error_message)\n    \"\"\"\n    if not mask or not mask.strip():\n        return True, \"\"\n\n    try:\n        # Test the query on a small sample\n        test_df = df.head(1) if len(df) &gt; 0 else df\n        test_df.query(mask)\n        self.logger.debug(f\"Mask validation successful: {mask}\")\n        return True, \"\"\n    except Exception as e:\n        self.logger.debug(f\"Mask validation failed: {mask} - {str(e)}\")\n        return False, str(e)\n</code></pre>"},{"location":"reference/api/orchestrator/","title":"DataOrchestrator","text":""},{"location":"reference/api/orchestrator/#core.orchestrator.DataOrchestrator","title":"<code>core.orchestrator.DataOrchestrator</code>","text":"<p>Orchestrates data generation across normal and streaming/batch modes.</p> <ul> <li>Normal mode uses <code>NormalConfigProcessor</code> and a specific file writer   created by <code>FileWriterFactory</code>.</li> <li>Streaming/Batch modes use <code>StreamingConfigProcessor</code> and a <code>BatchWriter</code>   or <code>StreamWriter</code> depending on the provided stream/batch config.</li> </ul> Source code in <code>core/orchestrator.py</code> <pre><code>class DataOrchestrator:\n    \"\"\"\n    Orchestrates data generation across normal and streaming/batch modes.\n\n    - Normal mode uses `NormalConfigProcessor` and a specific file writer\n      created by `FileWriterFactory`.\n    - Streaming/Batch modes use `StreamingConfigProcessor` and a `BatchWriter`\n      or `StreamWriter` depending on the provided stream/batch config.\n    \"\"\"\n\n    def __init__(\n        self,\n        config,\n        perf_report=SETTINGS.PERF_REPORT,\n        stream=None,\n        batch=None,\n        log_level=None,\n    ):\n        \"\"\"\n        Initialize the DataOrchestrator.\n\n        Args:\n            config (dict): Configuration dictionary\n            perf_report (bool): Whether to generate performance report\n            stream (str): Path to streaming config file\n            batch (str): Path to batch config file\n        \"\"\"\n        self.config = config\n        self.perf_report = perf_report\n        self.stream = stream\n        self.batch = batch\n        self.log_level = log_level\n        self.logger = Logger.get_logger(__name__, log_level)\n\n    def _create_writer(self, config: dict, stream_config: dict = None) -&gt; \"BaseWriter\":\n        \"\"\"\n        Create appropriate writer based on configuration.\n\n        Args:\n            config: Main configuration\n            stream_config: Optional streaming configuration\n\n        Returns:\n            Writer instance\n        \"\"\"\n        if stream_config:\n            # Check if this is batch processing (has batch_writer config)\n            if \"batch\" in stream_config:\n                self.logger.debug(\"Creating BatchWriter for batch processing\")\n                return BatchWriter(stream_config)\n            else:\n                # Create stream writer for streaming scenarios\n                self.logger.debug(\"Creating StreamWriter for streaming processing\")\n                return StreamWriter(stream_config)\n\n        else:\n            # Create specific file writer for normal processing using factory\n            self.logger.debug(\"Creating specific file writer for normal processing\")\n            factory = FileWriterFactory()\n            file_writer_config = config.get(\"file_writer\", {})\n            if isinstance(file_writer_config, list) and file_writer_config:\n                # Warn and normalize (should already be handled by validation, but safe here too)\n                self.logger.warning(\n                    \"'file_writer' as a list is deprecated; using the first item. Please migrate to a single dict.\"\n                )\n                file_writer_config = file_writer_config[0]\n\n            writer_type = file_writer_config.get(\"type\", \"csv\")\n            writer_params = file_writer_config.get(\"params\", {})\n\n            return factory.create_writer(writer_type, writer_params)\n\n    def run(self):\n        \"\"\"\n        Run the data generation process using the new processor architecture.\n\n        Returns:\n            dict: Processing results\n        \"\"\"\n        self.logger.info(\"Starting data generation with config\")\n        self.logger.info(\n            r\"\"\"\n  _____           __   _______        _\n / ____|          \\ \\ / /  __ \\      | |\n| |  __  ___ _ __  \\ V /| |  | | __ _| |_ __ _\n| | |_ |/ _ \\ '_ \\  &gt; &lt; | |  | |/ _` | __/ _` |\n| |__| |  __/ | | |/ . \\| |__| | (_| | || (_| |\n \\_____|\\___|_| |_/_/ \\_\\_____/ \\__,_|\\__\\__,_|\n\n\"\"\"\n        )\n\n        self.logger.debug(f\"Config: {self.config}\")\n        self.logger.debug(f\"Perf report: {self.perf_report}\")\n        self.logger.debug(f\"Stream: {self.stream}\")\n        self.logger.debug(f\"Batch: {self.batch}\")\n        self.logger.debug(f\"Log level: {self.log_level}\")\n\n        try:\n            # Validate running mode\n            if self.stream and self.batch:\n                raise InvalidRunningModeException()\n\n            # Determine processing mode and create appropriate processor\n            if self.stream or self.batch:\n                stream_config_path = self.stream or self.batch\n                stream_config = load_config(stream_config_path)\n\n                self.logger.info(\n                    f\"Processing {'streaming' if self.stream else 'batch'} config\"\n                )\n                self.logger.debug(f\"Stream/Batch config: {stream_config}\")\n\n                # Create writer\n                writer = self._create_writer(self.config, stream_config)\n\n                # Extract batch_size and chunk_size from the appropriate location\n                metadata_type = (\n                    stream_config.get(\"metadata\", {}).get(\"type\", \"\") or \"\"\n                ).lower()\n                if (\"batch\" in metadata_type) or (\"batch\" in stream_config):\n                    # For batch processing, get sizes from batch_writer config\n                    batch_config = stream_config[\"batch\"]\n                    batch_size = batch_config.get(\"batch_size\", 1000)\n                    chunk_size = batch_config.get(\"chunk_size\", batch_size)\n                else:\n                    # For streaming processing, infer from 'streaming' section if present\n                    streaming_section = stream_config.get(\"streaming\", {})\n                    batch_size = streaming_section.get(\n                        \"batch_size\", stream_config.get(\"batch_size\", 1000)\n                    )\n                    chunk_size = streaming_section.get(\n                        \"chunk_size\", stream_config.get(\"chunk_size\", batch_size)\n                    )\n\n                processor = StreamingConfigProcessor(\n                    config=self.config,\n                    writer=writer,\n                    batch_size=batch_size,\n                    chunk_size=chunk_size,\n                    perf_report=self.perf_report,\n                )\n\n            else:\n                # Normal processing\n                self.logger.info(\"Processing normal config\")\n\n                # Create writer and processor\n                writer = self._create_writer(self.config)\n                processor = NormalConfigProcessor(\n                    config=self.config,\n                    writer=writer,\n                    perf_report=self.perf_report,\n                )\n\n            return processor.process()\n\n        except Exception as e:\n            self.logger.error(f\"Error during processing: {str(e)}\")\n            return {\"status\": \"error\", \"error\": str(e), \"processor_type\": \"unknown\"}\n</code></pre>"},{"location":"reference/api/orchestrator/#core.orchestrator.DataOrchestrator.run","title":"<code>run()</code>","text":"<p>Run the data generation process using the new processor architecture.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>Processing results</p> Source code in <code>core/orchestrator.py</code> <pre><code>    def run(self):\n        \"\"\"\n        Run the data generation process using the new processor architecture.\n\n        Returns:\n            dict: Processing results\n        \"\"\"\n        self.logger.info(\"Starting data generation with config\")\n        self.logger.info(\n            r\"\"\"\n  _____           __   _______        _\n / ____|          \\ \\ / /  __ \\      | |\n| |  __  ___ _ __  \\ V /| |  | | __ _| |_ __ _\n| | |_ |/ _ \\ '_ \\  &gt; &lt; | |  | |/ _` | __/ _` |\n| |__| |  __/ | | |/ . \\| |__| | (_| | || (_| |\n \\_____|\\___|_| |_/_/ \\_\\_____/ \\__,_|\\__\\__,_|\n\n\"\"\"\n        )\n\n        self.logger.debug(f\"Config: {self.config}\")\n        self.logger.debug(f\"Perf report: {self.perf_report}\")\n        self.logger.debug(f\"Stream: {self.stream}\")\n        self.logger.debug(f\"Batch: {self.batch}\")\n        self.logger.debug(f\"Log level: {self.log_level}\")\n\n        try:\n            # Validate running mode\n            if self.stream and self.batch:\n                raise InvalidRunningModeException()\n\n            # Determine processing mode and create appropriate processor\n            if self.stream or self.batch:\n                stream_config_path = self.stream or self.batch\n                stream_config = load_config(stream_config_path)\n\n                self.logger.info(\n                    f\"Processing {'streaming' if self.stream else 'batch'} config\"\n                )\n                self.logger.debug(f\"Stream/Batch config: {stream_config}\")\n\n                # Create writer\n                writer = self._create_writer(self.config, stream_config)\n\n                # Extract batch_size and chunk_size from the appropriate location\n                metadata_type = (\n                    stream_config.get(\"metadata\", {}).get(\"type\", \"\") or \"\"\n                ).lower()\n                if (\"batch\" in metadata_type) or (\"batch\" in stream_config):\n                    # For batch processing, get sizes from batch_writer config\n                    batch_config = stream_config[\"batch\"]\n                    batch_size = batch_config.get(\"batch_size\", 1000)\n                    chunk_size = batch_config.get(\"chunk_size\", batch_size)\n                else:\n                    # For streaming processing, infer from 'streaming' section if present\n                    streaming_section = stream_config.get(\"streaming\", {})\n                    batch_size = streaming_section.get(\n                        \"batch_size\", stream_config.get(\"batch_size\", 1000)\n                    )\n                    chunk_size = streaming_section.get(\n                        \"chunk_size\", stream_config.get(\"chunk_size\", batch_size)\n                    )\n\n                processor = StreamingConfigProcessor(\n                    config=self.config,\n                    writer=writer,\n                    batch_size=batch_size,\n                    chunk_size=chunk_size,\n                    perf_report=self.perf_report,\n                )\n\n            else:\n                # Normal processing\n                self.logger.info(\"Processing normal config\")\n\n                # Create writer and processor\n                writer = self._create_writer(self.config)\n                processor = NormalConfigProcessor(\n                    config=self.config,\n                    writer=writer,\n                    perf_report=self.perf_report,\n                )\n\n            return processor.process()\n\n        except Exception as e:\n            self.logger.error(f\"Error during processing: {str(e)}\")\n            return {\"status\": \"error\", \"error\": str(e), \"processor_type\": \"unknown\"}\n</code></pre>"},{"location":"reference/api/strategy_config/","title":"Strategy Configs","text":""},{"location":"reference/api/strategy_config/#core.strategy_config","title":"<code>core.strategy_config</code>","text":"<p>Configuration classes for strategy parameters.</p> <p>Each strategy has its own configuration class that defines and validates the parameters required for that strategy.</p>"},{"location":"reference/api/strategy_config/#core.strategy_config.BaseConfig","title":"<code>BaseConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base configuration class for all strategies</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass(kw_only=True)\nclass BaseConfig(ABC):\n    \"\"\"Base configuration class for all strategies\"\"\"\n\n    mask: str = \"\"\n\n    @classmethod\n    def from_dict(cls, config_dict: dict[str, Any]) -&gt; \"BaseConfig\":\n        \"\"\"\n        Create a configuration instance from a dictionary.\n\n        Args:\n            config_dict: Dictionary containing configuration parameters\n\n        Returns:\n            An instance of the configuration class\n        \"\"\"\n        field_names = [f.name for f in fields(cls)]\n        filtered_dict = {k: v for k, v in config_dict.items() if k in field_names}\n\n        # Log unknown fields at debug level to avoid noisy warnings\n        try:\n            unknown_keys = [k for k in config_dict.keys() if k not in field_names]\n            if unknown_keys:\n                logger = Logger.get_logger(\"config\")\n                logger.debug(\n                    f\"Ignored unknown fields for {cls.__name__}: {unknown_keys}\"\n                )\n        except Exception:\n            # Do not fail config parsing due to logging issues\n            pass\n\n        return cls(**filtered_dict)\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Convert configuration to a dictionary.\n\n        Returns:\n            Dictionary representation of the configuration\n        \"\"\"\n        return {f.name: getattr(self, f.name) for f in fields(self)}\n\n    @abstractmethod\n    def validate(self) -&gt; None:\n        \"\"\"\n        Validate the configuration parameters.\n        Raises InvalidConfigParamException if validation fails.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.BaseConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Create a configuration instance from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>dict[str, Any]</code> <p>Dictionary containing configuration parameters</p> required <p>Returns:</p> Type Description <code>BaseConfig</code> <p>An instance of the configuration class</p> Source code in <code>core/strategy_config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: dict[str, Any]) -&gt; \"BaseConfig\":\n    \"\"\"\n    Create a configuration instance from a dictionary.\n\n    Args:\n        config_dict: Dictionary containing configuration parameters\n\n    Returns:\n        An instance of the configuration class\n    \"\"\"\n    field_names = [f.name for f in fields(cls)]\n    filtered_dict = {k: v for k, v in config_dict.items() if k in field_names}\n\n    # Log unknown fields at debug level to avoid noisy warnings\n    try:\n        unknown_keys = [k for k in config_dict.keys() if k not in field_names]\n        if unknown_keys:\n            logger = Logger.get_logger(\"config\")\n            logger.debug(\n                f\"Ignored unknown fields for {cls.__name__}: {unknown_keys}\"\n            )\n    except Exception:\n        # Do not fail config parsing due to logging issues\n        pass\n\n    return cls(**filtered_dict)\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.BaseConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert configuration to a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary representation of the configuration</p> Source code in <code>core/strategy_config.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Convert configuration to a dictionary.\n\n    Returns:\n        Dictionary representation of the configuration\n    \"\"\"\n    return {f.name: getattr(self, f.name) for f in fields(self)}\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.BaseConfig.validate","title":"<code>validate()</code>  <code>abstractmethod</code>","text":"<p>Validate the configuration parameters. Raises InvalidConfigParamException if validation fails.</p> Source code in <code>core/strategy_config.py</code> <pre><code>@abstractmethod\ndef validate(self) -&gt; None:\n    \"\"\"\n    Validate the configuration parameters.\n    Raises InvalidConfigParamException if validation fails.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.ChoiceItem","title":"<code>ChoiceItem</code>  <code>dataclass</code>","text":"<p>Choice with weight</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass ChoiceItem:\n    \"\"\"Choice with weight\"\"\"\n\n    value: Any\n    weight: int\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.ConcatConfig","title":"<code>ConcatConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for concatenation strategy</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass ConcatConfig(BaseConfig):\n    \"\"\"Configuration for concatenation strategy\"\"\"\n\n    lhs_col: str = \"\"\n    rhs_col: str = \"\"\n    separator: str = \"\"\n    suffix: str = \"\"\n    prefix: str = \"\"\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate concatenation parameters\"\"\"\n        if not self.lhs_col and not self.rhs_col:\n            raise InvalidConfigParamException(\n                \"At least one column must be specified for concatenation\"\n            )\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.ConcatConfig.validate","title":"<code>validate()</code>","text":"<p>Validate concatenation parameters</p> Source code in <code>core/strategy_config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate concatenation parameters\"\"\"\n    if not self.lhs_col and not self.rhs_col:\n        raise InvalidConfigParamException(\n            \"At least one column must be specified for concatenation\"\n        )\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.DateRangeConfig","title":"<code>DateRangeConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for date generator strategy</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass DateRangeConfig(BaseConfig):\n    \"\"\"Configuration for date generator strategy\"\"\"\n\n    start_date: str = \"2020-1-31\"\n    end_date: str = \"2020-12-31\"\n    format: str = \"%Y-%m-%d\"\n    output_format: str = \"%Y-%m-%d\"\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate date range parameters\"\"\"\n        from datetime import datetime\n\n        try:\n            start = datetime.strptime(self.start_date, self.format)\n            end = datetime.strptime(self.end_date, self.format)\n\n            if start &gt;= end:\n                raise InvalidConfigParamException(\n                    f\"Start date ({self.start_date}) must be before end date ({self.end_date})\"\n                )\n        except ValueError as e:\n            if \"unconverted data remains\" in str(e) or \"does not match format\" in str(\n                e\n            ):\n                raise InvalidConfigParamException(\n                    f\"Invalid date format. Expected {self.format}\"\n                ) from e\n            # Re-raise the original exception for other ValueError cases\n            raise\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.DateRangeConfig.validate","title":"<code>validate()</code>","text":"<p>Validate date range parameters</p> Source code in <code>core/strategy_config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate date range parameters\"\"\"\n    from datetime import datetime\n\n    try:\n        start = datetime.strptime(self.start_date, self.format)\n        end = datetime.strptime(self.end_date, self.format)\n\n        if start &gt;= end:\n            raise InvalidConfigParamException(\n                f\"Start date ({self.start_date}) must be before end date ({self.end_date})\"\n            )\n    except ValueError as e:\n        if \"unconverted data remains\" in str(e) or \"does not match format\" in str(\n            e\n        ):\n            raise InvalidConfigParamException(\n                f\"Invalid date format. Expected {self.format}\"\n            ) from e\n        # Re-raise the original exception for other ValueError cases\n        raise\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.DateRangeItem","title":"<code>DateRangeItem</code>  <code>dataclass</code>","text":"<p>Single date range definition with distribution weight</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass DateRangeItem:\n    \"\"\"Single date range definition with distribution weight\"\"\"\n\n    start_date: str = \"2020-01-01\"\n    end_date: str = \"2020-12-31\"\n    format: str = \"%Y-%m-%d\"\n    output_format: str | None = None\n    distribution: int | None = None\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate date range item\"\"\"\n        from datetime import datetime\n\n        try:\n            start_date = datetime.strptime(self.start_date, self.format)\n            end_date = datetime.strptime(self.end_date, self.format)\n\n            if start_date &gt;= end_date:\n                raise InvalidConfigParamException(\n                    f\"Start date ({self.start_date}) must be before end date ({self.end_date})\"\n                )\n        except ValueError as e:\n            if \"unconverted data remains\" in str(e) or \"does not match format\" in str(\n                e\n            ):\n                raise InvalidConfigParamException(\n                    f\"Invalid date format. Expected {self.format}\"\n                ) from e\n            # Re-raise the original exception for other ValueError cases\n            raise\n\n        # Require a non-empty output_format\n        if not isinstance(self.output_format, str) or not self.output_format.strip():\n            raise InvalidConfigParamException(\n                \"Output format must be provided and non-empty\"\n            )\n\n        if self.distribution is None:\n            raise InvalidConfigParamException(\n                \"Distribution weight (None) must be between 1 and 100\"\n            )\n        if not isinstance(self.distribution, int | float):\n            raise InvalidConfigParamException(\n                f\"Distribution weight ({self.distribution}) must be between 1 and 100\"\n            )\n        if self.distribution &lt;= 0 or self.distribution &gt; 100:\n            raise InvalidConfigParamException(\n                f\"Distribution weight ({self.distribution}) must be between 1 and 100\"\n            )\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.DateRangeItem.validate","title":"<code>validate()</code>","text":"<p>Validate date range item</p> Source code in <code>core/strategy_config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate date range item\"\"\"\n    from datetime import datetime\n\n    try:\n        start_date = datetime.strptime(self.start_date, self.format)\n        end_date = datetime.strptime(self.end_date, self.format)\n\n        if start_date &gt;= end_date:\n            raise InvalidConfigParamException(\n                f\"Start date ({self.start_date}) must be before end date ({self.end_date})\"\n            )\n    except ValueError as e:\n        if \"unconverted data remains\" in str(e) or \"does not match format\" in str(\n            e\n        ):\n            raise InvalidConfigParamException(\n                f\"Invalid date format. Expected {self.format}\"\n            ) from e\n        # Re-raise the original exception for other ValueError cases\n        raise\n\n    # Require a non-empty output_format\n    if not isinstance(self.output_format, str) or not self.output_format.strip():\n        raise InvalidConfigParamException(\n            \"Output format must be provided and non-empty\"\n        )\n\n    if self.distribution is None:\n        raise InvalidConfigParamException(\n            \"Distribution weight (None) must be between 1 and 100\"\n        )\n    if not isinstance(self.distribution, int | float):\n        raise InvalidConfigParamException(\n            f\"Distribution weight ({self.distribution}) must be between 1 and 100\"\n        )\n    if self.distribution &lt;= 0 or self.distribution &gt; 100:\n        raise InvalidConfigParamException(\n            f\"Distribution weight ({self.distribution}) must be between 1 and 100\"\n        )\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.DateSeriesConfig","title":"<code>DateSeriesConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for date series strategy</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass DateSeriesConfig(BaseConfig):\n    \"\"\"Configuration for date series strategy\"\"\"\n\n    start_date: str = \"2024-01-01\"\n    freq: str = \"D\"\n    format: str = \"%Y-%m-%d\"\n    output_format: str = \"%Y-%m-%d\"\n\n    def validate(self) -&gt; None:\n        from datetime import datetime\n\n        try:\n            start = datetime.strptime(self.start_date, self.format)\n        except ValueError as e:\n            raise InvalidConfigParamException(\n                \"Invalid date format for start_date\"\n            ) from e\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.DistributedChoiceConfig","title":"<code>DistributedChoiceConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for distributed choice strategy</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass DistributedChoiceConfig(BaseConfig):\n    \"\"\"Configuration for distributed choice strategy\"\"\"\n\n    choices: dict[str, int] = field(default_factory=dict)\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate distributed choice parameters\"\"\"\n        if not self.choices:\n            raise InvalidConfigParamException(\"At least one choice must be specified\")\n\n        if not isinstance(self.choices, dict):\n            raise InvalidConfigParamException(\n                \"Choices must be provided as a dictionary of value -&gt; weight\"\n            )\n\n        # Validate weights\n        total_weight = 0\n        for choice, weight in self.choices.items():\n            if not isinstance(weight, int | float):\n                raise InvalidConfigParamException(\n                    f\"Weight for choice '{choice}' must be numeric, got {type(weight).__name__}\"\n                )\n            if weight &lt;= 0:\n                raise InvalidConfigParamException(\n                    f\"Weight for choice '{choice}' must be positive, got {weight}\"\n                )\n            total_weight += weight\n\n        if total_weight != 100:\n            raise InvalidConfigParamException(\"Total weight must be 100\")\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.DistributedChoiceConfig.validate","title":"<code>validate()</code>","text":"<p>Validate distributed choice parameters</p> Source code in <code>core/strategy_config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate distributed choice parameters\"\"\"\n    if not self.choices:\n        raise InvalidConfigParamException(\"At least one choice must be specified\")\n\n    if not isinstance(self.choices, dict):\n        raise InvalidConfigParamException(\n            \"Choices must be provided as a dictionary of value -&gt; weight\"\n        )\n\n    # Validate weights\n    total_weight = 0\n    for choice, weight in self.choices.items():\n        if not isinstance(weight, int | float):\n            raise InvalidConfigParamException(\n                f\"Weight for choice '{choice}' must be numeric, got {type(weight).__name__}\"\n            )\n        if weight &lt;= 0:\n            raise InvalidConfigParamException(\n                f\"Weight for choice '{choice}' must be positive, got {weight}\"\n            )\n        total_weight += weight\n\n    if total_weight != 100:\n        raise InvalidConfigParamException(\"Total weight must be 100\")\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.DistributedDateRangeConfig","title":"<code>DistributedDateRangeConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for distributed date range strategy</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass DistributedDateRangeConfig(BaseConfig):\n    \"\"\"Configuration for distributed date range strategy\"\"\"\n\n    ranges: list[DateRangeItem] = field(default_factory=list)\n\n    @classmethod\n    def from_dict(cls, config_dict: dict[str, Any]) -&gt; \"DistributedDateRangeConfig\":\n        \"\"\"Create from dictionary with special handling for ranges\"\"\"\n        config = cls()\n        if \"ranges\" in config_dict:\n            for range_dict in config_dict[\"ranges\"]:\n                if isinstance(range_dict, DateRangeItem):\n                    config.ranges.append(range_dict)\n                elif isinstance(range_dict, dict):\n                    config.ranges.append(DateRangeItem(**range_dict))\n                else:\n                    raise InvalidConfigParamException(\n                        \"Each date range must be a dict or DateRangeItem\"\n                    )\n        return config\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate distributed date range parameters\"\"\"\n        if not self.ranges:\n            raise InvalidConfigParamException(\n                \"At least one date range must be specified\"\n            )\n\n        # Validate each range\n        for i, range_item in enumerate(self.ranges):\n            try:\n                range_item.validate()\n            except InvalidConfigParamException as e:\n                raise InvalidConfigParamException(\n                    f\"Invalid date range at index {i}: {str(e)}\"\n                ) from e\n\n        # Check that weights sum to 100\n        total_distribution = sum(r.distribution for r in self.ranges)\n        if total_distribution != 100:\n            raise InvalidConfigParamException(\n                f\"Distribution weights must sum to 100, got {total_distribution}\"\n            )\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.DistributedDateRangeConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Create from dictionary with special handling for ranges</p> Source code in <code>core/strategy_config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: dict[str, Any]) -&gt; \"DistributedDateRangeConfig\":\n    \"\"\"Create from dictionary with special handling for ranges\"\"\"\n    config = cls()\n    if \"ranges\" in config_dict:\n        for range_dict in config_dict[\"ranges\"]:\n            if isinstance(range_dict, DateRangeItem):\n                config.ranges.append(range_dict)\n            elif isinstance(range_dict, dict):\n                config.ranges.append(DateRangeItem(**range_dict))\n            else:\n                raise InvalidConfigParamException(\n                    \"Each date range must be a dict or DateRangeItem\"\n                )\n    return config\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.DistributedDateRangeConfig.validate","title":"<code>validate()</code>","text":"<p>Validate distributed date range parameters</p> Source code in <code>core/strategy_config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate distributed date range parameters\"\"\"\n    if not self.ranges:\n        raise InvalidConfigParamException(\n            \"At least one date range must be specified\"\n        )\n\n    # Validate each range\n    for i, range_item in enumerate(self.ranges):\n        try:\n            range_item.validate()\n        except InvalidConfigParamException as e:\n            raise InvalidConfigParamException(\n                f\"Invalid date range at index {i}: {str(e)}\"\n            ) from e\n\n    # Check that weights sum to 100\n    total_distribution = sum(r.distribution for r in self.ranges)\n    if total_distribution != 100:\n        raise InvalidConfigParamException(\n            f\"Distribution weights must sum to 100, got {total_distribution}\"\n        )\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.DistributedNumberRangeConfig","title":"<code>DistributedNumberRangeConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for distributed number range strategy</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass DistributedNumberRangeConfig(BaseConfig):\n    \"\"\"Configuration for distributed number range strategy\"\"\"\n\n    ranges: list[RangeItem] = field(default_factory=list)\n\n    @classmethod\n    def from_dict(cls, config_dict: dict[str, Any]) -&gt; \"DistributedNumberRangeConfig\":\n        \"\"\"Create from dictionary with special handling for ranges\"\"\"\n        config = cls()\n        if \"ranges\" in config_dict:\n            for range_dict in config_dict[\"ranges\"]:\n                if isinstance(range_dict, RangeItem):\n                    config.ranges.append(range_dict)\n                elif isinstance(range_dict, dict):\n                    config.ranges.append(RangeItem(**range_dict))\n                else:\n                    raise InvalidConfigParamException(\n                        \"Each range must be a dict or RangeItem\"\n                    )\n        return config\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate distributed number range parameters\"\"\"\n        if not self.ranges:\n            raise InvalidConfigParamException(\"At least one range must be specified\")\n\n        # Validate each range\n        for i, range_item in enumerate(self.ranges):\n            try:\n                range_item.validate()\n            except InvalidConfigParamException as e:\n                raise InvalidConfigParamException(\n                    f\"Invalid range at index {i}: {str(e)}\"\n                ) from e\n\n        # Check that weights sum to 100\n        total_distribution = sum(r.distribution for r in self.ranges)\n        if total_distribution != 100:\n            raise InvalidConfigParamException(\n                f\"Distribution weights must sum to 100, got {total_distribution}\"\n            )\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.DistributedNumberRangeConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Create from dictionary with special handling for ranges</p> Source code in <code>core/strategy_config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: dict[str, Any]) -&gt; \"DistributedNumberRangeConfig\":\n    \"\"\"Create from dictionary with special handling for ranges\"\"\"\n    config = cls()\n    if \"ranges\" in config_dict:\n        for range_dict in config_dict[\"ranges\"]:\n            if isinstance(range_dict, RangeItem):\n                config.ranges.append(range_dict)\n            elif isinstance(range_dict, dict):\n                config.ranges.append(RangeItem(**range_dict))\n            else:\n                raise InvalidConfigParamException(\n                    \"Each range must be a dict or RangeItem\"\n                )\n    return config\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.DistributedNumberRangeConfig.validate","title":"<code>validate()</code>","text":"<p>Validate distributed number range parameters</p> Source code in <code>core/strategy_config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate distributed number range parameters\"\"\"\n    if not self.ranges:\n        raise InvalidConfigParamException(\"At least one range must be specified\")\n\n    # Validate each range\n    for i, range_item in enumerate(self.ranges):\n        try:\n            range_item.validate()\n        except InvalidConfigParamException as e:\n            raise InvalidConfigParamException(\n                f\"Invalid range at index {i}: {str(e)}\"\n            ) from e\n\n    # Check that weights sum to 100\n    total_distribution = sum(r.distribution for r in self.ranges)\n    if total_distribution != 100:\n        raise InvalidConfigParamException(\n            f\"Distribution weights must sum to 100, got {total_distribution}\"\n        )\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.DistributedTimeRangeConfig","title":"<code>DistributedTimeRangeConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for distributed time range strategy</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass DistributedTimeRangeConfig(BaseConfig):\n    \"\"\"Configuration for distributed time range strategy\"\"\"\n\n    ranges: list[TimeRangeItem] = field(default_factory=list)\n    seed: int | None = None\n\n    @classmethod\n    def from_dict(cls, config_dict: dict[str, Any]) -&gt; \"DistributedTimeRangeConfig\":\n        \"\"\"Create from dictionary with special handling for ranges\"\"\"\n        config = cls()\n        if \"seed\" in config_dict:\n            config.seed = config_dict.get(\"seed\")\n        if \"ranges\" in config_dict:\n            for range_dict in config_dict[\"ranges\"]:\n                if isinstance(range_dict, TimeRangeItem):\n                    config.ranges.append(range_dict)\n                elif isinstance(range_dict, dict):\n                    config.ranges.append(TimeRangeItem(**range_dict))\n                else:\n                    raise InvalidConfigParamException(\n                        \"Each time range must be a dict or TimeRangeItem\"\n                    )\n        return config\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate distributed time range parameters\"\"\"\n        # Ensure ranges is a list and non-empty\n        if not isinstance(self.ranges, list) or len(self.ranges) == 0:\n            raise InvalidConfigParamException(\n                \"At least one time range must be specified\"\n            )\n\n        # Validate each range\n        for i, range_item in enumerate(self.ranges):\n            if not isinstance(range_item, TimeRangeItem):\n                raise InvalidConfigParamException(\n                    f\"Invalid time range at index {i}: expected TimeRangeItem, got {type(range_item).__name__}\"\n                )\n            try:\n                range_item.validate()\n            except InvalidConfigParamException as e:\n                raise InvalidConfigParamException(\n                    f\"Invalid time range at index {i}: {str(e)}\"\n                ) from e\n\n        # Check that weights sum to 100\n        total_distribution = sum(r.distribution for r in self.ranges)\n        if total_distribution != 100:\n            raise InvalidConfigParamException(\n                f\"Distribution weights must sum to 100, got {total_distribution}\"\n            )\n\n        # Validate seed if provided\n        if self.seed is not None:\n            try:\n                int(self.seed)\n            except (TypeError, ValueError) as e:\n                raise InvalidConfigParamException(\"Seed must be an integer\") from e\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.DistributedTimeRangeConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Create from dictionary with special handling for ranges</p> Source code in <code>core/strategy_config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: dict[str, Any]) -&gt; \"DistributedTimeRangeConfig\":\n    \"\"\"Create from dictionary with special handling for ranges\"\"\"\n    config = cls()\n    if \"seed\" in config_dict:\n        config.seed = config_dict.get(\"seed\")\n    if \"ranges\" in config_dict:\n        for range_dict in config_dict[\"ranges\"]:\n            if isinstance(range_dict, TimeRangeItem):\n                config.ranges.append(range_dict)\n            elif isinstance(range_dict, dict):\n                config.ranges.append(TimeRangeItem(**range_dict))\n            else:\n                raise InvalidConfigParamException(\n                    \"Each time range must be a dict or TimeRangeItem\"\n                )\n    return config\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.DistributedTimeRangeConfig.validate","title":"<code>validate()</code>","text":"<p>Validate distributed time range parameters</p> Source code in <code>core/strategy_config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate distributed time range parameters\"\"\"\n    # Ensure ranges is a list and non-empty\n    if not isinstance(self.ranges, list) or len(self.ranges) == 0:\n        raise InvalidConfigParamException(\n            \"At least one time range must be specified\"\n        )\n\n    # Validate each range\n    for i, range_item in enumerate(self.ranges):\n        if not isinstance(range_item, TimeRangeItem):\n            raise InvalidConfigParamException(\n                f\"Invalid time range at index {i}: expected TimeRangeItem, got {type(range_item).__name__}\"\n            )\n        try:\n            range_item.validate()\n        except InvalidConfigParamException as e:\n            raise InvalidConfigParamException(\n                f\"Invalid time range at index {i}: {str(e)}\"\n            ) from e\n\n    # Check that weights sum to 100\n    total_distribution = sum(r.distribution for r in self.ranges)\n    if total_distribution != 100:\n        raise InvalidConfigParamException(\n            f\"Distribution weights must sum to 100, got {total_distribution}\"\n        )\n\n    # Validate seed if provided\n    if self.seed is not None:\n        try:\n            int(self.seed)\n        except (TypeError, ValueError) as e:\n            raise InvalidConfigParamException(\"Seed must be an integer\") from e\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.MappingConfig","title":"<code>MappingConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for mapping strategy</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass MappingConfig(BaseConfig):\n    \"\"\"Configuration for mapping strategy\"\"\"\n\n    source: str = \"\"\n    source_column: str = \"\"\n    source_map_from: str = \"\"\n    map_from: str = \"\"\n    mapping: dict[str, str] = field(default_factory=dict)\n\n    def validate(self) -&gt; None:\n        # map_from is required\n        if not isinstance(self.map_from, str) or not self.map_from.strip():\n            raise InvalidConfigParamException(\"'map_from' must be provided\")\n\n        has_inline = isinstance(self.mapping, dict) and len(self.mapping) &gt; 0\n        has_file = bool(self.source) and bool(self.source_column)\n\n        if has_file:\n            if not os.path.isfile(self.source):\n                raise InvalidConfigParamException(\n                    f\"Source file '{self.source}' does not exist\"\n                )\n\n        if has_inline and has_file:\n            raise InvalidConfigParamException(\n                \"Provide either 'mapping' (inline) or ('source' and 'source_column'), not both\"\n            )\n\n        if has_inline:\n            return\n\n        if has_file:\n            return\n\n        # Neither inline nor file configuration provided\n        raise InvalidConfigParamException(\n            \"Provide 'mapping' dict or ('source' and 'source_column') for file mapping\"\n        )\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.PatternConfig","title":"<code>PatternConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for pattern strategy</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass PatternConfig(BaseConfig):\n    \"\"\"Configuration for pattern strategy\"\"\"\n\n    regex: str = r\"^[A-Za-z0-9]+$\"\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate pattern parameters\"\"\"\n        import re\n\n        try:\n            re.compile(self.regex)\n        except re.error as e:\n            raise InvalidConfigParamException(\n                f\"Invalid regular expression: {self.regex}\"\n            ) from e\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.PatternConfig.validate","title":"<code>validate()</code>","text":"<p>Validate pattern parameters</p> Source code in <code>core/strategy_config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate pattern parameters\"\"\"\n    import re\n\n    try:\n        re.compile(self.regex)\n    except re.error as e:\n        raise InvalidConfigParamException(\n            f\"Invalid regular expression: {self.regex}\"\n        ) from e\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.RandomNameConfig","title":"<code>RandomNameConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for random name generation strategy</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass RandomNameConfig(BaseConfig):\n    \"\"\"Configuration for random name generation strategy\"\"\"\n\n    name_type: str = \"first\"  # 'first', 'last', or 'full'\n    gender: str = \"any\"  # 'male', 'female', or 'any'\n    case: str = \"title\"  # 'title', 'upper', or 'lower'\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate random name parameters\"\"\"\n        valid_name_types = [\"first\", \"last\", \"full\"]\n        if self.name_type not in valid_name_types:\n            raise InvalidConfigParamException(\n                f\"Invalid name_type: {self.name_type}. Must be one of {valid_name_types}\"\n            )\n\n        valid_genders = [\"male\", \"female\", \"any\"]\n        if self.gender not in valid_genders:\n            raise InvalidConfigParamException(\n                f\"Invalid gender: {self.gender}. Must be one of {valid_genders}\"\n            )\n\n        valid_cases = [\"title\", \"upper\", \"lower\"]\n        if self.case not in valid_cases:\n            raise InvalidConfigParamException(\n                f\"Invalid case: {self.case}. Must be one of {valid_cases}\"\n            )\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.RandomNameConfig.validate","title":"<code>validate()</code>","text":"<p>Validate random name parameters</p> Source code in <code>core/strategy_config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate random name parameters\"\"\"\n    valid_name_types = [\"first\", \"last\", \"full\"]\n    if self.name_type not in valid_name_types:\n        raise InvalidConfigParamException(\n            f\"Invalid name_type: {self.name_type}. Must be one of {valid_name_types}\"\n        )\n\n    valid_genders = [\"male\", \"female\", \"any\"]\n    if self.gender not in valid_genders:\n        raise InvalidConfigParamException(\n            f\"Invalid gender: {self.gender}. Must be one of {valid_genders}\"\n        )\n\n    valid_cases = [\"title\", \"upper\", \"lower\"]\n    if self.case not in valid_cases:\n        raise InvalidConfigParamException(\n            f\"Invalid case: {self.case}. Must be one of {valid_cases}\"\n        )\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.RandomNumberRangeConfig","title":"<code>RandomNumberRangeConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for random number range strategy.</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass RandomNumberRangeConfig(BaseConfig):\n    \"\"\"Configuration for random number range strategy.\"\"\"\n\n    start: float = 0\n    end: float = 99\n    step: float = 1\n    precision: int = 0\n    unique: bool = False\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate number range parameters\"\"\"\n        if self.start &gt;= self.end:\n            raise InvalidConfigParamException(\n                f\"start ({self.start}) must be less than end ({self.end})\"\n            )\n        if not isinstance(self.start, (int | float)) or not isinstance(\n            self.end, (int | float)\n        ):\n            raise InvalidConfigParamException(\"Bounds must be numeric values\")\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.RandomNumberRangeConfig.validate","title":"<code>validate()</code>","text":"<p>Validate number range parameters</p> Source code in <code>core/strategy_config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate number range parameters\"\"\"\n    if self.start &gt;= self.end:\n        raise InvalidConfigParamException(\n            f\"start ({self.start}) must be less than end ({self.end})\"\n        )\n    if not isinstance(self.start, (int | float)) or not isinstance(\n        self.end, (int | float)\n    ):\n        raise InvalidConfigParamException(\"Bounds must be numeric values\")\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.RangeItem","title":"<code>RangeItem</code>  <code>dataclass</code>","text":"<p>Single range definition with distribution weight</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass RangeItem:\n    \"\"\"Single range definition with distribution weight\"\"\"\n\n    start: int = 10\n    end: int = 50\n    distribution: int | None = None\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate range item\"\"\"\n        if self.start &gt;= self.end:\n            raise InvalidConfigParamException(\n                f\"start ({self.start}) must be less than end ({self.end})\"\n            )\n        # Ensure distribution is provided and numeric\n        if not isinstance(self.distribution, int | float):\n            raise InvalidConfigParamException(\n                f\"Distribution weight ({self.distribution}) must be between 1 and 100\"\n            )\n        if self.distribution &lt;= 0 or self.distribution &gt; 100:\n            raise InvalidConfigParamException(\n                f\"Distribution weight ({self.distribution}) must be between 1 and 100\"\n            )\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.RangeItem.validate","title":"<code>validate()</code>","text":"<p>Validate range item</p> Source code in <code>core/strategy_config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate range item\"\"\"\n    if self.start &gt;= self.end:\n        raise InvalidConfigParamException(\n            f\"start ({self.start}) must be less than end ({self.end})\"\n        )\n    # Ensure distribution is provided and numeric\n    if not isinstance(self.distribution, int | float):\n        raise InvalidConfigParamException(\n            f\"Distribution weight ({self.distribution}) must be between 1 and 100\"\n        )\n    if self.distribution &lt;= 0 or self.distribution &gt; 100:\n        raise InvalidConfigParamException(\n            f\"Distribution weight ({self.distribution}) must be between 1 and 100\"\n        )\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.ReplacementConfig","title":"<code>ReplacementConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for replacement strategy</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass ReplacementConfig(BaseConfig):\n    \"\"\"Configuration for replacement strategy\"\"\"\n\n    from_value: Any = \"a\"\n    to_value: Any = \"b\"\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate replacement parameters\"\"\"\n        # No specific validation needed\n        pass\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.ReplacementConfig.validate","title":"<code>validate()</code>","text":"<p>Validate replacement parameters</p> Source code in <code>core/strategy_config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate replacement parameters\"\"\"\n    # No specific validation needed\n    pass\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.SeriesConfig","title":"<code>SeriesConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for series strategy</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass SeriesConfig(BaseConfig):\n    \"\"\"Configuration for series strategy\"\"\"\n\n    start: int = 1\n    step: int = 1\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate series parameters\"\"\"\n        if not isinstance(self.start, (int | float)) or not isinstance(\n            self.step, (int | float)\n        ):\n            raise InvalidConfigParamException(\"Start and step must be numeric values\")\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.SeriesConfig.validate","title":"<code>validate()</code>","text":"<p>Validate series parameters</p> Source code in <code>core/strategy_config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate series parameters\"\"\"\n    if not isinstance(self.start, (int | float)) or not isinstance(\n        self.step, (int | float)\n    ):\n        raise InvalidConfigParamException(\"Start and step must be numeric values\")\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.TimeRangeConfig","title":"<code>TimeRangeConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for time range strategy</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass TimeRangeConfig(BaseConfig):\n    \"\"\"Configuration for time range strategy\"\"\"\n\n    start_time: str = \"00:00:00\"\n    end_time: str = \"23:59:59\"\n    format: str = \"%H:%M:%S\"\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate time range parameters\"\"\"\n        from datetime import datetime\n\n        try:\n            start = datetime.strptime(self.start_time, self.format)\n            end = datetime.strptime(self.end_time, self.format)\n\n            if start &gt;= end:\n                # Allow overnight ranges when the start string is lexicographically after end\n                if self.start_time &gt; self.end_time:\n                    # Overnight range is valid\n                    pass\n                else:\n                    raise InvalidConfigParamException(\n                        f\"Start time ({self.start_time}) must be before end time ({self.end_time})\"\n                    )\n        except ValueError as e:\n            if \"unconverted data remains\" in str(e) or \"does not match format\" in str(\n                e\n            ):\n                raise InvalidConfigParamException(\n                    f\"Invalid time format. Expected {self.format}\"\n                ) from e\n            # Re-raise the original exception for other ValueError cases\n            raise\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.TimeRangeConfig.validate","title":"<code>validate()</code>","text":"<p>Validate time range parameters</p> Source code in <code>core/strategy_config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate time range parameters\"\"\"\n    from datetime import datetime\n\n    try:\n        start = datetime.strptime(self.start_time, self.format)\n        end = datetime.strptime(self.end_time, self.format)\n\n        if start &gt;= end:\n            # Allow overnight ranges when the start string is lexicographically after end\n            if self.start_time &gt; self.end_time:\n                # Overnight range is valid\n                pass\n            else:\n                raise InvalidConfigParamException(\n                    f\"Start time ({self.start_time}) must be before end time ({self.end_time})\"\n                )\n    except ValueError as e:\n        if \"unconverted data remains\" in str(e) or \"does not match format\" in str(\n            e\n        ):\n            raise InvalidConfigParamException(\n                f\"Invalid time format. Expected {self.format}\"\n            ) from e\n        # Re-raise the original exception for other ValueError cases\n        raise\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.TimeRangeItem","title":"<code>TimeRangeItem</code>  <code>dataclass</code>","text":"<p>Single time range definition with distribution weight</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass TimeRangeItem:\n    \"\"\"Single time range definition with distribution weight\"\"\"\n\n    start: str = \"00:00:00\"\n    end: str = \"23:59:59\"\n    format: str = \"%H:%M:%S\"\n    distribution: int | None = None\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate time range item\"\"\"\n        from datetime import datetime\n\n        try:\n            start_time = datetime.strptime(self.start, self.format)\n            end_time = datetime.strptime(self.end, self.format)\n\n            # Special handling for overnight time ranges (e.g., 22:00:00 to 06:00:00)\n            if start_time &gt;= end_time:\n                # Check if this could be an overnight range\n                if (\n                    self.start &gt; self.end\n                ):  # String comparison for times like \"22:00:00\" &gt; \"06:00:00\"\n                    # This is likely an overnight range, which is valid\n                    pass\n                else:\n                    raise InvalidConfigParamException(\n                        f\"Start time ({self.start}) must be before end time ({self.end})\"\n                    )\n        except ValueError as e:\n            if \"unconverted data remains\" in str(e) or \"does not match format\" in str(\n                e\n            ):\n                raise InvalidConfigParamException(\n                    f\"Invalid time format. Expected {self.format}\"\n                ) from e\n            # Re-raise the original exception for other ValueError cases\n            raise\n\n        if not isinstance(self.distribution, int | float):\n            raise InvalidConfigParamException(\n                f\"Distribution weight ({self.distribution}) must be between 1 and 100\"\n            )\n        if self.distribution &lt;= 0 or self.distribution &gt; 100:\n            raise InvalidConfigParamException(\n                f\"Distribution weight ({self.distribution}) must be between 1 and 100\"\n            )\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.TimeRangeItem.validate","title":"<code>validate()</code>","text":"<p>Validate time range item</p> Source code in <code>core/strategy_config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate time range item\"\"\"\n    from datetime import datetime\n\n    try:\n        start_time = datetime.strptime(self.start, self.format)\n        end_time = datetime.strptime(self.end, self.format)\n\n        # Special handling for overnight time ranges (e.g., 22:00:00 to 06:00:00)\n        if start_time &gt;= end_time:\n            # Check if this could be an overnight range\n            if (\n                self.start &gt; self.end\n            ):  # String comparison for times like \"22:00:00\" &gt; \"06:00:00\"\n                # This is likely an overnight range, which is valid\n                pass\n            else:\n                raise InvalidConfigParamException(\n                    f\"Start time ({self.start}) must be before end time ({self.end})\"\n                )\n    except ValueError as e:\n        if \"unconverted data remains\" in str(e) or \"does not match format\" in str(\n            e\n        ):\n            raise InvalidConfigParamException(\n                f\"Invalid time format. Expected {self.format}\"\n            ) from e\n        # Re-raise the original exception for other ValueError cases\n        raise\n\n    if not isinstance(self.distribution, int | float):\n        raise InvalidConfigParamException(\n            f\"Distribution weight ({self.distribution}) must be between 1 and 100\"\n        )\n    if self.distribution &lt;= 0 or self.distribution &gt; 100:\n        raise InvalidConfigParamException(\n            f\"Distribution weight ({self.distribution}) must be between 1 and 100\"\n        )\n</code></pre>"},{"location":"reference/api/strategy_config/#core.strategy_config.UuidConfig","title":"<code>UuidConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for UUID strategy</p> Source code in <code>core/strategy_config.py</code> <pre><code>@dataclass\nclass UuidConfig(BaseConfig):\n    \"\"\"Configuration for UUID strategy\"\"\"\n\n    hyphens: bool = True\n    uppercase: bool = False\n    prefix: str = \"\"\n    unique: bool = False\n    numbers_only: bool = False\n    version: int = 5\n\n    def validate(self) -&gt; None:\n        # All fields are simple types; nothing complex to validate\n        if not isinstance(self.hyphens, bool) or not isinstance(self.uppercase, bool):\n            raise InvalidConfigParamException(\n                \"'hyphens' and 'uppercase' must be booleans\"\n            )\n        if not isinstance(self.numbers_only, bool):\n            raise InvalidConfigParamException(\"'numbers_only' must be a boolean\")\n        if not isinstance(self.prefix, str):\n            raise InvalidConfigParamException(\"'prefix' must be a string\")\n\n        # Version must be 4 or 5\n        if self.version not in (4, 5):\n            raise InvalidConfigParamException(\"'version' must be 4 or 5\")\n</code></pre>"},{"location":"reference/api/strategy_factory/","title":"StrategyFactory","text":""},{"location":"reference/api/strategy_factory/#core.strategy_factory.StrategyFactory","title":"<code>core.strategy_factory.StrategyFactory</code>","text":"<p>Factory class for creating strategy instances.</p> <p>This class is responsible for creating strategy instances with validated configuration parameters.</p> Source code in <code>core/strategy_factory.py</code> <pre><code>class StrategyFactory:\n    \"\"\"\n    Factory class for creating strategy instances.\n\n    This class is responsible for creating strategy instances with validated\n    configuration parameters.\n    \"\"\"\n\n    def __init__(self, logger):\n        \"\"\"Initialize the factory\"\"\"\n        self.logger = logger\n        # Pool of reusable strategy instances across chunks\n        self._strategy_pool: dict[str, BaseStrategy] = {}\n\n    def create_strategy(self, mode: str, strategy_name: str, **kwargs) -&gt; BaseStrategy:\n        \"\"\"\n        Create a strategy instance.\n\n        Args:\n            strategy_name: Name of the strategy to create\n            **kwargs: Parameters for the strategy\n\n        Returns:\n            An instance of the strategy\n\n        Raises:\n            UnsupportedStrategyException: If the strategy cannot be created\n        \"\"\"\n\n        try:\n            # Get the strategy class from our mapping\n            strategy_class = get_strategy_class(strategy_name)\n\n            # Get the config class and create a config instance\n            config_class = get_config_class(strategy_name)\n\n            # Extract and validate params\n            params = kwargs.get(\"params\", {})\n            config = config_class.from_dict(params)\n\n            config.validate()\n\n            # Update kwargs with validated config, preserving extra keys (e.g., seed)\n            validated = config.to_dict()\n            # todo: check if this is needed\n            # Merge original params so non-config knobs like 'seed' survive\n            for k, v in params.items():\n                if k not in validated:\n                    validated[k] = v\n            kwargs[\"params\"] = validated\n\n            self.logger.debug(\n                f\"Strategy {strategy_name} created with params: {kwargs['params']}\"\n            )\n            # Create and return strategy instance\n            return strategy_class(mode=mode, logger=self.logger, **kwargs)\n\n        except Exception as e:\n            if self.logger:\n                self.logger.debug(f\"Error creating strategy {strategy_name}: {str(e)}\")\n            # Re-raise the original exception to preserve expected types in tests\n            raise\n\n    def get_or_create_strategy(\n        self, mode: str, strategy_name: str, pool_key: str, **kwargs\n    ) -&gt; BaseStrategy:\n        \"\"\"\n        Get a pooled strategy instance or create one if not exists.\n\n        When reusing an instance, update mutable fields: df, rows, params,\n        unique flag, and strategy_state reference.\n        \"\"\"\n        if pool_key in self._strategy_pool:\n            strategy = self._strategy_pool[pool_key]\n            # Refresh per-chunk context\n            strategy.df = kwargs.get(\"df\", strategy.df)\n            strategy.rows = kwargs.get(\"rows\", strategy.rows)\n            params = kwargs.get(\"params\")\n            if params is not None:\n                strategy.params = params\n            strategy.unique = kwargs.get(\"unique\", strategy.unique)\n            state = kwargs.get(\"strategy_state\")\n            if state is not None and state is not strategy.strategy_state:\n                strategy.strategy_state = state\n            return strategy\n\n        # Create and pool new instance\n        strategy = self.create_strategy(mode, strategy_name, **kwargs)\n        self._strategy_pool[pool_key] = strategy\n        return strategy\n\n    def execute_strategy(\n        self, strategy: BaseStrategy, mode: str\n    ) -&gt; tuple[pd.DataFrame, dict[str, Any]]:\n        \"\"\"\n        Execute a strategy by generating data and applying it to the dataframe.\n\n        Args:\n            strategy: The strategy instance to execute\n\n        Returns:\n            The updated dataframe\n        \"\"\"\n\n        # Get mask from params\n        mask = strategy.params.get(\"mask\")\n\n        # Use the new mask evaluation from base strategy\n        strategy.df = strategy.apply_to_dataframe(strategy.df, strategy.col_name, mask)\n\n        # Mark as intermediate if needed\n        if strategy.is_intermediate:\n            strategy.df = mark_as_intermediate(strategy.df, strategy.col_name)\n\n        # Ensure strategy state is propagated in streaming/batch scenarios\n        return strategy.df, strategy.strategy_state\n</code></pre>"},{"location":"reference/api/strategy_factory/#core.strategy_factory.StrategyFactory.create_strategy","title":"<code>create_strategy(mode, strategy_name, **kwargs)</code>","text":"<p>Create a strategy instance.</p> <p>Parameters:</p> Name Type Description Default <code>strategy_name</code> <code>str</code> <p>Name of the strategy to create</p> required <code>**kwargs</code> <p>Parameters for the strategy</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseStrategy</code> <p>An instance of the strategy</p> <p>Raises:</p> Type Description <code>UnsupportedStrategyException</code> <p>If the strategy cannot be created</p> Source code in <code>core/strategy_factory.py</code> <pre><code>def create_strategy(self, mode: str, strategy_name: str, **kwargs) -&gt; BaseStrategy:\n    \"\"\"\n    Create a strategy instance.\n\n    Args:\n        strategy_name: Name of the strategy to create\n        **kwargs: Parameters for the strategy\n\n    Returns:\n        An instance of the strategy\n\n    Raises:\n        UnsupportedStrategyException: If the strategy cannot be created\n    \"\"\"\n\n    try:\n        # Get the strategy class from our mapping\n        strategy_class = get_strategy_class(strategy_name)\n\n        # Get the config class and create a config instance\n        config_class = get_config_class(strategy_name)\n\n        # Extract and validate params\n        params = kwargs.get(\"params\", {})\n        config = config_class.from_dict(params)\n\n        config.validate()\n\n        # Update kwargs with validated config, preserving extra keys (e.g., seed)\n        validated = config.to_dict()\n        # todo: check if this is needed\n        # Merge original params so non-config knobs like 'seed' survive\n        for k, v in params.items():\n            if k not in validated:\n                validated[k] = v\n        kwargs[\"params\"] = validated\n\n        self.logger.debug(\n            f\"Strategy {strategy_name} created with params: {kwargs['params']}\"\n        )\n        # Create and return strategy instance\n        return strategy_class(mode=mode, logger=self.logger, **kwargs)\n\n    except Exception as e:\n        if self.logger:\n            self.logger.debug(f\"Error creating strategy {strategy_name}: {str(e)}\")\n        # Re-raise the original exception to preserve expected types in tests\n        raise\n</code></pre>"},{"location":"reference/api/strategy_factory/#core.strategy_factory.StrategyFactory.execute_strategy","title":"<code>execute_strategy(strategy, mode)</code>","text":"<p>Execute a strategy by generating data and applying it to the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>BaseStrategy</code> <p>The strategy instance to execute</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, dict[str, Any]]</code> <p>The updated dataframe</p> Source code in <code>core/strategy_factory.py</code> <pre><code>def execute_strategy(\n    self, strategy: BaseStrategy, mode: str\n) -&gt; tuple[pd.DataFrame, dict[str, Any]]:\n    \"\"\"\n    Execute a strategy by generating data and applying it to the dataframe.\n\n    Args:\n        strategy: The strategy instance to execute\n\n    Returns:\n        The updated dataframe\n    \"\"\"\n\n    # Get mask from params\n    mask = strategy.params.get(\"mask\")\n\n    # Use the new mask evaluation from base strategy\n    strategy.df = strategy.apply_to_dataframe(strategy.df, strategy.col_name, mask)\n\n    # Mark as intermediate if needed\n    if strategy.is_intermediate:\n        strategy.df = mark_as_intermediate(strategy.df, strategy.col_name)\n\n    # Ensure strategy state is propagated in streaming/batch scenarios\n    return strategy.df, strategy.strategy_state\n</code></pre>"},{"location":"reference/api/strategy_factory/#core.strategy_factory.StrategyFactory.get_or_create_strategy","title":"<code>get_or_create_strategy(mode, strategy_name, pool_key, **kwargs)</code>","text":"<p>Get a pooled strategy instance or create one if not exists.</p> <p>When reusing an instance, update mutable fields: df, rows, params, unique flag, and strategy_state reference.</p> Source code in <code>core/strategy_factory.py</code> <pre><code>def get_or_create_strategy(\n    self, mode: str, strategy_name: str, pool_key: str, **kwargs\n) -&gt; BaseStrategy:\n    \"\"\"\n    Get a pooled strategy instance or create one if not exists.\n\n    When reusing an instance, update mutable fields: df, rows, params,\n    unique flag, and strategy_state reference.\n    \"\"\"\n    if pool_key in self._strategy_pool:\n        strategy = self._strategy_pool[pool_key]\n        # Refresh per-chunk context\n        strategy.df = kwargs.get(\"df\", strategy.df)\n        strategy.rows = kwargs.get(\"rows\", strategy.rows)\n        params = kwargs.get(\"params\")\n        if params is not None:\n            strategy.params = params\n        strategy.unique = kwargs.get(\"unique\", strategy.unique)\n        state = kwargs.get(\"strategy_state\")\n        if state is not None and state is not strategy.strategy_state:\n            strategy.strategy_state = state\n        return strategy\n\n    # Create and pool new instance\n    strategy = self.create_strategy(mode, strategy_name, **kwargs)\n    self._strategy_pool[pool_key] = strategy\n    return strategy\n</code></pre>"},{"location":"reference/api/strategy_mapping/","title":"Strategy Mapping","text":""},{"location":"reference/api/strategy_mapping/#core.strategy_mapping","title":"<code>core.strategy_mapping</code>","text":"<p>Mapping between strategy names, strategy classes, and configuration classes.</p> <p>Provides helpers to list all strategies and to resolve a strategy or config class from its string identifier. Raises <code>UnsupportedStrategyException</code> for unknown names.</p>"},{"location":"reference/api/strategy_mapping/#core.strategy_mapping.get_config_class","title":"<code>get_config_class(strategy_name)</code>","text":"<p>Get the configuration class for the given strategy name.</p> <p>Parameters:</p> Name Type Description Default <code>strategy_name</code> <code>str</code> <p>Name of the strategy</p> required <p>Returns:</p> Type Description <code>type[BaseConfig]</code> <p>Configuration class</p> <p>Raises:</p> Type Description <code>UnsupportedStrategyException</code> <p>If the strategy is not supported</p> Source code in <code>core/strategy_mapping.py</code> <pre><code>def get_config_class(strategy_name: str) -&gt; type[BaseConfig]:\n    \"\"\"\n    Get the configuration class for the given strategy name.\n\n    Args:\n        strategy_name: Name of the strategy\n\n    Returns:\n        Configuration class\n\n    Raises:\n        UnsupportedStrategyException: If the strategy is not supported\n    \"\"\"\n    if strategy_name not in STRATEGY_MAP:\n        raise UnsupportedStrategyException(f\"Unsupported strategy: {strategy_name}\")\n\n    return STRATEGY_MAP[strategy_name][1]\n</code></pre>"},{"location":"reference/api/strategy_mapping/#core.strategy_mapping.get_strategy_class","title":"<code>get_strategy_class(strategy_name)</code>","text":"<p>Get the strategy class for the given strategy name.</p> <p>Parameters:</p> Name Type Description Default <code>strategy_name</code> <code>str</code> <p>Name of the strategy</p> required <p>Returns:</p> Type Description <code>type[BaseStrategy]</code> <p>Strategy class</p> <p>Raises:</p> Type Description <code>UnsupportedStrategyException</code> <p>If the strategy is not supported</p> Source code in <code>core/strategy_mapping.py</code> <pre><code>def get_strategy_class(strategy_name: str) -&gt; type[BaseStrategy]:\n    \"\"\"\n    Get the strategy class for the given strategy name.\n\n    Args:\n        strategy_name: Name of the strategy\n\n    Returns:\n        Strategy class\n\n    Raises:\n        UnsupportedStrategyException: If the strategy is not supported\n    \"\"\"\n    if strategy_name not in STRATEGY_MAP:\n        raise UnsupportedStrategyException(f\"Unsupported strategy: {strategy_name}\")\n\n    return STRATEGY_MAP[strategy_name][0]\n</code></pre>"},{"location":"reference/processors/","title":"Processors","text":"<ul> <li>base_config_processor</li> <li>normal_config_processor</li> <li>streaming_config_processor</li> </ul>"},{"location":"reference/processors/base_config_processor/","title":"base_config_processor","text":""},{"location":"reference/processors/base_config_processor/#core.processors.base_config_processor","title":"<code>core.processors.base_config_processor</code>","text":"<p>Abstract base config processor for GenXData.</p> <p>Provides shared flow for: - Validating generator configs - Creating a base DataFrame - Iterating over column strategies via <code>StrategyFactory</code> - Shuffling and filtering intermediate columns</p> <p>Concrete processors implement <code>process()</code> to orchestrate chunked vs full data generation and writing.</p>"},{"location":"reference/processors/base_config_processor/#core.processors.base_config_processor.BaseConfigProcessor","title":"<code>BaseConfigProcessor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all config processors in GenXData.</p> <p>Contains common functionality for data generation, strategy management, and configuration validation.</p> Source code in <code>core/processors/base_config_processor.py</code> <pre><code>class BaseConfigProcessor(ABC):\n    \"\"\"\n    Abstract base class for all config processors in GenXData.\n\n    Contains common functionality for data generation, strategy management,\n    and configuration validation.\n    \"\"\"\n\n    def __init__(self, config: dict[str, Any], writer: BaseWriter):\n        \"\"\"\n        Initialize the base config processor.\n\n        Args:\n            config: Configuration dictionary\n            writer: Writer instance for output\n        \"\"\"\n        self.config = config\n        self.writer = writer\n        self.logger = Logger.get_logger(self.__class__.__name__.lower())\n\n        # Initialize common configuration values\n        self.column_names = config[\"column_name\"]\n        self.rows = config[\"num_of_rows\"]\n        self.configs = config[\"configs\"]\n        self.shuffle_data = config.get(\"shuffle\", SETTINGS.SHUFFLE)\n\n        # Strict schema: do not allow legacy keys\n        for cfg in self.configs:\n            if \"column_names\" not in cfg:\n                raise InvalidConfigParamException(\n                    \"Each config entry must include 'column_names'\"\n                )\n\n        # Validate minimum rows\n        if self.rows &lt; SETTINGS.MINIMUM_ROWS_ALLOWED:\n            self.logger.warning(\n                f\"Requested rows ({self.rows}) below minimum allowed \"\n                f\"({SETTINGS.MINIMUM_ROWS_ALLOWED}). Using minimum.\"\n            )\n            self.rows = SETTINGS.MINIMUM_ROWS_ALLOWED\n\n        # Initialize strategy factory\n        self.strategy_factory = StrategyFactory(self.logger)\n\n        self.logger.debug(f\"BaseConfigProcessor initialized with {self.rows} rows\")\n\n    def validate_config(self) -&gt; bool:\n        \"\"\"\n        Validate the processor configuration.\n\n        Returns:\n            True if configuration is valid\n\n        Raises:\n            InvalidConfigParamException: If configuration is invalid\n        \"\"\"\n        try:\n            validate_generator_config(self.config)\n            self.logger.debug(\"Configuration validation passed\")\n            return True\n        except InvalidConfigParamException as e:\n            self.logger.error(f\"Configuration validation failed: {e}\")\n            raise\n\n    def create_base_dataframe(self, size: int = None) -&gt; pd.DataFrame:\n        \"\"\"\n        Create the base DataFrame with correct structure.\n\n        Args:\n            size: Number of rows to create (defaults to self.rows)\n\n        Returns:\n            Empty DataFrame with proper index and columns\n        \"\"\"\n        num_rows = size if size is not None else self.rows\n        df = pd.DataFrame(index=range(num_rows), columns=self.column_names)\n        self.logger.debug(\n            f\"Created base DataFrame with {num_rows} rows and \"\n            f\"columns: {self.column_names}\"\n        )\n        return df\n\n    def process_column_strategies(\n        self,\n        df: pd.DataFrame,\n        strategy_state: dict[str, Any] = None,\n        mode: str = \"NORMAL\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Process all column strategies and populate the DataFrame.\n\n        Args:\n            df: Base DataFrame to populate\n            strategy_state: State of the strategy\n        Returns:\n            DataFrame with generated data\n        \"\"\"\n        self.logger.info(f\"Processing {len(self.configs)} column configurations\")\n\n        # Ensure we keep a single shared state object across all columns\n        shared_state = strategy_state if strategy_state is not None else {}\n\n        with measure_time(\"data_generation\", rows_processed=len(df)):\n            for cur_config in self.configs:\n                df = self._process_single_config(df, cur_config, shared_state, mode)\n\n        return df\n\n    def _process_single_config(\n        self,\n        df: pd.DataFrame,\n        cur_config: dict[str, Any],\n        strategy_state: dict[str, Any],\n        mode: str,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Process a single column configuration.\n\n        Args:\n            df: DataFrame to update\n            cur_config: Configuration for current columns\n            strategy_state: State of the strategy\n        Returns:\n            Updated DataFrame\n        \"\"\"\n        self.logger.debug(\n            f\"Processing config: {cur_config.get('column_names', 'unknown')}\"\n        )\n\n        # Use strict key 'column_names'\n        target_names = cur_config.get(\"column_names\")\n        for col_name in target_names:\n            self.logger.debug(f\"Processing column: {col_name}\")\n\n            if cur_config.get(\"disabled\", False):\n                self.logger.info(f\"Column {col_name} is disabled, skipping\")\n                continue\n\n            # Check if this is an intermediate column\n            is_intermediate = cur_config.get(\"intermediate\", False)\n            self.logger.debug(f\"Column {col_name} - Is intermediate: {is_intermediate}\")\n\n            strategy_name = cur_config[\"strategy\"][\"name\"]\n            self.logger.debug(f\"Column {col_name} - Strategy: {strategy_name}\")\n\n            try:\n                df = self._execute_column_strategy(\n                    df, col_name, cur_config, is_intermediate, strategy_state, mode\n                )\n            except Exception as e:\n                self.logger.error(f\"Column {col_name} - Error: {e}\")\n                raise\n\n        return df\n\n    def _execute_column_strategy(\n        self,\n        df: pd.DataFrame,\n        col_name: str,\n        cur_config: dict[str, Any],\n        is_intermediate: bool,\n        strategy_state: dict[str, Any],\n        mode: str,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Execute strategy for a single column.\n\n        Args:\n            df: DataFrame to update\n            col_name: Name of the column to process\n            cur_config: Configuration for the column\n            is_intermediate: Whether this is an intermediate column\n            strategy_state: State of the strategy\n        Returns:\n            Updated DataFrame\n        \"\"\"\n        strategy_name = cur_config[\"strategy\"][\"name\"]\n\n        with measure_time(\n            f\"strategy.{strategy_name}.{col_name}\", rows_processed=len(df)\n        ):\n            # Prepare strategy parameters\n            strategy_params = cur_config.get(\"strategy\", {}).get(\"params\", {})\n            self.logger.debug(f\"Column {col_name} - Strategy params: {strategy_params}\")\n\n            # Add mask to strategy params if it exists\n            if \"mask\" in cur_config:\n                strategy_params[\"mask\"] = cur_config[\"mask\"]\n                self.logger.debug(\n                    f\"Column {col_name} - Added mask: {cur_config['mask']}\"\n                )\n\n            params = {\n                \"df\": df,\n                \"col_name\": col_name,\n                # Provide current frame size so strategies are chunk-aware\n                \"rows\": len(df),\n                \"intermediate\": is_intermediate,\n                \"params\": strategy_params,\n                \"unique\": cur_config.get(\"strategy\", {}).get(\"unique\", False),\n                # Pass the shared state reference even if it's empty\n                \"strategy_state\": strategy_state if strategy_state is not None else {},\n            }\n\n            # Create or reuse a strategy instance from the factory pool\n            pool_key = f\"{strategy_name}:{col_name}\"\n            strategy = self.strategy_factory.get_or_create_strategy(\n                mode, strategy_name, pool_key, **params\n            )\n            self.logger.debug(\n                f\"Column {col_name} - Strategy created: {strategy.__class__.__name__}\"\n            )\n\n            df, new_state = self.strategy_factory.execute_strategy(strategy, mode)\n            # Ensure the passed-in state dictionary is updated in-place so callers retain state\n            if isinstance(new_state, dict) and new_state is not strategy_state:\n                strategy_state.update(new_state)\n            self.logger.debug(\n                f\"Column {col_name} - Strategy executed. Sample: \"\n                f\"{df[col_name].head(3).tolist() if col_name in df.columns else 'N/A'}\"\n            )\n\n        return df\n\n    def apply_shuffle(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Apply shuffle to the DataFrame if configured.\n\n        Args:\n            df: DataFrame to shuffle\n\n        Returns:\n            Shuffled DataFrame (or original if shuffle disabled)\n        \"\"\"\n        if self.shuffle_data:\n            self.logger.debug(\"Shuffling data\")\n            with measure_time(\"shuffle_data\", rows_processed=len(df)):\n                original_df = df\n                df = df.sample(frac=1).reset_index(drop=True)\n                # Preserve intermediate column metadata across shuffle via attrs\n                if \"intermediate_columns\" in original_df.attrs:\n                    df.attrs[\"intermediate_columns\"] = original_df.attrs[\n                        \"intermediate_columns\"\n                    ]\n            self.logger.debug(\"Data shuffling completed\")\n\n        return df\n\n    def filter_intermediate_columns(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Filter out intermediate columns from the final DataFrame.\n\n        Args:\n            df: DataFrame to filter\n\n        Returns:\n            DataFrame with intermediate columns removed\n        \"\"\"\n        self.logger.debug(\"Filtering intermediate columns\")\n\n        with measure_time(\"filter_intermediate_columns\"):\n            df = filter_intermediate_columns(df)\n\n        self.logger.debug(\n            f\"Intermediate columns filtered. Final columns: {list(df.columns)}\"\n        )\n        return df\n\n    @abstractmethod\n    def process(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Process the configuration and generate data.\n\n        This method must be implemented by concrete processor classes.\n\n        Returns:\n            Dictionary with processing results\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/processors/base_config_processor/#core.processors.base_config_processor.BaseConfigProcessor.apply_shuffle","title":"<code>apply_shuffle(df)</code>","text":"<p>Apply shuffle to the DataFrame if configured.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to shuffle</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Shuffled DataFrame (or original if shuffle disabled)</p> Source code in <code>core/processors/base_config_processor.py</code> <pre><code>def apply_shuffle(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply shuffle to the DataFrame if configured.\n\n    Args:\n        df: DataFrame to shuffle\n\n    Returns:\n        Shuffled DataFrame (or original if shuffle disabled)\n    \"\"\"\n    if self.shuffle_data:\n        self.logger.debug(\"Shuffling data\")\n        with measure_time(\"shuffle_data\", rows_processed=len(df)):\n            original_df = df\n            df = df.sample(frac=1).reset_index(drop=True)\n            # Preserve intermediate column metadata across shuffle via attrs\n            if \"intermediate_columns\" in original_df.attrs:\n                df.attrs[\"intermediate_columns\"] = original_df.attrs[\n                    \"intermediate_columns\"\n                ]\n        self.logger.debug(\"Data shuffling completed\")\n\n    return df\n</code></pre>"},{"location":"reference/processors/base_config_processor/#core.processors.base_config_processor.BaseConfigProcessor.create_base_dataframe","title":"<code>create_base_dataframe(size=None)</code>","text":"<p>Create the base DataFrame with correct structure.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Number of rows to create (defaults to self.rows)</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Empty DataFrame with proper index and columns</p> Source code in <code>core/processors/base_config_processor.py</code> <pre><code>def create_base_dataframe(self, size: int = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Create the base DataFrame with correct structure.\n\n    Args:\n        size: Number of rows to create (defaults to self.rows)\n\n    Returns:\n        Empty DataFrame with proper index and columns\n    \"\"\"\n    num_rows = size if size is not None else self.rows\n    df = pd.DataFrame(index=range(num_rows), columns=self.column_names)\n    self.logger.debug(\n        f\"Created base DataFrame with {num_rows} rows and \"\n        f\"columns: {self.column_names}\"\n    )\n    return df\n</code></pre>"},{"location":"reference/processors/base_config_processor/#core.processors.base_config_processor.BaseConfigProcessor.filter_intermediate_columns","title":"<code>filter_intermediate_columns(df)</code>","text":"<p>Filter out intermediate columns from the final DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to filter</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with intermediate columns removed</p> Source code in <code>core/processors/base_config_processor.py</code> <pre><code>def filter_intermediate_columns(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter out intermediate columns from the final DataFrame.\n\n    Args:\n        df: DataFrame to filter\n\n    Returns:\n        DataFrame with intermediate columns removed\n    \"\"\"\n    self.logger.debug(\"Filtering intermediate columns\")\n\n    with measure_time(\"filter_intermediate_columns\"):\n        df = filter_intermediate_columns(df)\n\n    self.logger.debug(\n        f\"Intermediate columns filtered. Final columns: {list(df.columns)}\"\n    )\n    return df\n</code></pre>"},{"location":"reference/processors/base_config_processor/#core.processors.base_config_processor.BaseConfigProcessor.process","title":"<code>process()</code>  <code>abstractmethod</code>","text":"<p>Process the configuration and generate data.</p> <p>This method must be implemented by concrete processor classes.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with processing results</p> Source code in <code>core/processors/base_config_processor.py</code> <pre><code>@abstractmethod\ndef process(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Process the configuration and generate data.\n\n    This method must be implemented by concrete processor classes.\n\n    Returns:\n        Dictionary with processing results\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/processors/base_config_processor/#core.processors.base_config_processor.BaseConfigProcessor.process_column_strategies","title":"<code>process_column_strategies(df, strategy_state=None, mode='NORMAL')</code>","text":"<p>Process all column strategies and populate the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Base DataFrame to populate</p> required <code>strategy_state</code> <code>dict[str, Any]</code> <p>State of the strategy</p> <code>None</code> <p>Returns:     DataFrame with generated data</p> Source code in <code>core/processors/base_config_processor.py</code> <pre><code>def process_column_strategies(\n    self,\n    df: pd.DataFrame,\n    strategy_state: dict[str, Any] = None,\n    mode: str = \"NORMAL\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Process all column strategies and populate the DataFrame.\n\n    Args:\n        df: Base DataFrame to populate\n        strategy_state: State of the strategy\n    Returns:\n        DataFrame with generated data\n    \"\"\"\n    self.logger.info(f\"Processing {len(self.configs)} column configurations\")\n\n    # Ensure we keep a single shared state object across all columns\n    shared_state = strategy_state if strategy_state is not None else {}\n\n    with measure_time(\"data_generation\", rows_processed=len(df)):\n        for cur_config in self.configs:\n            df = self._process_single_config(df, cur_config, shared_state, mode)\n\n    return df\n</code></pre>"},{"location":"reference/processors/base_config_processor/#core.processors.base_config_processor.BaseConfigProcessor.validate_config","title":"<code>validate_config()</code>","text":"<p>Validate the processor configuration.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if configuration is valid</p> <p>Raises:</p> Type Description <code>InvalidConfigParamException</code> <p>If configuration is invalid</p> Source code in <code>core/processors/base_config_processor.py</code> <pre><code>def validate_config(self) -&gt; bool:\n    \"\"\"\n    Validate the processor configuration.\n\n    Returns:\n        True if configuration is valid\n\n    Raises:\n        InvalidConfigParamException: If configuration is invalid\n    \"\"\"\n    try:\n        validate_generator_config(self.config)\n        self.logger.debug(\"Configuration validation passed\")\n        return True\n    except InvalidConfigParamException as e:\n        self.logger.error(f\"Configuration validation failed: {e}\")\n        raise\n</code></pre>"},{"location":"reference/processors/normal_config_processor/","title":"normal_config_processor","text":""},{"location":"reference/processors/normal_config_processor/#core.processors.normal_config_processor","title":"<code>core.processors.normal_config_processor</code>","text":"<p>Normal config processor for GenXData.</p> <p>Handles traditional data generation where all data is generated at once and then written to output. This replaces the functionality from process_config.py.</p>"},{"location":"reference/processors/normal_config_processor/#core.processors.normal_config_processor.NormalConfigProcessor","title":"<code>NormalConfigProcessor</code>","text":"<p>               Bases: <code>BaseConfigProcessor</code></p> <p>Processor for normal (non-streaming) data generation.</p> <p>Generates all data at once, applies strategy-driven transformations per column, optionally shuffles data, filters intermediate columns, and writes the complete dataset using the configured writer.</p> Source code in <code>core/processors/normal_config_processor.py</code> <pre><code>class NormalConfigProcessor(BaseConfigProcessor):\n    \"\"\"\n    Processor for normal (non-streaming) data generation.\n\n    Generates all data at once, applies strategy-driven transformations per\n    column, optionally shuffles data, filters intermediate columns, and writes\n    the complete dataset using the configured writer.\n    \"\"\"\n\n    def __init__(self, config: dict[str, Any], writer, perf_report: bool = False):\n        \"\"\"\n        Initialize the normal config processor.\n\n        Args:\n            config: Configuration dictionary\n            writer: Writer instance for output\n            perf_report: Whether to generate performance report\n        \"\"\"\n        super().__init__(config, writer)\n        self.perf_report = perf_report\n\n        self.logger.info(f\"NormalConfigProcessor initialized for {self.rows} rows\")\n\n    def process(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Process the configuration using normal (all-at-once) generation.\n\n        Returns a standardized result schema:\n        - status: \"success\" | \"error\"\n        - processor_type: \"normal\"\n        - rows_generated: int\n        - columns_generated: int\n        - column_names: list[str]\n        - writer_summary: dict\n        - performance_report: dict | None\n        - config_name: str\n        \"\"\"\n        self.logger.info(\"Starting normal config processing\")\n\n        try:\n            # Validate configuration\n            self.validate_config()\n\n            # Create base DataFrame\n            df = self.create_base_dataframe()\n\n            # Process all column strategies\n            df = self.process_column_strategies(df)\n\n            # Apply shuffle if enabled\n\n            # TODO: Apply overridable columne level shuffle.\n            df = self.apply_shuffle(df)\n\n\n            # Filter out intermediate columns\n            df = self.filter_intermediate_columns(df)\n\n            # Write output using the writer\n            self.logger.info(\n                f\"Writing output using {self.writer.__class__.__name__} writer\"\n            )\n            with measure_time(\"file_writing\", rows_processed=len(df)):\n                write_result = self.writer.write(df)\n                self.logger.info(f\"Write result: {write_result}\")\n\n            # Finalize writer\n            writer_summary = self.writer.finalize()\n\n            # Generate performance report if requested\n            perf_summary = None\n            if self.perf_report:\n                self.logger.debug(\"Generating performance report\")\n                perf_summary = get_performance_report()\n                self.logger.info(\"Performance report generated\")\n\n            self.logger.info(\n                f\"Normal config processing completed. Generated {len(df)} rows \"\n                f\"with {len(df.columns)} columns.\"\n            )\n\n            return {\n                \"status\": \"success\",\n                \"processor_type\": \"normal\",\n                \"rows_generated\": len(df),\n                \"columns_generated\": len(df.columns),\n                \"column_names\": list(df.columns),\n                # \"df\": df,\n                \"writer_summary\": writer_summary,\n                \"performance_report\": perf_summary,\n                \"config_name\": self.config.get(\"metadata\", {}).get(\"name\", \"unknown\"),\n            }\n\n        except Exception as e:\n            self.logger.error(\n                f\"Error during normal config processing: {e}\", exc_info=True\n            )\n            return {\n                \"status\": \"error\",\n                \"processor_type\": \"normal\",\n                \"error\": str(e),\n                \"config_name\": self.config.get(\"metadata\", {}).get(\"name\", \"unknown\"),\n            }\n</code></pre>"},{"location":"reference/processors/normal_config_processor/#core.processors.normal_config_processor.NormalConfigProcessor.process","title":"<code>process()</code>","text":"<p>Process the configuration using normal (all-at-once) generation.</p> <p>Returns a standardized result schema: - status: \"success\" | \"error\" - processor_type: \"normal\" - rows_generated: int - columns_generated: int - column_names: list[str] - writer_summary: dict - performance_report: dict | None - config_name: str</p> Source code in <code>core/processors/normal_config_processor.py</code> <pre><code>def process(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Process the configuration using normal (all-at-once) generation.\n\n    Returns a standardized result schema:\n    - status: \"success\" | \"error\"\n    - processor_type: \"normal\"\n    - rows_generated: int\n    - columns_generated: int\n    - column_names: list[str]\n    - writer_summary: dict\n    - performance_report: dict | None\n    - config_name: str\n    \"\"\"\n    self.logger.info(\"Starting normal config processing\")\n\n    try:\n        # Validate configuration\n        self.validate_config()\n\n        # Create base DataFrame\n        df = self.create_base_dataframe()\n\n        # Process all column strategies\n        df = self.process_column_strategies(df)\n\n        # Apply shuffle if enabled\n\n        # TODO: Apply overridable columne level shuffle.\n        df = self.apply_shuffle(df)\n\n\n        # Filter out intermediate columns\n        df = self.filter_intermediate_columns(df)\n\n        # Write output using the writer\n        self.logger.info(\n            f\"Writing output using {self.writer.__class__.__name__} writer\"\n        )\n        with measure_time(\"file_writing\", rows_processed=len(df)):\n            write_result = self.writer.write(df)\n            self.logger.info(f\"Write result: {write_result}\")\n\n        # Finalize writer\n        writer_summary = self.writer.finalize()\n\n        # Generate performance report if requested\n        perf_summary = None\n        if self.perf_report:\n            self.logger.debug(\"Generating performance report\")\n            perf_summary = get_performance_report()\n            self.logger.info(\"Performance report generated\")\n\n        self.logger.info(\n            f\"Normal config processing completed. Generated {len(df)} rows \"\n            f\"with {len(df.columns)} columns.\"\n        )\n\n        return {\n            \"status\": \"success\",\n            \"processor_type\": \"normal\",\n            \"rows_generated\": len(df),\n            \"columns_generated\": len(df.columns),\n            \"column_names\": list(df.columns),\n            # \"df\": df,\n            \"writer_summary\": writer_summary,\n            \"performance_report\": perf_summary,\n            \"config_name\": self.config.get(\"metadata\", {}).get(\"name\", \"unknown\"),\n        }\n\n    except Exception as e:\n        self.logger.error(\n            f\"Error during normal config processing: {e}\", exc_info=True\n        )\n        return {\n            \"status\": \"error\",\n            \"processor_type\": \"normal\",\n            \"error\": str(e),\n            \"config_name\": self.config.get(\"metadata\", {}).get(\"name\", \"unknown\"),\n        }\n</code></pre>"},{"location":"reference/processors/streaming_config_processor/","title":"streaming_config_processor","text":""},{"location":"reference/processors/streaming_config_processor/#core.processors.streaming_config_processor","title":"<code>core.processors.streaming_config_processor</code>","text":"<p>Streaming config processor for GenXData.</p> <p>Handles chunked data generation where data is generated in batches/chunks and written incrementally. This handles both streaming and batch scenarios.</p>"},{"location":"reference/processors/streaming_config_processor/#core.processors.streaming_config_processor.StreamingConfigProcessor","title":"<code>StreamingConfigProcessor</code>","text":"<p>               Bases: <code>BaseConfigProcessor</code></p> <p>Processor for streaming/batch data generation.</p> <p>Generates data in chunks, applying the same column-strategy flow as normal mode, and writes incrementally via a batch-compatible writer. Maintains per- strategy state across chunks to ensure consistent sequences.</p> Source code in <code>core/processors/streaming_config_processor.py</code> <pre><code>class StreamingConfigProcessor(BaseConfigProcessor):\n    \"\"\"\n    Processor for streaming/batch data generation.\n\n    Generates data in chunks, applying the same column-strategy flow as normal\n    mode, and writes incrementally via a batch-compatible writer. Maintains per-\n    strategy state across chunks to ensure consistent sequences.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: dict[str, Any],\n        writer,\n        batch_size: int = 1000,\n        chunk_size: int = 1000,\n        perf_report: bool = False,\n    ):\n        \"\"\"\n        Initialize the streaming config processor.\n\n        Args:\n            config: Configuration dictionary\n            writer: Writer instance for output\n            batch_size: Size of each batch to write\n            chunk_size: Size of chunks to generate at once\n            perf_report: Whether to generate performance report\n        \"\"\"\n        super().__init__(config, writer)\n        self.batch_size = batch_size\n        self.chunk_size = min(chunk_size, batch_size)  # Don't exceed batch size\n        self.perf_report = perf_report\n        self.strategy_state = {}\n        # Pool of strategy instances reused across chunks keyed by \"STRATEGY:column\"\n        self.strategy_pool: dict[str, Any] = {}\n\n        # Ensure writer is batch-compatible\n        if not isinstance(writer, BatchWriter):\n            self.writer = BatchWriter(config, writer)\n\n        self.logger.info(\n            f\"StreamingConfigProcessor initialized: target_rows={self.rows}, \"\n            f\"batch_size={self.batch_size}, chunk_size={self.chunk_size}\"\n        )\n\n    def _process_chunk(self, chunk_size: int) -&gt; pd.DataFrame:\n        \"\"\"\n        Process a single chunk following the same steps as normal processor.\n\n        Args:\n            chunk_size: Number of rows to generate in this chunk\n\n        Returns:\n            Processed DataFrame chunk ready for writing\n        \"\"\"\n        self.logger.debug(f\"Processing chunk of {chunk_size} rows\")\n\n        # Create base DataFrame (same as normal processor)\n        chunk_df = self.create_base_dataframe(chunk_size)\n\n        # Process column strategies (same as normal processor)\n        chunk_df = self.process_column_strategies(\n            chunk_df, self.strategy_state, mode=\"STREAM&amp;BATCH\"\n        )\n\n        # Apply shuffle if enabled (same as normal processor)\n        chunk_df = self.apply_shuffle(chunk_df)\n\n        # Filter out intermediate columns (same as normal processor)\n        chunk_df = self.filter_intermediate_columns(chunk_df)\n\n        return chunk_df\n\n    def process(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Process the configuration using streaming/batch generation.\n\n        Follows the same pattern as NormalConfigProcessor but processes data in chunks.\n\n        Returns a standardized result schema:\n        - status: \"success\" | \"error\"\n        - processor_type: \"streaming\"\n        - rows_generated: int\n        - columns_generated: int\n        - column_names: list[str]\n        - chunks_processed: int\n        - chunk_size: int\n        - batch_size: int\n        - writer_summary: dict\n        - performance_report: dict | None\n        - config_name: str\n        \"\"\"\n        self.logger.info(f\"Starting streaming config processing for {self.rows} rows\")\n\n        try:\n            # Validate configuration (same as normal processor)\n            self.validate_config()\n\n            # Process data in chunks, following the same steps as normal processor\n            total_generated = 0\n            batch_count = 0\n            last_columns: list[str] | None = None\n\n            while total_generated &lt; self.rows:\n                # Calculate chunk size\n                remaining_rows = self.rows - total_generated\n                chunk_size = min(self.chunk_size, remaining_rows)\n                self.logger.debug(\"--------------------------------\" * 3)\n                self.logger.debug(\n                    f\"Processing chunk {batch_count + 1}: size={chunk_size}, \"\n                    f\"total_generated={total_generated}, \"\n                    f\"remaining={remaining_rows}\"\n                )\n\n                # Process chunk following the same steps as normal processor\n                with measure_time(\"chunk_processing\", rows_processed=chunk_size):\n                    chunk_df = self._process_chunk(chunk_size)\n                    # Track column schema for standardized summary\n                    last_columns = list(chunk_df.columns)\n\n                # Write chunk using the writer (same as normal processor)\n                self.logger.debug(\n                    f\"Writing chunk {batch_count + 1} with {len(chunk_df)} rows\"\n                )\n                # with measure_time(\"chunk_writing\", rows_processed=len(chunk_df)):\n                batch_info = {\n                    \"batch_index\": batch_count,\n                    \"chunk_index\": batch_count + 1,\n                    \"batch_size\": len(chunk_df),\n                    \"total_batches\": (self.rows + self.batch_size - 1)\n                    // self.batch_size,\n                    \"timestamp\": pd.Timestamp.now().isoformat(),\n                }\n                # with measure_time(\"chunk_writing\", rows_processed=len(chunk_df)):\n                write_result = self.writer.write(chunk_df, batch_info)\n                self.logger.debug(f\"Chunk write result: {write_result}\")\n\n                # Update counters\n                total_generated += chunk_size\n                batch_count += 1\n\n                self.logger.debug(\n                    f\"Chunk {batch_count} completed, total generated: {total_generated}\"\n                )\n\n            # Finalize writer (same as normal processor)\n            writer_summary = self.writer.finalize()\n\n            # Generate performance report if requested (same as normal processor)\n            perf_summary = None\n            if self.perf_report:\n                self.logger.debug(\"Generating performance report\")\n                perf_summary = get_performance_report()\n                self.logger.info(\"Performance report generated\")\n\n            self.logger.info(\n                f\"Streaming config processing completed. Generated {total_generated} rows \"\n                f\"in {batch_count} chunks.\"\n            )\n\n            return {\n                \"status\": \"success\",\n                \"processor_type\": \"streaming\",\n                \"rows_generated\": total_generated,\n                \"columns_generated\": len(last_columns) if last_columns else 0,\n                \"column_names\": last_columns or [],\n                \"chunks_processed\": batch_count,\n                \"chunk_size\": self.chunk_size,\n                \"batch_size\": self.batch_size,\n                \"writer_summary\": writer_summary,\n                \"performance_report\": perf_summary,\n                \"config_name\": self.config.get(\"metadata\", {}).get(\"name\", \"unknown\"),\n            }\n\n        except Exception as e:\n            self.logger.error(f\"Error during streaming config processing: {e}\")\n            return {\n                \"status\": \"error\",\n                \"processor_type\": \"streaming\",\n                \"error\": str(e),\n                \"config_name\": self.config.get(\"metadata\", {}).get(\"name\", \"unknown\"),\n            }\n</code></pre>"},{"location":"reference/processors/streaming_config_processor/#core.processors.streaming_config_processor.StreamingConfigProcessor.process","title":"<code>process()</code>","text":"<p>Process the configuration using streaming/batch generation.</p> <p>Follows the same pattern as NormalConfigProcessor but processes data in chunks.</p> <p>Returns a standardized result schema: - status: \"success\" | \"error\" - processor_type: \"streaming\" - rows_generated: int - columns_generated: int - column_names: list[str] - chunks_processed: int - chunk_size: int - batch_size: int - writer_summary: dict - performance_report: dict | None - config_name: str</p> Source code in <code>core/processors/streaming_config_processor.py</code> <pre><code>def process(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Process the configuration using streaming/batch generation.\n\n    Follows the same pattern as NormalConfigProcessor but processes data in chunks.\n\n    Returns a standardized result schema:\n    - status: \"success\" | \"error\"\n    - processor_type: \"streaming\"\n    - rows_generated: int\n    - columns_generated: int\n    - column_names: list[str]\n    - chunks_processed: int\n    - chunk_size: int\n    - batch_size: int\n    - writer_summary: dict\n    - performance_report: dict | None\n    - config_name: str\n    \"\"\"\n    self.logger.info(f\"Starting streaming config processing for {self.rows} rows\")\n\n    try:\n        # Validate configuration (same as normal processor)\n        self.validate_config()\n\n        # Process data in chunks, following the same steps as normal processor\n        total_generated = 0\n        batch_count = 0\n        last_columns: list[str] | None = None\n\n        while total_generated &lt; self.rows:\n            # Calculate chunk size\n            remaining_rows = self.rows - total_generated\n            chunk_size = min(self.chunk_size, remaining_rows)\n            self.logger.debug(\"--------------------------------\" * 3)\n            self.logger.debug(\n                f\"Processing chunk {batch_count + 1}: size={chunk_size}, \"\n                f\"total_generated={total_generated}, \"\n                f\"remaining={remaining_rows}\"\n            )\n\n            # Process chunk following the same steps as normal processor\n            with measure_time(\"chunk_processing\", rows_processed=chunk_size):\n                chunk_df = self._process_chunk(chunk_size)\n                # Track column schema for standardized summary\n                last_columns = list(chunk_df.columns)\n\n            # Write chunk using the writer (same as normal processor)\n            self.logger.debug(\n                f\"Writing chunk {batch_count + 1} with {len(chunk_df)} rows\"\n            )\n            # with measure_time(\"chunk_writing\", rows_processed=len(chunk_df)):\n            batch_info = {\n                \"batch_index\": batch_count,\n                \"chunk_index\": batch_count + 1,\n                \"batch_size\": len(chunk_df),\n                \"total_batches\": (self.rows + self.batch_size - 1)\n                // self.batch_size,\n                \"timestamp\": pd.Timestamp.now().isoformat(),\n            }\n            # with measure_time(\"chunk_writing\", rows_processed=len(chunk_df)):\n            write_result = self.writer.write(chunk_df, batch_info)\n            self.logger.debug(f\"Chunk write result: {write_result}\")\n\n            # Update counters\n            total_generated += chunk_size\n            batch_count += 1\n\n            self.logger.debug(\n                f\"Chunk {batch_count} completed, total generated: {total_generated}\"\n            )\n\n        # Finalize writer (same as normal processor)\n        writer_summary = self.writer.finalize()\n\n        # Generate performance report if requested (same as normal processor)\n        perf_summary = None\n        if self.perf_report:\n            self.logger.debug(\"Generating performance report\")\n            perf_summary = get_performance_report()\n            self.logger.info(\"Performance report generated\")\n\n        self.logger.info(\n            f\"Streaming config processing completed. Generated {total_generated} rows \"\n            f\"in {batch_count} chunks.\"\n        )\n\n        return {\n            \"status\": \"success\",\n            \"processor_type\": \"streaming\",\n            \"rows_generated\": total_generated,\n            \"columns_generated\": len(last_columns) if last_columns else 0,\n            \"column_names\": last_columns or [],\n            \"chunks_processed\": batch_count,\n            \"chunk_size\": self.chunk_size,\n            \"batch_size\": self.batch_size,\n            \"writer_summary\": writer_summary,\n            \"performance_report\": perf_summary,\n            \"config_name\": self.config.get(\"metadata\", {}).get(\"name\", \"unknown\"),\n        }\n\n    except Exception as e:\n        self.logger.error(f\"Error during streaming config processing: {e}\")\n        return {\n            \"status\": \"error\",\n            \"processor_type\": \"streaming\",\n            \"error\": str(e),\n            \"config_name\": self.config.get(\"metadata\", {}).get(\"name\", \"unknown\"),\n        }\n</code></pre>"},{"location":"reference/strategies/","title":"Strategies","text":"<ul> <li>concat_strategy</li> <li>date_series_strategy</li> <li>delete_strategy</li> <li>distributed_choice_strategy</li> <li>distributed_date_range_strategy</li> <li>distributed_number_range_strategy</li> <li>distributed_time_range_strategy</li> <li>mapping_strategy</li> <li>pattern_strategy</li> <li>random_date_range_strategy</li> <li>random_name_strategy</li> <li>random_number_range_strategy</li> <li>replacement_strategy</li> <li>series_strategy</li> <li>time_range_strategy</li> <li>uuid_strategy</li> </ul>"},{"location":"reference/strategies/concat_strategy/","title":"concat_strategy","text":""},{"location":"reference/strategies/concat_strategy/#core.strategies.concat_strategy","title":"<code>core.strategies.concat_strategy</code>","text":"<p>Concat strategy for concatenating values from multiple columns.</p> <p>This strategy has been refactored to use the new mixin pattern, reducing boilerplate code by ~30 lines while maintaining the same functionality.</p>"},{"location":"reference/strategies/concat_strategy/#core.strategies.concat_strategy.ConcatStrategy","title":"<code>ConcatStrategy</code>","text":"<p>               Bases: <code>BaseStrategy</code>, <code>StatefulMixin</code>, <code>ValidationMixin</code></p> <p>Strategy for concatenating values from multiple columns. Implements stateful interface for consistency.</p> <p>Uses mixins for: - StatefulMixin: Standardized state management and reporting - ValidationMixin: Common parameter validation patterns</p> Source code in <code>core/strategies/concat_strategy.py</code> <pre><code>class ConcatStrategy(BaseStrategy, StatefulMixin, ValidationMixin):\n    \"\"\"\n    Strategy for concatenating values from multiple columns.\n    Implements stateful interface for consistency.\n\n    Uses mixins for:\n    - StatefulMixin: Standardized state management and reporting\n    - ValidationMixin: Common parameter validation patterns\n    \"\"\"\n\n    def __init__(self, mode: str, logger=None, **kwargs):\n        \"\"\"Initialize the strategy with configuration parameters\"\"\"\n        super().__init__(mode=mode, logger=logger, **kwargs)\n\n        self._initialize_state()  # From StatefulMixin\n\n    def _initialize_state(self):\n        \"\"\"Initialize internal state for stateful generation\"\"\"\n        super()._initialize_state()  # Call StatefulMixin's _initialize_state first\n\n        # Store concatenation parameters for efficient access\n        self._lhs_col = self.params[\"lhs_col\"]\n        self._rhs_col = self.params[\"rhs_col\"]\n        self._prefix = self.params.get(\"prefix\", \"\")\n        self._suffix = self.params.get(\"suffix\", \"\")\n        self._separator = self.params.get(\"separator\", \"\")\n\n        self.logger.debug(\n            f\"ConcatStrategy initialized: {self._lhs_col} + {self._rhs_col} \"\n            f\"with prefix='{self._prefix}', suffix='{self._suffix}', separator='{self._separator}'\"\n        )\n\n    def generate_chunk(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Generate a chunk of concatenated values maintaining internal state.\n        For concat strategy, this works with existing data from multiple columns.\n\n        Args:\n            count: Number of values to generate\n\n        Returns:\n            pd.Series: Series of concatenated values\n        \"\"\"\n        self.logger.debug(f\"Generating chunk of {count} concatenated values\")\n\n        if self.df is None:\n            self.logger.warning(\"No dataframe available for concatenation\")\n            return pd.Series([f\"{self._prefix}{self._separator}{self._suffix}\"] * count)\n\n        # Get data from both columns, limited to count\n        lhs_data = self.df[self._lhs_col].head(count)\n        rhs_data = self.df[self._rhs_col].head(count)\n\n        # Convert to string for concatenation\n        if lhs_data.dtype != \"str\":\n            lhs_data_as_string = lhs_data.astype(str)\n        else:\n            lhs_data_as_string = lhs_data\n\n        if rhs_data.dtype != \"str\":\n            rhs_data_as_string = rhs_data.astype(str)\n        else:\n            rhs_data_as_string = rhs_data\n\n        # Perform concatenation\n        result = (\n            self._prefix\n            + lhs_data_as_string\n            + self._separator\n            + rhs_data_as_string\n            + self._suffix\n        )\n\n        return result\n\n    def reset_state(self):\n        \"\"\"Reset the internal state to initial values\"\"\"\n        self.logger.debug(\"Resetting ConcatStrategy state\")\n        self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/concat_strategy/#core.strategies.concat_strategy.ConcatStrategy.generate_chunk","title":"<code>generate_chunk(count)</code>","text":"<p>Generate a chunk of concatenated values maintaining internal state. For concat strategy, this works with existing data from multiple columns.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of values to generate</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Series of concatenated values</p> Source code in <code>core/strategies/concat_strategy.py</code> <pre><code>def generate_chunk(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Generate a chunk of concatenated values maintaining internal state.\n    For concat strategy, this works with existing data from multiple columns.\n\n    Args:\n        count: Number of values to generate\n\n    Returns:\n        pd.Series: Series of concatenated values\n    \"\"\"\n    self.logger.debug(f\"Generating chunk of {count} concatenated values\")\n\n    if self.df is None:\n        self.logger.warning(\"No dataframe available for concatenation\")\n        return pd.Series([f\"{self._prefix}{self._separator}{self._suffix}\"] * count)\n\n    # Get data from both columns, limited to count\n    lhs_data = self.df[self._lhs_col].head(count)\n    rhs_data = self.df[self._rhs_col].head(count)\n\n    # Convert to string for concatenation\n    if lhs_data.dtype != \"str\":\n        lhs_data_as_string = lhs_data.astype(str)\n    else:\n        lhs_data_as_string = lhs_data\n\n    if rhs_data.dtype != \"str\":\n        rhs_data_as_string = rhs_data.astype(str)\n    else:\n        rhs_data_as_string = rhs_data\n\n    # Perform concatenation\n    result = (\n        self._prefix\n        + lhs_data_as_string\n        + self._separator\n        + rhs_data_as_string\n        + self._suffix\n    )\n\n    return result\n</code></pre>"},{"location":"reference/strategies/concat_strategy/#core.strategies.concat_strategy.ConcatStrategy.reset_state","title":"<code>reset_state()</code>","text":"<p>Reset the internal state to initial values</p> Source code in <code>core/strategies/concat_strategy.py</code> <pre><code>def reset_state(self):\n    \"\"\"Reset the internal state to initial values\"\"\"\n    self.logger.debug(\"Resetting ConcatStrategy state\")\n    self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/date_series_strategy/","title":"date_series_strategy","text":""},{"location":"reference/strategies/date_series_strategy/#core.strategies.date_series_strategy","title":"<code>core.strategies.date_series_strategy</code>","text":"<p>Date series strategy: generates a contiguous date sequence starting from a date.</p> <p>Supports: - start_date: inclusive start (string in format) - freq: pandas offset alias (e.g., 'D', 'W', 'M') - format: input format for parsing start - output_format: strftime format applied to output strings</p>"},{"location":"reference/strategies/delete_strategy/","title":"delete_strategy","text":""},{"location":"reference/strategies/delete_strategy/#core.strategies.delete_strategy","title":"<code>core.strategies.delete_strategy</code>","text":"<p>Delete strategy for removing values from a column.</p> <p>This strategy has been refactored to use the new mixin pattern, reducing boilerplate code by ~25 lines while maintaining the same functionality.</p>"},{"location":"reference/strategies/delete_strategy/#core.strategies.delete_strategy.DeleteStrategy","title":"<code>DeleteStrategy</code>","text":"<p>               Bases: <code>BaseStrategy</code>, <code>StatefulMixin</code>, <code>ValidationMixin</code></p> <p>Strategy for deleting/nullifying values in a column. Implements stateful interface for consistency.</p> <p>Uses mixins for: - StatefulMixin: Standardized state management and reporting - ValidationMixin: Common parameter validation patterns</p> Source code in <code>core/strategies/delete_strategy.py</code> <pre><code>class DeleteStrategy(BaseStrategy, StatefulMixin, ValidationMixin):\n    \"\"\"\n    Strategy for deleting/nullifying values in a column.\n    Implements stateful interface for consistency.\n\n    Uses mixins for:\n    - StatefulMixin: Standardized state management and reporting\n    - ValidationMixin: Common parameter validation patterns\n    \"\"\"\n\n    def __init__(self, mode: str, logger=None, **kwargs):\n        \"\"\"Initialize the strategy with configuration parameters\"\"\"\n        super().__init__(mode=mode, logger=logger, **kwargs)\n\n        # Initialize state\n        self._initialize_state()  # From StatefulMixin\n\n    def _initialize_state(self):\n        \"\"\"Initialize internal state for stateful generation\"\"\"\n        super()._initialize_state()  # Call StatefulMixin's _initialize_state first\n\n        # Store mask for efficient access\n        self._mask = self.params[\"mask\"]\n\n        self.logger.debug(f\"DeleteStrategy initialized with mask='{self._mask}'\")\n\n    def generate_chunk(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Generate a chunk of None values maintaining internal state.\n        For delete strategy, this is stateless but implements the interface.\n\n        Args:\n            count: Number of values to generate\n\n        Returns:\n            pd.Series: Series of None values\n        \"\"\"\n        self.logger.debug(f\"Generating chunk of {count} None values for deletion\")\n\n        # Return None values for the masked rows\n        return pd.Series([None] * count, dtype=object)\n\n    def reset_state(self):\n        \"\"\"Reset the internal state to initial values\"\"\"\n        self.logger.debug(\"Resetting DeleteStrategy state\")\n        self._initialize_state()\n\n    # def get_current_state(self) -&gt; dict:\n    #     \"\"\"Get current state information for debugging\"\"\"\n    #     return {\n    #         \"strategy\": \"DeleteStrategy\",\n    #         \"stateful\": True,\n    #         \"column\": self.col_name,\n    #         \"mask\": self._mask,\n    #     }\n\n    def generate_data(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Generate None values by calling generate_chunk.\n        This ensures consistent behavior between batch and non-batch modes.\n\n        Args:\n            count: Number of values to generate\n\n        Returns:\n            pd.Series: Series of None values\n        \"\"\"\n        self.logger.debug(\n            f\"Generating {count} values using unified chunk-based approach\"\n        )\n\n        # For non-batch mode, reset state to ensure consistent behavior\n        self.reset_state()\n        # Generate the chunk\n        result = self.generate_chunk(count)\n\n        self.logger.debug(f\"Generated {len(result)} None values for deletion\")\n\n        return result\n</code></pre>"},{"location":"reference/strategies/delete_strategy/#core.strategies.delete_strategy.DeleteStrategy.generate_chunk","title":"<code>generate_chunk(count)</code>","text":"<p>Generate a chunk of None values maintaining internal state. For delete strategy, this is stateless but implements the interface.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of values to generate</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Series of None values</p> Source code in <code>core/strategies/delete_strategy.py</code> <pre><code>def generate_chunk(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Generate a chunk of None values maintaining internal state.\n    For delete strategy, this is stateless but implements the interface.\n\n    Args:\n        count: Number of values to generate\n\n    Returns:\n        pd.Series: Series of None values\n    \"\"\"\n    self.logger.debug(f\"Generating chunk of {count} None values for deletion\")\n\n    # Return None values for the masked rows\n    return pd.Series([None] * count, dtype=object)\n</code></pre>"},{"location":"reference/strategies/delete_strategy/#core.strategies.delete_strategy.DeleteStrategy.generate_data","title":"<code>generate_data(count)</code>","text":"<p>Generate None values by calling generate_chunk. This ensures consistent behavior between batch and non-batch modes.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of values to generate</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Series of None values</p> Source code in <code>core/strategies/delete_strategy.py</code> <pre><code>def generate_data(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Generate None values by calling generate_chunk.\n    This ensures consistent behavior between batch and non-batch modes.\n\n    Args:\n        count: Number of values to generate\n\n    Returns:\n        pd.Series: Series of None values\n    \"\"\"\n    self.logger.debug(\n        f\"Generating {count} values using unified chunk-based approach\"\n    )\n\n    # For non-batch mode, reset state to ensure consistent behavior\n    self.reset_state()\n    # Generate the chunk\n    result = self.generate_chunk(count)\n\n    self.logger.debug(f\"Generated {len(result)} None values for deletion\")\n\n    return result\n</code></pre>"},{"location":"reference/strategies/delete_strategy/#core.strategies.delete_strategy.DeleteStrategy.reset_state","title":"<code>reset_state()</code>","text":"<p>Reset the internal state to initial values</p> Source code in <code>core/strategies/delete_strategy.py</code> <pre><code>def reset_state(self):\n    \"\"\"Reset the internal state to initial values\"\"\"\n    self.logger.debug(\"Resetting DeleteStrategy state\")\n    self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/distributed_choice_strategy/","title":"distributed_choice_strategy","text":""},{"location":"reference/strategies/distributed_choice_strategy/#core.strategies.distributed_choice_strategy","title":"<code>core.strategies.distributed_choice_strategy</code>","text":"<p>Distributed choice strategy for generating values based on weighted choices.</p> <p>This strategy has been refactored to use the new mixin pattern, reducing boilerplate code by ~40 lines while maintaining the same functionality.</p>"},{"location":"reference/strategies/distributed_choice_strategy/#core.strategies.distributed_choice_strategy.DistributedChoiceStrategy","title":"<code>DistributedChoiceStrategy</code>","text":"<p>               Bases: <code>BaseStrategy</code>, <code>SeedMixin</code>, <code>StatefulMixin</code>, <code>ValidationMixin</code>, <code>CategoricalMixin</code></p> <p>Strategy for generating values based on weighted choices.</p> <p>Uses mixins for: - SeedMixin: Automatic seed validation and random state initialization - StatefulMixin: Standardized state management and reporting - ValidationMixin: Common parameter validation patterns - CategoricalMixin: Specialized categorical data validation and utilities</p> Source code in <code>core/strategies/distributed_choice_strategy.py</code> <pre><code>class DistributedChoiceStrategy(\n    BaseStrategy, SeedMixin, StatefulMixin, ValidationMixin, CategoricalMixin\n):\n    \"\"\"\n    Strategy for generating values based on weighted choices.\n\n    Uses mixins for:\n    - SeedMixin: Automatic seed validation and random state initialization\n    - StatefulMixin: Standardized state management and reporting\n    - ValidationMixin: Common parameter validation patterns\n    - CategoricalMixin: Specialized categorical data validation and utilities\n    \"\"\"\n\n    def _validate_params(self):\n        \"\"\"Delegate validation to DistributedChoiceConfig.\"\"\"\n        self._validate_required_params([\"choices\"])\n        DistributedChoiceConfig(choices=self.params[\"choices\"]).validate()\n\n    def __init__(self, mode: str, logger=None, **kwargs):\n        \"\"\"Initialize the strategy with configuration parameters\"\"\"\n        super().__init__(mode=mode, logger=logger, **kwargs)\n\n        # Use mixins for common functionality\n        self._validate_seed()  # From SeedMixin\n        self._initialize_random_seed()  # From SeedMixin\n        self._initialize_state()  # From StatefulMixin\n        self._validate_params()  # From ValidationMixin\n\n    def _initialize_state(self):\n        \"\"\"Initialize internal state for stateful generation\"\"\"\n        super()._initialize_state()  # Call StatefulMixin's _initialize_state first\n\n        # Seed initialization is handled by SeedMixin\n        self.logger.debug(\n            f\"DistributedChoiceStrategy initialized with seed={self._seed}\"\n        )\n\n    def generate_chunk(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Generate a chunk of data maintaining internal state.\n        This method is stateful and maintains consistent random sequence.\n        Args:\n            count: Number of values to generate\n        Returns:\n            pd.Series: Generated values\n        \"\"\"\n        self.logger.debug(f\"Generating chunk of {count} values\")\n        # Use the original generation logic\n        \"\"\"\n        Generate random values based on weighted choices.\n\n        Args:\n            count: Number of values to generate\n\n        Returns:\n            Series of chosen values\n        \"\"\"\n        choices_dict = self.params[\"choices\"]\n\n        # Extract choices and weights\n        choices = list(choices_dict.keys())\n        weights = list(choices_dict.values())\n\n        # Generate random choices based on weights\n        values = []\n\n        # Calculate total weight\n        total_weight = sum(weights)\n\n        # Generate values proportionally based on weights\n        for choice, weight in choices_dict.items():\n            # Calculate how many values this choice should get\n            proportion = weight / total_weight\n            num_values = int(proportion * count)\n\n            # Add the values for this choice\n            for _ in range(num_values):\n                values.append(choice)\n\n        # Handle any remaining values due to rounding\n        while len(values) &lt; count:\n            # Add random choice based on weights\n            choice = np.random.choice(choices, p=[w / total_weight for w in weights])\n            values.append(choice)\n\n        return pd.Series(values, dtype=object)\n\n    def reset_state(self):\n        \"\"\"Reset the internal state to initial values\"\"\"\n        self.logger.debug(\"Resetting DistributedChoiceStrategy state\")\n        self._initialize_state()\n\n    # def get_current_state(self) -&gt; dict:\n    #     \"\"\"Get current state information for debugging\"\"\"\n    #     return {\n    #         \"strategy\": \"DistributedChoiceStrategy\",\n    #         \"stateful\": True,\n    #         \"column\": self.col_name,\n    #         \"seed\": self.params.get(\"seed\", None),\n    #     }\n\n    def generate_data(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Generate data by calling generate_chunk.\n        This ensures consistent behavior between batch and non-batch modes.\n        Args:\n            count: Number of values to generate\n        Returns:\n            pd.Series: Generated values\n        \"\"\"\n        self.logger.debug(\n            f\"Generating {count} values using unified chunk-based approach\"\n        )\n        # For non-streaming mode, reset state to ensure consistent behavior\n        if not self.is_streaming_and_batch():\n            self.reset_state()\n        # Generate the chunk\n        result = self.generate_chunk(count)\n        return result\n</code></pre>"},{"location":"reference/strategies/distributed_choice_strategy/#core.strategies.distributed_choice_strategy.DistributedChoiceStrategy.generate_chunk","title":"<code>generate_chunk(count)</code>","text":"<p>Generate a chunk of data maintaining internal state. This method is stateful and maintains consistent random sequence. Args:     count: Number of values to generate Returns:     pd.Series: Generated values</p> Source code in <code>core/strategies/distributed_choice_strategy.py</code> <pre><code>def generate_chunk(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Generate a chunk of data maintaining internal state.\n    This method is stateful and maintains consistent random sequence.\n    Args:\n        count: Number of values to generate\n    Returns:\n        pd.Series: Generated values\n    \"\"\"\n    self.logger.debug(f\"Generating chunk of {count} values\")\n    # Use the original generation logic\n    \"\"\"\n    Generate random values based on weighted choices.\n\n    Args:\n        count: Number of values to generate\n\n    Returns:\n        Series of chosen values\n    \"\"\"\n    choices_dict = self.params[\"choices\"]\n\n    # Extract choices and weights\n    choices = list(choices_dict.keys())\n    weights = list(choices_dict.values())\n\n    # Generate random choices based on weights\n    values = []\n\n    # Calculate total weight\n    total_weight = sum(weights)\n\n    # Generate values proportionally based on weights\n    for choice, weight in choices_dict.items():\n        # Calculate how many values this choice should get\n        proportion = weight / total_weight\n        num_values = int(proportion * count)\n\n        # Add the values for this choice\n        for _ in range(num_values):\n            values.append(choice)\n\n    # Handle any remaining values due to rounding\n    while len(values) &lt; count:\n        # Add random choice based on weights\n        choice = np.random.choice(choices, p=[w / total_weight for w in weights])\n        values.append(choice)\n\n    return pd.Series(values, dtype=object)\n</code></pre>"},{"location":"reference/strategies/distributed_choice_strategy/#core.strategies.distributed_choice_strategy.DistributedChoiceStrategy.generate_data","title":"<code>generate_data(count)</code>","text":"<p>Generate data by calling generate_chunk. This ensures consistent behavior between batch and non-batch modes. Args:     count: Number of values to generate Returns:     pd.Series: Generated values</p> Source code in <code>core/strategies/distributed_choice_strategy.py</code> <pre><code>def generate_data(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Generate data by calling generate_chunk.\n    This ensures consistent behavior between batch and non-batch modes.\n    Args:\n        count: Number of values to generate\n    Returns:\n        pd.Series: Generated values\n    \"\"\"\n    self.logger.debug(\n        f\"Generating {count} values using unified chunk-based approach\"\n    )\n    # For non-streaming mode, reset state to ensure consistent behavior\n    if not self.is_streaming_and_batch():\n        self.reset_state()\n    # Generate the chunk\n    result = self.generate_chunk(count)\n    return result\n</code></pre>"},{"location":"reference/strategies/distributed_choice_strategy/#core.strategies.distributed_choice_strategy.DistributedChoiceStrategy.reset_state","title":"<code>reset_state()</code>","text":"<p>Reset the internal state to initial values</p> Source code in <code>core/strategies/distributed_choice_strategy.py</code> <pre><code>def reset_state(self):\n    \"\"\"Reset the internal state to initial values\"\"\"\n    self.logger.debug(\"Resetting DistributedChoiceStrategy state\")\n    self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/distributed_date_range_strategy/","title":"distributed_date_range_strategy","text":""},{"location":"reference/strategies/distributed_date_range_strategy/#core.strategies.distributed_date_range_strategy","title":"<code>core.strategies.distributed_date_range_strategy</code>","text":"<p>Distributed date range strategy for generating date values from multiple weighted date ranges.</p> <p>This strategy has been refactored to use the new mixin pattern, reducing boilerplate code by ~40 lines while maintaining the same functionality.</p>"},{"location":"reference/strategies/distributed_date_range_strategy/#core.strategies.distributed_date_range_strategy.DistributedDateRangeStrategy","title":"<code>DistributedDateRangeStrategy</code>","text":"<p>               Bases: <code>BaseStrategy</code>, <code>SeedMixin</code>, <code>StatefulMixin</code>, <code>ValidationMixin</code>, <code>DateTimeMixin</code></p> <p>Strategy for generating random date values from multiple weighted date ranges.</p> <p>Output values are strings formatted per each range's <code>output_format</code>.</p> <p>Uses mixins for: - SeedMixin: Automatic seed validation and random state initialization - StatefulMixin: Standardized state management and reporting - ValidationMixin: Common parameter validation patterns - DateTimeMixin: Specialized datetime validation and utilities</p> Source code in <code>core/strategies/distributed_date_range_strategy.py</code> <pre><code>class DistributedDateRangeStrategy(\n    BaseStrategy, SeedMixin, StatefulMixin, ValidationMixin, DateTimeMixin\n):\n    \"\"\"\n    Strategy for generating random date values from multiple weighted date ranges.\n\n    Output values are strings formatted per each range's `output_format`.\n\n    Uses mixins for:\n    - SeedMixin: Automatic seed validation and random state initialization\n    - StatefulMixin: Standardized state management and reporting\n    - ValidationMixin: Common parameter validation patterns\n    - DateTimeMixin: Specialized datetime validation and utilities\n    \"\"\"\n\n    def _generate_random_date_in_range(self, range_item: DateRangeItem) -&gt; str:\n        \"\"\"Generate a single random date within the specified range\"\"\"\n        start_date = datetime.strptime(range_item.start_date, range_item.format)\n        end_date = datetime.strptime(range_item.end_date, range_item.format)\n\n        # Calculate the total number of days between start and end\n        total_days = (end_date - start_date).days\n\n        # Generate a random number of days to add to start_date\n        random_days = random.randint(0, total_days)\n\n        # Calculate the random date\n        random_date = start_date + timedelta(days=random_days)\n\n        # Format the date according to output_format\n        return random_date.strftime(range_item.output_format)\n\n    def __init__(self, mode: str, logger=None, **kwargs):\n        \"\"\"Initialize the strategy with configuration parameters\"\"\"\n        super().__init__(mode=mode, logger=logger, **kwargs)\n\n        # Use mixins for common functionality\n        self._validate_seed()  # From SeedMixin\n        self._initialize_random_seed()  # From SeedMixin\n        self._initialize_state()  # From StatefulMixin\n        # Validation is handled by config via factory\n\n    def _initialize_state(self):\n        \"\"\"Initialize internal state for stateful generation\"\"\"\n        super()._initialize_state()  # Call StatefulMixin's _initialize_state first\n\n        # Seed initialization is handled by SeedMixin\n        self.logger.debug(\n            f\"DistributedDateRangeStrategy initialized with seed={self._seed}\"\n        )\n\n    def generate_chunk(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Generate a chunk of data maintaining internal state.\n        This method is stateful and maintains consistent random sequence.\n        Args:\n            count: Number of values to generate\n        Returns:\n            pd.Series: Generated date strings (formatted using each range's `output_format`)\n        \"\"\"\n        self.logger.debug(f\"Generating chunk of {count} values\")\n        # Use the original generation logic\n        \"\"\"\n        Generate random date values from multiple weighted date ranges.\n        Args:\n            count: Number of values to generate\n        Returns:\n            Series of random date values\n        \"\"\"\n        ranges = self.params[\"ranges\"]\n\n        # Calculate the number of values to generate from each range\n        distributions = [r.distribution for r in ranges]\n        total_dist = sum(distributions)\n\n        # Normalize distributions\n        normalized_dist = [d / total_dist for d in distributions]\n\n        # Calculate counts for each range\n        range_counts = np.random.multinomial(count, normalized_dist)\n\n        # Generate values for each range\n        all_values = []\n        for i, range_item in enumerate(ranges):\n            range_count = range_counts[i]\n            if range_count == 0:\n                continue\n\n            # Generate date values for this range\n            for _ in range(range_count):\n                date_value = self._generate_random_date_in_range(range_item)\n                all_values.append(date_value)\n\n        # Return as strings per tests expectations\n        return pd.Series(all_values)\n\n    def reset_state(self):\n        \"\"\"Reset the internal state to initial values\"\"\"\n        self.logger.debug(\"Resetting DistributedDateRangeStrategy state\")\n        self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/distributed_date_range_strategy/#core.strategies.distributed_date_range_strategy.DistributedDateRangeStrategy.generate_chunk","title":"<code>generate_chunk(count)</code>","text":"<p>Generate a chunk of data maintaining internal state. This method is stateful and maintains consistent random sequence. Args:     count: Number of values to generate Returns:     pd.Series: Generated date strings (formatted using each range's <code>output_format</code>)</p> Source code in <code>core/strategies/distributed_date_range_strategy.py</code> <pre><code>def generate_chunk(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Generate a chunk of data maintaining internal state.\n    This method is stateful and maintains consistent random sequence.\n    Args:\n        count: Number of values to generate\n    Returns:\n        pd.Series: Generated date strings (formatted using each range's `output_format`)\n    \"\"\"\n    self.logger.debug(f\"Generating chunk of {count} values\")\n    # Use the original generation logic\n    \"\"\"\n    Generate random date values from multiple weighted date ranges.\n    Args:\n        count: Number of values to generate\n    Returns:\n        Series of random date values\n    \"\"\"\n    ranges = self.params[\"ranges\"]\n\n    # Calculate the number of values to generate from each range\n    distributions = [r.distribution for r in ranges]\n    total_dist = sum(distributions)\n\n    # Normalize distributions\n    normalized_dist = [d / total_dist for d in distributions]\n\n    # Calculate counts for each range\n    range_counts = np.random.multinomial(count, normalized_dist)\n\n    # Generate values for each range\n    all_values = []\n    for i, range_item in enumerate(ranges):\n        range_count = range_counts[i]\n        if range_count == 0:\n            continue\n\n        # Generate date values for this range\n        for _ in range(range_count):\n            date_value = self._generate_random_date_in_range(range_item)\n            all_values.append(date_value)\n\n    # Return as strings per tests expectations\n    return pd.Series(all_values)\n</code></pre>"},{"location":"reference/strategies/distributed_date_range_strategy/#core.strategies.distributed_date_range_strategy.DistributedDateRangeStrategy.reset_state","title":"<code>reset_state()</code>","text":"<p>Reset the internal state to initial values</p> Source code in <code>core/strategies/distributed_date_range_strategy.py</code> <pre><code>def reset_state(self):\n    \"\"\"Reset the internal state to initial values\"\"\"\n    self.logger.debug(\"Resetting DistributedDateRangeStrategy state\")\n    self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/distributed_number_range_strategy/","title":"distributed_number_range_strategy","text":""},{"location":"reference/strategies/distributed_number_range_strategy/#core.strategies.distributed_number_range_strategy","title":"<code>core.strategies.distributed_number_range_strategy</code>","text":"<p>Distributed number range strategy for generating numeric values from multiple weighted ranges.</p> <p>This strategy has been refactored to use the new mixin pattern, reducing boilerplate code by ~40 lines while maintaining the same functionality.</p>"},{"location":"reference/strategies/distributed_number_range_strategy/#core.strategies.distributed_number_range_strategy.DistributedNumberRangeStrategy","title":"<code>DistributedNumberRangeStrategy</code>","text":"<p>               Bases: <code>BaseStrategy</code>, <code>SeedMixin</code>, <code>StatefulMixin</code>, <code>ValidationMixin</code>, <code>NumericMixin</code></p> <p>Strategy for generating random numbers from multiple weighted ranges.</p> <p>Uses mixins for: - SeedMixin: Automatic seed validation and random state initialization - StatefulMixin: Standardized state management and reporting - ValidationMixin: Common parameter validation patterns - NumericMixin: Specialized numeric validation and utilities</p> Source code in <code>core/strategies/distributed_number_range_strategy.py</code> <pre><code>class DistributedNumberRangeStrategy(\n    BaseStrategy, SeedMixin, StatefulMixin, ValidationMixin, NumericMixin\n):\n    \"\"\"\n    Strategy for generating random numbers from multiple weighted ranges.\n\n    Uses mixins for:\n    - SeedMixin: Automatic seed validation and random state initialization\n    - StatefulMixin: Standardized state management and reporting\n    - ValidationMixin: Common parameter validation patterns\n    - NumericMixin: Specialized numeric validation and utilities\n    \"\"\"\n\n    def __init__(self, mode: str, logger=None, **kwargs):\n        \"\"\"Initialize the strategy with configuration parameters\"\"\"\n        super().__init__(mode=mode, logger=logger, **kwargs)\n\n        # Use mixins for common functionality\n        self._validate_seed()  # From SeedMixin\n        self._initialize_random_seed()  # From SeedMixin\n        self._initialize_state()  # From StatefulMixin\n        # Validation is handled by config via factory\n\n    def _initialize_state(self):\n        \"\"\"Initialize internal state for stateful generation\"\"\"\n        super()._initialize_state()  # Call StatefulMixin's _initialize_state first\n\n        # Seed initialization is handled by SeedMixin\n        self.logger.debug(\n            f\"DistributedNumberRangeStrategy initialized with seed={self._seed}\"\n        )\n\n    def generate_chunk(self, count: int) -&gt; pd.Series:\n        \"\"\"\n\n\n        Generate a chunk of data maintaining internal state.\n\n\n        This method is stateful and maintains consistent random sequence.\n\n\n\n        Args:\n\n\n            count: Number of values to generate\n\n\n\n        Returns:\n\n\n            pd.Series: Generated values\n\n\n        \"\"\"\n\n        self.logger.debug(f\"Generating chunk of {count} values\")\n\n        # Use the original generation logic\n        \"\"\"\n        Generate random numbers from multiple weighted ranges.\n\n        Args:\n            count: Number of values to generate\n\n        Returns:\n            Series of random numbers\n        \"\"\"\n        ranges = self.params[\"ranges\"]\n\n        # Calculate the number of values to generate from each range\n        distributions = [r.distribution for r in ranges]\n        total_dist = sum(distributions)\n\n        # Normalize distributions\n        normalized_dist = [d / total_dist for d in distributions]\n\n        # Calculate counts for each range\n        range_counts = np.random.multinomial(count, normalized_dist)\n\n        # Generate values for each range\n        all_values = []\n        for i, range_item in enumerate(ranges):\n            range_count = range_counts[i]\n            if range_count == 0:\n                continue\n\n            lb = range_item.start\n            ub = range_item.end\n\n            # Handle integer vs float generation\n            if isinstance(lb, int) and isinstance(ub, int):\n                # Generate integers\n                values = np.random.randint(lb, ub + 1, size=range_count).tolist()\n            else:\n                # Generate floats\n                values = np.random.uniform(lb, ub, size=range_count).tolist()\n\n            all_values.extend(values)\n\n        # Shuffle the values to mix ranges\n        np.random.shuffle(all_values)\n\n        return pd.Series(all_values, dtype=float)\n\n    def reset_state(self):\n        \"\"\"Reset the internal state to initial values\"\"\"\n\n        self.logger.debug(\"Resetting DistributedNumberRangeStrategy state\")\n\n        self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/distributed_number_range_strategy/#core.strategies.distributed_number_range_strategy.DistributedNumberRangeStrategy.generate_chunk","title":"<code>generate_chunk(count)</code>","text":"<p>Generate a chunk of data maintaining internal state.</p> <p>This method is stateful and maintains consistent random sequence.</p> <p>Args:</p> <pre><code>count: Number of values to generate\n</code></pre> <p>Returns:</p> <pre><code>pd.Series: Generated values\n</code></pre> Source code in <code>core/strategies/distributed_number_range_strategy.py</code> <pre><code>def generate_chunk(self, count: int) -&gt; pd.Series:\n    \"\"\"\n\n\n    Generate a chunk of data maintaining internal state.\n\n\n    This method is stateful and maintains consistent random sequence.\n\n\n\n    Args:\n\n\n        count: Number of values to generate\n\n\n\n    Returns:\n\n\n        pd.Series: Generated values\n\n\n    \"\"\"\n\n    self.logger.debug(f\"Generating chunk of {count} values\")\n\n    # Use the original generation logic\n    \"\"\"\n    Generate random numbers from multiple weighted ranges.\n\n    Args:\n        count: Number of values to generate\n\n    Returns:\n        Series of random numbers\n    \"\"\"\n    ranges = self.params[\"ranges\"]\n\n    # Calculate the number of values to generate from each range\n    distributions = [r.distribution for r in ranges]\n    total_dist = sum(distributions)\n\n    # Normalize distributions\n    normalized_dist = [d / total_dist for d in distributions]\n\n    # Calculate counts for each range\n    range_counts = np.random.multinomial(count, normalized_dist)\n\n    # Generate values for each range\n    all_values = []\n    for i, range_item in enumerate(ranges):\n        range_count = range_counts[i]\n        if range_count == 0:\n            continue\n\n        lb = range_item.start\n        ub = range_item.end\n\n        # Handle integer vs float generation\n        if isinstance(lb, int) and isinstance(ub, int):\n            # Generate integers\n            values = np.random.randint(lb, ub + 1, size=range_count).tolist()\n        else:\n            # Generate floats\n            values = np.random.uniform(lb, ub, size=range_count).tolist()\n\n        all_values.extend(values)\n\n    # Shuffle the values to mix ranges\n    np.random.shuffle(all_values)\n\n    return pd.Series(all_values, dtype=float)\n</code></pre>"},{"location":"reference/strategies/distributed_number_range_strategy/#core.strategies.distributed_number_range_strategy.DistributedNumberRangeStrategy.reset_state","title":"<code>reset_state()</code>","text":"<p>Reset the internal state to initial values</p> Source code in <code>core/strategies/distributed_number_range_strategy.py</code> <pre><code>def reset_state(self):\n    \"\"\"Reset the internal state to initial values\"\"\"\n\n    self.logger.debug(\"Resetting DistributedNumberRangeStrategy state\")\n\n    self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/distributed_time_range_strategy/","title":"distributed_time_range_strategy","text":""},{"location":"reference/strategies/distributed_time_range_strategy/#core.strategies.distributed_time_range_strategy","title":"<code>core.strategies.distributed_time_range_strategy</code>","text":"<p>Distributed time range strategy for generating time values from multiple weighted time ranges.</p> <p>This strategy has been refactored to use the new mixin pattern, reducing boilerplate code by ~40 lines while maintaining the same functionality.</p>"},{"location":"reference/strategies/distributed_time_range_strategy/#core.strategies.distributed_time_range_strategy.DistributedTimeRangeStrategy","title":"<code>DistributedTimeRangeStrategy</code>","text":"<p>               Bases: <code>BaseStrategy</code>, <code>SeedMixin</code>, <code>StatefulMixin</code>, <code>ValidationMixin</code>, <code>DateTimeMixin</code></p> <p>Strategy for generating random time values from multiple weighted time ranges. Output values are strings formatted per each range's <code>format</code>.</p> Source code in <code>core/strategies/distributed_time_range_strategy.py</code> <pre><code>class DistributedTimeRangeStrategy(\n    BaseStrategy, SeedMixin, StatefulMixin, ValidationMixin, DateTimeMixin\n):\n    \"\"\"\n    Strategy for generating random time values from multiple weighted time ranges.\n    Output values are strings formatted per each range's `format`.\n\n    \"\"\"\n\n    # Validation handled via DistributedTimeRangeConfig in factory\n\n    def _time_to_seconds(self, time_str: str, format_str: str) -&gt; int:\n        \"\"\"Convert time string to seconds since midnight\"\"\"\n        time_obj = datetime.strptime(time_str, format_str).time()\n        return time_obj.hour * 3600 + time_obj.minute * 60 + time_obj.second\n\n    def _seconds_to_time_str(self, seconds: int, format_str: str) -&gt; str:\n        \"\"\"Convert seconds since midnight to time string\"\"\"\n        # Handle overflow (next day)\n        seconds = seconds % (24 * 3600)\n        hours = seconds // 3600\n        minutes = (seconds % 3600) // 60\n        secs = seconds % 60\n        time_obj = time(hours, minutes, secs)\n        return time_obj.strftime(format_str)\n\n    def _generate_random_time_in_range(self, range_item: TimeRangeItem) -&gt; str:\n        \"\"\"Generate a single random time within the specified range\"\"\"\n        start_seconds = self._time_to_seconds(range_item.start, range_item.format)\n        end_seconds = self._time_to_seconds(range_item.end, range_item.format)\n\n        # Handle overnight ranges (e.g., 22:00:00 to 06:00:00)\n        if end_seconds &lt;= start_seconds:\n            # This is an overnight range; randomly choose segment using numpy RNG\n            if np.random.rand() &lt; 0.5:\n                # Before midnight: start_seconds to 24*3600 - 1\n                random_seconds = int(np.random.randint(start_seconds, 24 * 3600))\n            else:\n                # After midnight: 0 to end_seconds\n                random_seconds = int(np.random.randint(0, end_seconds + 1))\n        else:\n            # Normal range within the same day (inclusive of end)\n            random_seconds = int(np.random.randint(start_seconds, end_seconds + 1))\n\n        return self._seconds_to_time_str(random_seconds, range_item.format)\n\n    def __init__(self, mode: str, logger=None, **kwargs):\n        \"\"\"Initialize the strategy with configuration parameters\"\"\"\n        super().__init__(mode=mode, logger=logger, **kwargs)\n\n        # Use mixins for common functionality\n        self._validate_seed()  # From SeedMixin\n        self._initialize_random_seed()  # From SeedMixin\n        self._initialize_state()  # From StatefulMixin\n        # Validation is handled by config via factory\n\n    def _initialize_state(self):\n        \"\"\"Initialize internal state for stateful generation\"\"\"\n        super()._initialize_state()  # Call StatefulMixin's _initialize_state first\n\n        # Seed initialization is handled by SeedMixin\n        self.logger.debug(\n            f\"DistributedTimeRangeStrategy initialized with seed={self._seed}\"\n        )\n\n    def generate_chunk(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Generate a chunk of data maintaining internal state.\n\n        This method is stateful and maintains consistent random sequence.\n\n        Args:\n            count: Number of values to generate\n\n        Returns:\n            pd.Series: Generated time strings (formatted per range `format`)\n        \"\"\"\n\n        self.logger.debug(f\"Generating chunk of {count} values\")\n\n        # Use the original generation logic\n        \"\"\"\n        Generate random time values from multiple weighted time ranges.\n\n        Args:\n            count: Number of values to generate\n\n        Returns:\n            Series of random time values\n        \"\"\"\n        ranges = self.params[\"ranges\"]\n\n        # Calculate the number of values to generate from each range\n        distributions = [r.distribution for r in ranges]\n        total_dist = sum(distributions)\n\n        # Normalize distributions\n        normalized_dist = [d / total_dist for d in distributions]\n\n        # Calculate counts for each range\n        range_counts = np.random.multinomial(count, normalized_dist)\n\n        # Generate values for each range\n        all_values = []\n        for i, range_item in enumerate(ranges):\n            range_count = range_counts[i]\n            if range_count == 0:\n                continue\n\n            # Generate time values for this range\n            for _ in range(range_count):\n                time_value = self._generate_random_time_in_range(range_item)\n                all_values.append(time_value)\n\n        # Return as strings per tests expectations, preserving deterministic order\n        return pd.Series(all_values)\n\n    def reset_state(self):\n        \"\"\"Reset the internal state to initial values\"\"\"\n\n        self.logger.debug(\"Resetting DistributedTimeRangeStrategy state\")\n\n        self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/distributed_time_range_strategy/#core.strategies.distributed_time_range_strategy.DistributedTimeRangeStrategy.generate_chunk","title":"<code>generate_chunk(count)</code>","text":"<p>Generate a chunk of data maintaining internal state.</p> <p>This method is stateful and maintains consistent random sequence.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of values to generate</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Generated time strings (formatted per range <code>format</code>)</p> Source code in <code>core/strategies/distributed_time_range_strategy.py</code> <pre><code>def generate_chunk(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Generate a chunk of data maintaining internal state.\n\n    This method is stateful and maintains consistent random sequence.\n\n    Args:\n        count: Number of values to generate\n\n    Returns:\n        pd.Series: Generated time strings (formatted per range `format`)\n    \"\"\"\n\n    self.logger.debug(f\"Generating chunk of {count} values\")\n\n    # Use the original generation logic\n    \"\"\"\n    Generate random time values from multiple weighted time ranges.\n\n    Args:\n        count: Number of values to generate\n\n    Returns:\n        Series of random time values\n    \"\"\"\n    ranges = self.params[\"ranges\"]\n\n    # Calculate the number of values to generate from each range\n    distributions = [r.distribution for r in ranges]\n    total_dist = sum(distributions)\n\n    # Normalize distributions\n    normalized_dist = [d / total_dist for d in distributions]\n\n    # Calculate counts for each range\n    range_counts = np.random.multinomial(count, normalized_dist)\n\n    # Generate values for each range\n    all_values = []\n    for i, range_item in enumerate(ranges):\n        range_count = range_counts[i]\n        if range_count == 0:\n            continue\n\n        # Generate time values for this range\n        for _ in range(range_count):\n            time_value = self._generate_random_time_in_range(range_item)\n            all_values.append(time_value)\n\n    # Return as strings per tests expectations, preserving deterministic order\n    return pd.Series(all_values)\n</code></pre>"},{"location":"reference/strategies/distributed_time_range_strategy/#core.strategies.distributed_time_range_strategy.DistributedTimeRangeStrategy.reset_state","title":"<code>reset_state()</code>","text":"<p>Reset the internal state to initial values</p> Source code in <code>core/strategies/distributed_time_range_strategy.py</code> <pre><code>def reset_state(self):\n    \"\"\"Reset the internal state to initial values\"\"\"\n\n    self.logger.debug(\"Resetting DistributedTimeRangeStrategy state\")\n\n    self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/mapping_strategy/","title":"mapping_strategy","text":""},{"location":"reference/strategies/mapping_strategy/#core.strategies.mapping_strategy","title":"<code>core.strategies.mapping_strategy</code>","text":""},{"location":"reference/strategies/mapping_strategy/#core.strategies.mapping_strategy.MappingStrategy","title":"<code>MappingStrategy</code>","text":"<p>               Bases: <code>BaseStrategy</code>, <code>StatefulMixin</code>, <code>ValidationMixin</code></p> <p>Strategy that maps values from an existing source column to the target column using a provided mapping dictionary. Intended for use-cases like mapping <code>department_id</code> -&gt; <code>department_name</code>.</p> <p>Expected params: - map_from: name of source column in df to read keys from - mapping: dict of key -&gt; value (inline mode) - OR - source: path to CSV file containing mapping table (file mode) - source_column: target column name in the CSV to map to</p> Source code in <code>core/strategies/mapping_strategy.py</code> <pre><code>class MappingStrategy(BaseStrategy, StatefulMixin, ValidationMixin):\n    \"\"\"\n    Strategy that maps values from an existing source column to the target\n    column using a provided mapping dictionary. Intended for use-cases like\n    mapping `department_id` -&gt; `department_name`.\n\n    Expected params:\n    - map_from: name of source column in df to read keys from\n    - mapping: dict of key -&gt; value (inline mode)\n    - OR\n    - source: path to CSV file containing mapping table (file mode)\n    - source_column: target column name in the CSV to map to\n    \"\"\"\n\n    def __init__(self, mode: str, logger=None, **kwargs):\n        super().__init__(mode=mode, logger=logger, **kwargs)\n        self._initialize_state()\n\n    def _initialize_state(self):\n        super()._initialize_state()\n        self._map_from: str = self.params.get(\"map_from\", \"\")\n\n        raw_mapping = self.params.get(\"mapping\", None)\n        self._mapping_dict: dict = (\n            dict(raw_mapping) if isinstance(raw_mapping, dict) else {}\n        )\n\n        self._source_path: str | None = self.params.get(\"source\")\n        self._source_column: str | None = self.params.get(\"source_column\")\n        self._source_map_from: str | None = self.params.get(\"source_map_from\")\n\n        self._mode: str = \"MAPPING\"\n\n        if self._source_path and self._source_column:\n            try:\n                import os\n\n                if not os.path.isfile(self._source_path):\n                    self.logger.warning(\n                        f\"MappingStrategy: source file '{self._source_path}' does not exist\"\n                    )\n                else:\n                    src_df = self._read_mapping_source(self._source_path)\n                    join_key = self._source_map_from or self._map_from\n                    if (\n                        join_key not in src_df.columns\n                        or self._source_column not in src_df.columns\n                    ):\n                        self.logger.warning(\n                            f\"MappingStrategy: required columns not found in source file. \"\n                            f\"Needed: '{join_key}', '{self._source_column}'. Found: {list(src_df.columns)}\"\n                        )\n                    else:\n                        # Build mapping from file\n                        self._mapping_dict = (\n                            src_df[[join_key, self._source_column]]\n                            .dropna(subset=[join_key])\n                            .set_index(join_key)[self._source_column]\n                            .to_dict()\n                        )\n                        self._mode: str = \"FILE\"\n            except Exception as e:\n                self.logger.warning(\n                    f\"MappingStrategy: failed to load mapping from file: {e}\"\n                )\n\n        self.logger.debug(\n            f\"MappingStrategy initialized: map_from='{self._map_from}', \"\n            f\"target='{self.col_name}', mapping_size={len(self._mapping_dict)}, mode={self._mode}\"\n        )\n\n    def _read_mapping_source(self, path: str) -&gt; pd.DataFrame:\n        \"\"\"Read mapping source file based on extension (csv, json, parquet, xlsx/xls).\"\"\"\n        lower = path.lower()\n        if lower.endswith(\".csv\"):\n            return pd.read_csv(path)\n        if lower.endswith(\".json\"):\n            return pd.read_json(path)\n        if lower.endswith(\".parquet\"):\n            return pd.read_parquet(path)\n        if lower.endswith(\".xlsx\") or lower.endswith(\".xls\"):\n            return pd.read_excel(path)\n        # Fallback: attempt csv for unknown extension\n        self.logger.debug(\n            f\"MappingStrategy: unknown extension for '{path}', attempting CSV reader\"\n        )\n        return pd.read_csv(path)\n\n    def generate_chunk(self, count: int) -&gt; pd.Series:\n        self.logger.debug(\n            f\"Generating mapping for {count} rows from '{self._map_from}' to '{self.col_name}'\"\n        )\n\n        if (\n            self.df is None\n            or not self._map_from\n            or self._map_from not in self.df.columns\n        ):\n            self.logger.warning(\n                \"MappingStrategy: df or source column missing; returning NaNs\"\n            )\n            return pd.Series([None] * count, dtype=object)\n\n        src = self.df[self._map_from].head(count).rename(None)\n        mapped = src.map(self._mapping_dict, na_action=\"ignore\")\n\n        # Preserve existing target values; only overwrite where mapping produced a value\n        if self.col_name in self.df.columns:\n            existing = self.df[self.col_name].head(count)\n            result = mapped.combine_first(existing)\n        else:\n            result = mapped\n\n        return result.astype(object)\n\n    def reset_state(self):\n        # Ensure internal state (including mapping dict and map_from) reflects\n        # the latest params when this pooled strategy instance is reused.\n        super().reset_state()\n        self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/pattern_strategy/","title":"pattern_strategy","text":""},{"location":"reference/strategies/pattern_strategy/#core.strategies.pattern_strategy","title":"<code>core.strategies.pattern_strategy</code>","text":"<p>Pattern strategy for generating random strings matching a regex or pattern.</p> <p>This strategy has been refactored to use the new mixin pattern, reducing boilerplate code by ~30 lines while maintaining the same functionality.</p>"},{"location":"reference/strategies/pattern_strategy/#core.strategies.pattern_strategy.PatternStrategy","title":"<code>PatternStrategy</code>","text":"<p>               Bases: <code>BaseStrategy</code>, <code>SeedMixin</code>, <code>StatefulMixin</code>, <code>ValidationMixin</code></p> <p>Strategy for generating random strings matching a regex or pattern. Supports stateful generation with consistent random state.</p> <p>Uses mixins for: - SeedMixin: Automatic seed validation and random state initialization - StatefulMixin: Standardized state management and reporting - ValidationMixin: Common parameter validation patterns</p> Source code in <code>core/strategies/pattern_strategy.py</code> <pre><code>class PatternStrategy(BaseStrategy, SeedMixin, StatefulMixin, ValidationMixin):\n    \"\"\"\n    Strategy for generating random strings matching a regex or pattern.\n    Supports stateful generation with consistent random state.\n\n    Uses mixins for:\n    - SeedMixin: Automatic seed validation and random state initialization\n    - StatefulMixin: Standardized state management and reporting\n    - ValidationMixin: Common parameter validation patterns\n    \"\"\"\n\n    def __init__(self, mode: str, logger=None, **kwargs):\n        \"\"\"Initialize the strategy with configuration parameters\"\"\"\n        super().__init__(mode=mode, logger=logger, **kwargs)\n\n        # Use mixins for common functionality\n        self._validate_seed()  # From SeedMixin\n        self._initialize_random_seed()  # From SeedMixin\n        self._initialize_state()  # From StatefulMixin\n        # Validation is handled by config via factory\n\n    def _initialize_state(self):\n        \"\"\"Initialize internal state for stateful generation\"\"\"\n        super()._initialize_state()  # Call StatefulMixin's _initialize_state first\n\n        # Store pattern for efficient access\n        self._pattern = self.params[\"regex\"]\n        self._unique_values = set() if self.unique else None\n\n        self.logger.debug(\n            f\"PatternStrategy initialized with pattern='{self._pattern}', \"\n            f\"unique={self.unique}, seed={self._seed}\"\n        )\n\n    def generate_chunk(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Generate a chunk of random strings matching the pattern maintaining internal state.\n        This method is stateful and maintains consistent random sequence and uniqueness.\n\n        Args:\n            count: Number of strings to generate\n\n        Returns:\n            pd.Series: Generated strings\n        \"\"\"\n        self.logger.debug(\n            f\"Generating chunk of {count} strings matching pattern '{self._pattern}'\"\n        )\n\n        max_attempts = count * 3  # tries 3 times more than the count\n        attempts = 0\n\n        if self.unique and self._unique_values is not None:\n            result = []\n            while len(result) &lt; count and attempts &lt; max_attempts:\n                attempts += 1\n                value = rstr.xeger(self._pattern)\n\n                # Check if it's unique\n                if value not in self._unique_values:\n                    self._unique_values.add(value)\n                    result.append(value)\n\n            # If we couldn't generate enough unique values, pad with existing ones\n            if len(result) &lt; count:\n                remaining = count - len(result)\n                existing_values = list(self._unique_values)\n                if existing_values:\n                    padding = [\n                        np.random.choice(existing_values) for _ in range(remaining)\n                    ]\n                    result.extend(padding)\n                else:\n                    # Fallback: generate non-unique values\n                    padding = [rstr.xeger(self._pattern) for _ in range(remaining)]\n                    result.extend(padding)\n        else:\n            result = [rstr.xeger(self._pattern) for _ in range(count)]\n\n        return pd.Series(result, dtype=str)\n\n    def reset_state(self):\n        \"\"\"Reset the internal state to initial values\"\"\"\n        super().reset_state()  # Call StatefulMixin's reset_state\n        # Re-initialize random state with original seed\n        if self._seed is not None:\n            random.seed(self._seed)\n            np.random.seed(self._seed)\n        # Re-derive any cached fields from current params for pooled reuse\n        self._initialize_state()\n\n    def get_current_state(self) -&gt; dict:\n        \"\"\"Get current state information for debugging\"\"\"\n        state = super().get_current_state()  # Get base state from StatefulMixin\n        state.update(\n            {\n                \"pattern\": self._pattern,\n                \"unique\": self.unique,\n                \"unique_count\": len(self._unique_values) if self._unique_values else 0,\n            }\n        )\n        return state\n</code></pre>"},{"location":"reference/strategies/pattern_strategy/#core.strategies.pattern_strategy.PatternStrategy.generate_chunk","title":"<code>generate_chunk(count)</code>","text":"<p>Generate a chunk of random strings matching the pattern maintaining internal state. This method is stateful and maintains consistent random sequence and uniqueness.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of strings to generate</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Generated strings</p> Source code in <code>core/strategies/pattern_strategy.py</code> <pre><code>def generate_chunk(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Generate a chunk of random strings matching the pattern maintaining internal state.\n    This method is stateful and maintains consistent random sequence and uniqueness.\n\n    Args:\n        count: Number of strings to generate\n\n    Returns:\n        pd.Series: Generated strings\n    \"\"\"\n    self.logger.debug(\n        f\"Generating chunk of {count} strings matching pattern '{self._pattern}'\"\n    )\n\n    max_attempts = count * 3  # tries 3 times more than the count\n    attempts = 0\n\n    if self.unique and self._unique_values is not None:\n        result = []\n        while len(result) &lt; count and attempts &lt; max_attempts:\n            attempts += 1\n            value = rstr.xeger(self._pattern)\n\n            # Check if it's unique\n            if value not in self._unique_values:\n                self._unique_values.add(value)\n                result.append(value)\n\n        # If we couldn't generate enough unique values, pad with existing ones\n        if len(result) &lt; count:\n            remaining = count - len(result)\n            existing_values = list(self._unique_values)\n            if existing_values:\n                padding = [\n                    np.random.choice(existing_values) for _ in range(remaining)\n                ]\n                result.extend(padding)\n            else:\n                # Fallback: generate non-unique values\n                padding = [rstr.xeger(self._pattern) for _ in range(remaining)]\n                result.extend(padding)\n    else:\n        result = [rstr.xeger(self._pattern) for _ in range(count)]\n\n    return pd.Series(result, dtype=str)\n</code></pre>"},{"location":"reference/strategies/pattern_strategy/#core.strategies.pattern_strategy.PatternStrategy.get_current_state","title":"<code>get_current_state()</code>","text":"<p>Get current state information for debugging</p> Source code in <code>core/strategies/pattern_strategy.py</code> <pre><code>def get_current_state(self) -&gt; dict:\n    \"\"\"Get current state information for debugging\"\"\"\n    state = super().get_current_state()  # Get base state from StatefulMixin\n    state.update(\n        {\n            \"pattern\": self._pattern,\n            \"unique\": self.unique,\n            \"unique_count\": len(self._unique_values) if self._unique_values else 0,\n        }\n    )\n    return state\n</code></pre>"},{"location":"reference/strategies/pattern_strategy/#core.strategies.pattern_strategy.PatternStrategy.reset_state","title":"<code>reset_state()</code>","text":"<p>Reset the internal state to initial values</p> Source code in <code>core/strategies/pattern_strategy.py</code> <pre><code>def reset_state(self):\n    \"\"\"Reset the internal state to initial values\"\"\"\n    super().reset_state()  # Call StatefulMixin's reset_state\n    # Re-initialize random state with original seed\n    if self._seed is not None:\n        random.seed(self._seed)\n        np.random.seed(self._seed)\n    # Re-derive any cached fields from current params for pooled reuse\n    self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/random_date_range_strategy/","title":"random_date_range_strategy","text":""},{"location":"reference/strategies/random_date_range_strategy/#core.strategies.random_date_range_strategy","title":"<code>core.strategies.random_date_range_strategy</code>","text":"<p>Random date range strategy for generating random dates within a range.</p> <p>This strategy uses the mixin pattern to reduce boilerplate while maintaining functionality.</p>"},{"location":"reference/strategies/random_date_range_strategy/#core.strategies.random_date_range_strategy.RandomDateRangeStrategy","title":"<code>RandomDateRangeStrategy</code>","text":"<p>               Bases: <code>BaseStrategy</code>, <code>SeedMixin</code>, <code>StatefulMixin</code>, <code>ValidationMixin</code></p> <p>Strategy for generating random dates within a specified range.</p> <p>Uses mixins for: - SeedMixin: Automatic seed validation and random state initialization - StatefulMixin: Standardized state management and reporting - ValidationMixin: Common parameter validation patterns</p> Source code in <code>core/strategies/random_date_range_strategy.py</code> <pre><code>class RandomDateRangeStrategy(BaseStrategy, SeedMixin, StatefulMixin, ValidationMixin):\n    \"\"\"\n    Strategy for generating random dates within a specified range.\n\n    Uses mixins for:\n    - SeedMixin: Automatic seed validation and random state initialization\n    - StatefulMixin: Standardized state management and reporting\n    - ValidationMixin: Common parameter validation patterns\n    \"\"\"\n\n    def __init__(self, mode: str, logger=None, **kwargs):\n        \"\"\"Initialize the strategy with configuration parameters\"\"\"\n        super().__init__(mode=mode, logger=logger, **kwargs)\n\n        # Use mixins for common functionality\n        self._validate_seed()  # From SeedMixin\n        self._initialize_random_seed()  # From SeedMixin\n        self._initialize_state()  # From StatefulMixin\n        # Validation is handled by config via factory\n\n    def _initialize_state(self):\n        \"\"\"Initialize internal state for stateful generation\"\"\"\n        super()._initialize_state()  # Call StatefulMixin's _initialize_state first\n\n        # Seed initialization is handled by SeedMixin._initialize_random_seed()\n        self.logger.debug(f\"RandomDateRangeStrategy initialized with seed={self._seed}\")\n\n    def generate_chunk(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Generate a chunk of data maintaining internal state.\n        This method is stateful and maintains consistent random sequence.\n        Args:\n            count: Number of values to generate\n        Returns:\n            pd.Series: Generated values\n        \"\"\"\n\n        self.logger.debug(f\"Generating chunk of {count} values\")\n        # Get date generation parameters\n        if isinstance(self.params[\"start_date\"], str):\n            start_date = datetime.strptime(\n                self.params[\"start_date\"], self.params[\"format\"]\n            )\n        else:\n            start_date = self.params[\"start_date\"]\n        if isinstance(self.params[\"end_date\"], str):\n            end_date = datetime.strptime(self.params[\"end_date\"], self.params[\"format\"])\n        else:\n            end_date = self.params[\"end_date\"]\n\n        params = {\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n        }\n\n        if \"output_format\" in self.params:\n            params[\"output_format\"] = self.params[\"output_format\"]\n        else:\n            params[\"output_format\"] = \"%Y-%m-%d\"\n\n        # Generate the dates\n        dates = [generate_random_date(**params) for _ in range(count)]\n        return pd.Series(dates)\n\n    def reset_state(self):\n        \"\"\"Reset the internal state to initial values\"\"\"\n        self.logger.debug(\"Resetting RandomDateRangeStrategy state\")\n        self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/random_date_range_strategy/#core.strategies.random_date_range_strategy.RandomDateRangeStrategy.generate_chunk","title":"<code>generate_chunk(count)</code>","text":"<p>Generate a chunk of data maintaining internal state. This method is stateful and maintains consistent random sequence. Args:     count: Number of values to generate Returns:     pd.Series: Generated values</p> Source code in <code>core/strategies/random_date_range_strategy.py</code> <pre><code>def generate_chunk(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Generate a chunk of data maintaining internal state.\n    This method is stateful and maintains consistent random sequence.\n    Args:\n        count: Number of values to generate\n    Returns:\n        pd.Series: Generated values\n    \"\"\"\n\n    self.logger.debug(f\"Generating chunk of {count} values\")\n    # Get date generation parameters\n    if isinstance(self.params[\"start_date\"], str):\n        start_date = datetime.strptime(\n            self.params[\"start_date\"], self.params[\"format\"]\n        )\n    else:\n        start_date = self.params[\"start_date\"]\n    if isinstance(self.params[\"end_date\"], str):\n        end_date = datetime.strptime(self.params[\"end_date\"], self.params[\"format\"])\n    else:\n        end_date = self.params[\"end_date\"]\n\n    params = {\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n    }\n\n    if \"output_format\" in self.params:\n        params[\"output_format\"] = self.params[\"output_format\"]\n    else:\n        params[\"output_format\"] = \"%Y-%m-%d\"\n\n    # Generate the dates\n    dates = [generate_random_date(**params) for _ in range(count)]\n    return pd.Series(dates)\n</code></pre>"},{"location":"reference/strategies/random_date_range_strategy/#core.strategies.random_date_range_strategy.RandomDateRangeStrategy.reset_state","title":"<code>reset_state()</code>","text":"<p>Reset the internal state to initial values</p> Source code in <code>core/strategies/random_date_range_strategy.py</code> <pre><code>def reset_state(self):\n    \"\"\"Reset the internal state to initial values\"\"\"\n    self.logger.debug(\"Resetting RandomDateRangeStrategy state\")\n    self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/random_name_strategy/","title":"random_name_strategy","text":""},{"location":"reference/strategies/random_name_strategy/#core.strategies.random_name_strategy","title":"<code>core.strategies.random_name_strategy</code>","text":"<p>Random name strategy for generating realistic person names using the names package.</p> <p>This strategy has been refactored to use the new mixin pattern, reducing boilerplate code by ~40 lines while maintaining the same functionality.</p>"},{"location":"reference/strategies/random_name_strategy/#core.strategies.random_name_strategy.RandomNameStrategy","title":"<code>RandomNameStrategy</code>","text":"<p>               Bases: <code>BaseStrategy</code>, <code>SeedMixin</code>, <code>StatefulMixin</code>, <code>ValidationMixin</code></p> <p>Strategy for generating random person names with configurable parameters. Supports first names, last names, full names, gender filtering, and case formatting. Supports stateful generation with consistent random state.</p> <p>Uses mixins for: - SeedMixin: Automatic seed validation and random state initialization - StatefulMixin: Standardized state management and reporting - ValidationMixin: Common parameter validation patterns</p> Source code in <code>core/strategies/random_name_strategy.py</code> <pre><code>class RandomNameStrategy(BaseStrategy, SeedMixin, StatefulMixin, ValidationMixin):\n    \"\"\"\n    Strategy for generating random person names with configurable parameters.\n    Supports first names, last names, full names, gender filtering, and case formatting.\n    Supports stateful generation with consistent random state.\n\n    Uses mixins for:\n    - SeedMixin: Automatic seed validation and random state initialization\n    - StatefulMixin: Standardized state management and reporting\n    - ValidationMixin: Common parameter validation patterns\n    \"\"\"\n\n    def __init__(self, mode, logger=None, **kwargs):\n        \"\"\"Initialize the strategy with configuration parameters\"\"\"\n        super().__init__(mode, logger, **kwargs)\n\n        # Use mixins for common functionality\n        self._validate_seed()  # From SeedMixin\n        self._initialize_random_seed()  # From SeedMixin\n        self._initialize_state()  # From StatefulMixin\n        # Validation is handled by config via factory\n\n    def sync_state(self, result: pd.Series):\n        pass\n\n    def _initialize_state(self):\n        \"\"\"Initialize internal state for stateful generation\"\"\"\n        super()._initialize_state()  # Call StatefulMixin's _initialize_state first\n\n        # Store parameters for efficient access\n        self._name_type = self.params[\"name_type\"]\n        self._gender = None if self.params[\"gender\"] == \"any\" else self.params[\"gender\"]\n        self._case_format = self.params[\"case\"]\n\n        self.logger.debug(\n            f\"RandomNameStrategy initialized with name_type={self._name_type}, \"\n            f\"gender={self._gender}, case={self._case_format}, seed={self._seed}\"\n        )\n\n    def generate_chunk(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Generate a chunk of random names maintaining internal random state.\n        This method is stateful and maintains consistent random sequence.\n\n        Args:\n            count: Number of names to generate\n\n        Returns:\n            pd.Series: Generated names\n        \"\"\"\n        self.logger.debug(\n            f\"Generating chunk of {count} names (type={self._name_type}, gender={self._gender})\"\n        )\n\n        result = []\n        for _ in range(count):\n            # Generate the name using the names package\n            name = get_name(name_type=self._name_type, gender=self._gender)\n\n            # Apply case formatting\n            formatted_name = apply_case_formatting(name, self._case_format)\n\n            result.append(formatted_name)\n\n        return pd.Series(result, dtype=str)\n\n    def reset_state(self):\n        \"\"\"Reset the internal random state to initial values\"\"\"\n        super().reset_state()  # Call StatefulMixin's reset_state\n        # Re-initialize random state with original seed\n        if self._seed is not None:\n            random.seed(self._seed)\n        # Re-derive any cached fields from current params for pooled reuse\n        self._initialize_state()\n\n    def get_current_state(self) -&gt; dict:\n        \"\"\"Get current state information for debugging\"\"\"\n        state = super().get_current_state()  # Get base state from StatefulMixin\n        state.update(\n            {\n                \"name_type\": self._name_type,\n                \"gender\": self._gender,\n                \"case_format\": self._case_format,\n            }\n        )\n        return state\n</code></pre>"},{"location":"reference/strategies/random_name_strategy/#core.strategies.random_name_strategy.RandomNameStrategy.generate_chunk","title":"<code>generate_chunk(count)</code>","text":"<p>Generate a chunk of random names maintaining internal random state. This method is stateful and maintains consistent random sequence.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of names to generate</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Generated names</p> Source code in <code>core/strategies/random_name_strategy.py</code> <pre><code>def generate_chunk(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Generate a chunk of random names maintaining internal random state.\n    This method is stateful and maintains consistent random sequence.\n\n    Args:\n        count: Number of names to generate\n\n    Returns:\n        pd.Series: Generated names\n    \"\"\"\n    self.logger.debug(\n        f\"Generating chunk of {count} names (type={self._name_type}, gender={self._gender})\"\n    )\n\n    result = []\n    for _ in range(count):\n        # Generate the name using the names package\n        name = get_name(name_type=self._name_type, gender=self._gender)\n\n        # Apply case formatting\n        formatted_name = apply_case_formatting(name, self._case_format)\n\n        result.append(formatted_name)\n\n    return pd.Series(result, dtype=str)\n</code></pre>"},{"location":"reference/strategies/random_name_strategy/#core.strategies.random_name_strategy.RandomNameStrategy.get_current_state","title":"<code>get_current_state()</code>","text":"<p>Get current state information for debugging</p> Source code in <code>core/strategies/random_name_strategy.py</code> <pre><code>def get_current_state(self) -&gt; dict:\n    \"\"\"Get current state information for debugging\"\"\"\n    state = super().get_current_state()  # Get base state from StatefulMixin\n    state.update(\n        {\n            \"name_type\": self._name_type,\n            \"gender\": self._gender,\n            \"case_format\": self._case_format,\n        }\n    )\n    return state\n</code></pre>"},{"location":"reference/strategies/random_name_strategy/#core.strategies.random_name_strategy.RandomNameStrategy.reset_state","title":"<code>reset_state()</code>","text":"<p>Reset the internal random state to initial values</p> Source code in <code>core/strategies/random_name_strategy.py</code> <pre><code>def reset_state(self):\n    \"\"\"Reset the internal random state to initial values\"\"\"\n    super().reset_state()  # Call StatefulMixin's reset_state\n    # Re-initialize random state with original seed\n    if self._seed is not None:\n        random.seed(self._seed)\n    # Re-derive any cached fields from current params for pooled reuse\n    self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/random_number_range_strategy/","title":"random_number_range_strategy","text":""},{"location":"reference/strategies/random_number_range_strategy/#core.strategies.random_number_range_strategy","title":"<code>core.strategies.random_number_range_strategy</code>","text":"<p>Random number range strategy for generating numbers within a range.</p> <p>This strategy uses the mixin pattern to reduce boilerplate while maintaining functionality.</p>"},{"location":"reference/strategies/random_number_range_strategy/#core.strategies.random_number_range_strategy.RandomNumberRangeStrategy","title":"<code>RandomNumberRangeStrategy</code>","text":"<p>               Bases: <code>BaseStrategy</code>, <code>SeedMixin</code>, <code>StatefulMixin</code>, <code>ValidationMixin</code></p> <p>Strategy for generating random numbers within a specified range. Supports stateful generation with consistent random state.</p> <p>Uses mixins for: - SeedMixin: Automatic seed validation and random state initialization - StatefulMixin: Standardized state management and reporting - ValidationMixin: Common parameter validation patterns</p> Source code in <code>core/strategies/random_number_range_strategy.py</code> <pre><code>class RandomNumberRangeStrategy(\n    BaseStrategy, SeedMixin, StatefulMixin, ValidationMixin\n):\n    \"\"\"\n    Strategy for generating random numbers within a specified range.\n    Supports stateful generation with consistent random state.\n\n    Uses mixins for:\n    - SeedMixin: Automatic seed validation and random state initialization\n    - StatefulMixin: Standardized state management and reporting\n    - ValidationMixin: Common parameter validation patterns\n    \"\"\"\n\n    def __init__(self, mode: str, logger=None, **kwargs):\n        \"\"\"Initialize the strategy with configuration parameters\"\"\"\n        super().__init__(mode=mode, logger=logger, **kwargs)\n\n        # Use mixins for common functionality\n        self._validate_seed()  # From SeedMixin\n        self._initialize_random_seed()  # From SeedMixin\n        self._initialize_state()  # From StatefulMixin\n        # Validation is handled by config via factory\n\n    def _initialize_state(self):\n        \"\"\"Initialize internal state for stateful generation\"\"\"\n        super()._initialize_state()  # Call StatefulMixin's _initialize_state first\n\n        # Initialize random state (seed handled by SeedMixin)\n        self._random_state = np.random.RandomState(self._seed)\n\n        # Store bounds for efficient access\n        self._lower = float(self.params[\"start\"])\n        self._upper = float(self.params[\"end\"])\n        self._is_integer = isinstance(self.params[\"start\"], int) and isinstance(\n            self.params[\"end\"], int\n        )\n\n        self.logger.debug(\n            f\"RandomNumberRangeStrategy initialized with range=[{self._lower}, {self._upper}], \"\n            f\"integer={self._is_integer}, seed={self._seed}\"\n        )\n\n    def generate_chunk(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Generate a chunk of random numbers maintaining internal random state.\n        This method is stateful and maintains consistent random sequence.\n\n        Args:\n            count: Number of values to generate\n\n        Returns:\n            pd.Series: Generated values\n        \"\"\"\n        self.logger.debug(\n            f\"Generating chunk of {count} random numbers in range [{self._lower}, {self._upper}]\"\n        )\n\n        # Generate random numbers using internal state\n        values = self._random_state.uniform(self._lower, self._upper, count)\n\n        # Convert to integers if both bounds are integers\n        if self._is_integer:\n            values = values.astype(int)\n\n        return pd.Series(values, dtype=int if self._is_integer else float)\n\n\n    def get_current_state(self) -&gt; dict:\n        \"\"\"Get current state information for debugging\"\"\"\n        state = super().get_current_state()  # Get base state from StatefulMixin\n        state.update(\n            {\n                \"lower_bound\": self._lower,\n                \"upper_bound\": self._upper,\n                \"is_integer\": self._is_integer,\n                \"random_state_type\": type(self._random_state).__name__,\n            }\n        )\n        return state\n\n    def reset_state(self):\n        \"\"\"Reset the internal state to initial values\"\"\"\n        self.logger.debug(\"Resetting RandomNumberRangeStrategy state\")\n        super().reset_state()\n        self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/random_number_range_strategy/#core.strategies.random_number_range_strategy.RandomNumberRangeStrategy.generate_chunk","title":"<code>generate_chunk(count)</code>","text":"<p>Generate a chunk of random numbers maintaining internal random state. This method is stateful and maintains consistent random sequence.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of values to generate</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Generated values</p> Source code in <code>core/strategies/random_number_range_strategy.py</code> <pre><code>def generate_chunk(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Generate a chunk of random numbers maintaining internal random state.\n    This method is stateful and maintains consistent random sequence.\n\n    Args:\n        count: Number of values to generate\n\n    Returns:\n        pd.Series: Generated values\n    \"\"\"\n    self.logger.debug(\n        f\"Generating chunk of {count} random numbers in range [{self._lower}, {self._upper}]\"\n    )\n\n    # Generate random numbers using internal state\n    values = self._random_state.uniform(self._lower, self._upper, count)\n\n    # Convert to integers if both bounds are integers\n    if self._is_integer:\n        values = values.astype(int)\n\n    return pd.Series(values, dtype=int if self._is_integer else float)\n</code></pre>"},{"location":"reference/strategies/random_number_range_strategy/#core.strategies.random_number_range_strategy.RandomNumberRangeStrategy.get_current_state","title":"<code>get_current_state()</code>","text":"<p>Get current state information for debugging</p> Source code in <code>core/strategies/random_number_range_strategy.py</code> <pre><code>def get_current_state(self) -&gt; dict:\n    \"\"\"Get current state information for debugging\"\"\"\n    state = super().get_current_state()  # Get base state from StatefulMixin\n    state.update(\n        {\n            \"lower_bound\": self._lower,\n            \"upper_bound\": self._upper,\n            \"is_integer\": self._is_integer,\n            \"random_state_type\": type(self._random_state).__name__,\n        }\n    )\n    return state\n</code></pre>"},{"location":"reference/strategies/random_number_range_strategy/#core.strategies.random_number_range_strategy.RandomNumberRangeStrategy.reset_state","title":"<code>reset_state()</code>","text":"<p>Reset the internal state to initial values</p> Source code in <code>core/strategies/random_number_range_strategy.py</code> <pre><code>def reset_state(self):\n    \"\"\"Reset the internal state to initial values\"\"\"\n    self.logger.debug(\"Resetting RandomNumberRangeStrategy state\")\n    super().reset_state()\n    self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/replacement_strategy/","title":"replacement_strategy","text":""},{"location":"reference/strategies/replacement_strategy/#core.strategies.replacement_strategy","title":"<code>core.strategies.replacement_strategy</code>","text":"<p>Replacement strategy for replacing specific values with other values.</p>"},{"location":"reference/strategies/replacement_strategy/#core.strategies.replacement_strategy.ReplacementStrategy","title":"<code>ReplacementStrategy</code>","text":"<p>               Bases: <code>BaseStrategy</code></p> <p>Strategy for replacing specific values with other values. Implements stateful interface for consistency.</p> Source code in <code>core/strategies/replacement_strategy.py</code> <pre><code>class ReplacementStrategy(BaseStrategy):\n    \"\"\"\n    Strategy for replacing specific values with other values.\n    Implements stateful interface for consistency.\n    \"\"\"\n\n    def __init__(self, mode: str, logger=None, **kwargs):\n        \"\"\"Initialize the strategy with configuration parameters\"\"\"\n        super().__init__(mode=mode, logger=logger, **kwargs)\n\n        # Initialize state (simple for replacement strategy)\n        self._initialize_state()\n\n    def _initialize_state(self):\n        \"\"\"Initialize internal state for stateful generation\"\"\"\n        # Store replacement values for efficient access\n        self._from_value = self.params[\"from_value\"]\n        self._to_value = self.params[\"to_value\"]\n\n        self.logger.debug(\n            f\"ReplacementStrategy initialized: {self._from_value} -&gt; {self._to_value}\"\n        )\n\n    def _validate_params(self):\n        \"\"\"Delegate validation to ReplacementConfig (no-op) while ensuring presence.\"\"\"\n        if \"from_value\" not in self.params or \"to_value\" not in self.params:\n            raise InvalidConfigParamException(\n                \"Missing required parameter: from_value/to_value\"\n            )\n        ReplacementConfig(\n            from_value=self.params[\"from_value\"], to_value=self.params[\"to_value\"]\n        ).validate()\n\n    def generate_chunk(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Generate a chunk of replacement values maintaining internal state.\n        For replacement strategy, this works with existing data.\n\n        Args:\n            count: Number of values to generate\n\n        Returns:\n            pd.Series: Series with replacements applied\n        \"\"\"\n        self.logger.debug(f\"Generating chunk of {count} replacement values\")\n\n        if self.df is not None and self.col_name in self.df.columns:\n            # Work with existing data\n            values = self.df[self.col_name].head(count)\n            result = values.replace(self._from_value, self._to_value)\n        else:\n            # If no existing data, return the to_value repeated\n            self.logger.warning(\"No existing data found, returning to_value repeated\")\n            result = pd.Series([self._to_value] * count)\n\n        return result\n\n    def reset_state(self):\n        \"\"\"Reset the internal state to initial values\"\"\"\n        self.logger.debug(\"Resetting ReplacementStrategy state\")\n        self._initialize_state()\n\n    # def get_current_state(self) -&gt; dict:\n    #     \"\"\"Get current state information for debugging\"\"\"\n    #     return {\n    #         \"strategy\": \"ReplacementStrategy\",\n    #         \"stateful\": True,\n    #         \"column\": self.col_name,\n    #         \"from_value\": self._from_value,\n    #         \"to_value\": self._to_value,\n    #     }\n\n    def generate_data(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Replace values by calling generate_chunk.\n        This ensures consistent behavior between batch and non-batch modes.\n\n        Args:\n            count: Number of values to generate\n\n        Returns:\n            pd.Series: Series with replacements applied\n        \"\"\"\n        self.logger.debug(\n            f\"Generating {count} values using unified chunk-based approach\"\n        )\n\n        # For non-batch mode, reset state to ensure consistent behavior\n        self.reset_state()\n        # Generate the chunk\n        result = self.generate_chunk(count)\n\n        self.logger.debug(f\"Generated {len(result)} replacement values\")\n\n        return result\n</code></pre>"},{"location":"reference/strategies/replacement_strategy/#core.strategies.replacement_strategy.ReplacementStrategy.generate_chunk","title":"<code>generate_chunk(count)</code>","text":"<p>Generate a chunk of replacement values maintaining internal state. For replacement strategy, this works with existing data.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of values to generate</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Series with replacements applied</p> Source code in <code>core/strategies/replacement_strategy.py</code> <pre><code>def generate_chunk(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Generate a chunk of replacement values maintaining internal state.\n    For replacement strategy, this works with existing data.\n\n    Args:\n        count: Number of values to generate\n\n    Returns:\n        pd.Series: Series with replacements applied\n    \"\"\"\n    self.logger.debug(f\"Generating chunk of {count} replacement values\")\n\n    if self.df is not None and self.col_name in self.df.columns:\n        # Work with existing data\n        values = self.df[self.col_name].head(count)\n        result = values.replace(self._from_value, self._to_value)\n    else:\n        # If no existing data, return the to_value repeated\n        self.logger.warning(\"No existing data found, returning to_value repeated\")\n        result = pd.Series([self._to_value] * count)\n\n    return result\n</code></pre>"},{"location":"reference/strategies/replacement_strategy/#core.strategies.replacement_strategy.ReplacementStrategy.generate_data","title":"<code>generate_data(count)</code>","text":"<p>Replace values by calling generate_chunk. This ensures consistent behavior between batch and non-batch modes.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of values to generate</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Series with replacements applied</p> Source code in <code>core/strategies/replacement_strategy.py</code> <pre><code>def generate_data(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Replace values by calling generate_chunk.\n    This ensures consistent behavior between batch and non-batch modes.\n\n    Args:\n        count: Number of values to generate\n\n    Returns:\n        pd.Series: Series with replacements applied\n    \"\"\"\n    self.logger.debug(\n        f\"Generating {count} values using unified chunk-based approach\"\n    )\n\n    # For non-batch mode, reset state to ensure consistent behavior\n    self.reset_state()\n    # Generate the chunk\n    result = self.generate_chunk(count)\n\n    self.logger.debug(f\"Generated {len(result)} replacement values\")\n\n    return result\n</code></pre>"},{"location":"reference/strategies/replacement_strategy/#core.strategies.replacement_strategy.ReplacementStrategy.reset_state","title":"<code>reset_state()</code>","text":"<p>Reset the internal state to initial values</p> Source code in <code>core/strategies/replacement_strategy.py</code> <pre><code>def reset_state(self):\n    \"\"\"Reset the internal state to initial values\"\"\"\n    self.logger.debug(\"Resetting ReplacementStrategy state\")\n    self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/series_strategy/","title":"series_strategy","text":""},{"location":"reference/strategies/series_strategy/#core.strategies.series_strategy","title":"<code>core.strategies.series_strategy</code>","text":"<p>Series strategy for generating sequential numeric values.</p> <p>This strategy has been refactored to use the new mixin pattern, reducing boilerplate code by ~30 lines while maintaining the same functionality.</p>"},{"location":"reference/strategies/series_strategy/#core.strategies.series_strategy.SeriesStrategy","title":"<code>SeriesStrategy</code>","text":"<p>               Bases: <code>BaseStrategy</code>, <code>SeedMixin</code>, <code>StatefulMixin</code>, <code>ValidationMixin</code></p> <p>Strategy for generating sequential numeric values. Supports both traditional batch generation and stateful chunked generation.</p> <p>Uses mixins for: - SeedMixin: Automatic seed validation and random state initialization - StatefulMixin: Standardized state management and reporting - ValidationMixin: Common parameter validation patterns - NumericMixin: Specialized numeric validation and utilities</p> Source code in <code>core/strategies/series_strategy.py</code> <pre><code>class SeriesStrategy(BaseStrategy, SeedMixin, StatefulMixin, ValidationMixin):\n    \"\"\"\n    Strategy for generating sequential numeric values.\n    Supports both traditional batch generation and stateful chunked generation.\n\n    Uses mixins for:\n    - SeedMixin: Automatic seed validation and random state initialization\n    - StatefulMixin: Standardized state management and reporting\n    - ValidationMixin: Common parameter validation patterns\n    - NumericMixin: Specialized numeric validation and utilities\n    \"\"\"\n\n    def __init__(self, mode: str, logger=None, **kwargs):\n        \"\"\"Initialize the strategy with configuration parameters\"\"\"\n        super().__init__(mode=mode, logger=logger, **kwargs)\n\n        # Use mixins for common functionality\n        self._validate_seed()  # From SeedMixin\n        self._initialize_random_seed()  # From SeedMixin\n        self._initialize_state()  # From StatefulMixin\n        # Validation is handled by config via factory\n\n    def _initialize_state(self):\n        \"\"\"Initialize internal state for stateful generation\"\"\"\n        super()._initialize_state()  # Call StatefulMixin's _initialize_state first\n\n        # Determine if we're working with floats or integers\n        start_value = self.params[\"start\"]\n        step_value = self.params.get(\"step\", 1 if isinstance(start_value, int) else 0.1)\n\n        if isinstance(start_value, float) or isinstance(step_value, float):\n            # Use Decimal for floating point precision\n            from decimal import Decimal, getcontext\n\n            getcontext().prec = 10  # Increased precision\n\n            self._current_value = Decimal(str(start_value))\n            self._step = Decimal(str(step_value))\n            self._is_float = True\n        else:\n            self._current_value = int(start_value)\n            self._step = int(step_value)\n            self._is_float = False\n\n        # If we're in streaming/batch mode and have prior state, continue from it\n        if hasattr(self, \"mode\") and self.mode == \"STREAM&amp;BATCH\":\n            prev_state = (self.strategy_state or {}).get(self._state_key)\n            if prev_state and \"last_value\" in prev_state:\n                last_value = prev_state[\"last_value\"]\n                if self._is_float:\n                    from decimal import Decimal\n\n                    self._current_value = Decimal(str(last_value)) + self._step\n                else:\n                    self._current_value = int(last_value) + int(self._step)\n\n        self.logger.debug(\n            f\"SeriesStrategy initialized with start={self._current_value}, \"\n            f\"step={self._step}, is_float={self._is_float}\"\n        )\n\n    def _validate_params(self):\n        \"\"\"Delegate validation to SeriesConfig.\"\"\"\n        self._validate_required_params([\"start\"])  # ensure presence\n        SeriesConfig(\n            start=self.params[\"start\"], step=self.params.get(\"step\", 1)\n        ).validate()\n\n    def generate_chunk(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Generate a chunk of sequential values maintaining internal state.\n        This method is stateful and continues from where the last chunk ended.\n\n        Args:\n            count: Number of values to generate\n\n        Returns:\n            pd.Series: Generated values starting from current state\n        \"\"\"\n        self.logger.debug(\n            f\"Generating chunk of {count} values starting from {self._current_value}\"\n        )\n\n        if self._is_float:\n            # Generate float values using Decimal for precision\n            values = []\n            current = self._current_value\n            for _ in range(count):\n                values.append(float(current))\n                current += self._step\n            self._current_value = current\n\n            return pd.Series(values, dtype=float)\n        else:\n            # Generate integer values\n            start = self._current_value\n            end = start + (count * self._step)\n            values = np.arange(start, end, self._step, dtype=int)\n\n            # Update current value for next chunk\n            self._current_value = end\n\n            return pd.Series(values, dtype=int)\n\n    def reset_state(self):\n        \"\"\"Reset the internal state to initial values\"\"\"\n        self.logger.debug(\"Resetting SeriesStrategy state\")\n        self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/series_strategy/#core.strategies.series_strategy.SeriesStrategy.generate_chunk","title":"<code>generate_chunk(count)</code>","text":"<p>Generate a chunk of sequential values maintaining internal state. This method is stateful and continues from where the last chunk ended.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of values to generate</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Generated values starting from current state</p> Source code in <code>core/strategies/series_strategy.py</code> <pre><code>def generate_chunk(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Generate a chunk of sequential values maintaining internal state.\n    This method is stateful and continues from where the last chunk ended.\n\n    Args:\n        count: Number of values to generate\n\n    Returns:\n        pd.Series: Generated values starting from current state\n    \"\"\"\n    self.logger.debug(\n        f\"Generating chunk of {count} values starting from {self._current_value}\"\n    )\n\n    if self._is_float:\n        # Generate float values using Decimal for precision\n        values = []\n        current = self._current_value\n        for _ in range(count):\n            values.append(float(current))\n            current += self._step\n        self._current_value = current\n\n        return pd.Series(values, dtype=float)\n    else:\n        # Generate integer values\n        start = self._current_value\n        end = start + (count * self._step)\n        values = np.arange(start, end, self._step, dtype=int)\n\n        # Update current value for next chunk\n        self._current_value = end\n\n        return pd.Series(values, dtype=int)\n</code></pre>"},{"location":"reference/strategies/series_strategy/#core.strategies.series_strategy.SeriesStrategy.reset_state","title":"<code>reset_state()</code>","text":"<p>Reset the internal state to initial values</p> Source code in <code>core/strategies/series_strategy.py</code> <pre><code>def reset_state(self):\n    \"\"\"Reset the internal state to initial values\"\"\"\n    self.logger.debug(\"Resetting SeriesStrategy state\")\n    self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/time_range_strategy/","title":"time_range_strategy","text":""},{"location":"reference/strategies/time_range_strategy/#core.strategies.time_range_strategy","title":"<code>core.strategies.time_range_strategy</code>","text":"<p>Time range strategy for generating random time values within a range.</p> <p>This strategy has been refactored to use the new mixin pattern, reducing boilerplate code by ~35 lines while maintaining the same functionality.</p>"},{"location":"reference/strategies/time_range_strategy/#core.strategies.time_range_strategy.TimeRangeStrategy","title":"<code>TimeRangeStrategy</code>","text":"<p>               Bases: <code>BaseStrategy</code>, <code>SeedMixin</code>, <code>StatefulMixin</code>, <code>ValidationMixin</code>, <code>DateTimeMixin</code></p> <p>Strategy for generating random time values within a specified range.</p> <p>Uses mixins for: - SeedMixin: Automatic seed validation and random state initialization - StatefulMixin: Standardized state management and reporting - ValidationMixin: Common parameter validation patterns - DateTimeMixin: Specialized datetime validation and utilities</p> Source code in <code>core/strategies/time_range_strategy.py</code> <pre><code>class TimeRangeStrategy(\n    BaseStrategy, SeedMixin, StatefulMixin, ValidationMixin, DateTimeMixin\n):\n    \"\"\"\n    Strategy for generating random time values within a specified range.\n\n    Uses mixins for:\n    - SeedMixin: Automatic seed validation and random state initialization\n    - StatefulMixin: Standardized state management and reporting\n    - ValidationMixin: Common parameter validation patterns\n    - DateTimeMixin: Specialized datetime validation and utilities\n    \"\"\"\n\n    def __init__(self, mode: str, logger=None, **kwargs):\n        \"\"\"Initialize the strategy with configuration parameters\"\"\"\n        super().__init__(mode=mode, logger=logger, **kwargs)\n\n        # Use mixins for common functionality\n        self._validate_seed()  # From SeedMixin\n        self._initialize_random_seed()  # From SeedMixin\n        self._initialize_state()  # From StatefulMixin\n        # Validation is handled by config via factory\n\n    def _initialize_state(self):\n        \"\"\"Initialize internal state for stateful generation\"\"\"\n\n        # Initialize with seed if provided for consistent generation\n\n        seed = self.params.get(\"seed\", None)\n\n        if seed is not None:\n            import random\n\n            import numpy as np\n\n            random.seed(seed)\n\n            np.random.seed(seed)\n\n        self.logger.debug(f\"TimeRangeStrategy initialized with seed={seed}\")\n\n    def generate_chunk(self, count: int) -&gt; pd.Series:\n        \"\"\"\n        Generate a chunk of data maintaining internal state.\n        This method is stateful and maintains consistent random sequence.\n        Args:\n            count: Number of values to generate\n        Returns:\n            pd.Series: Generated values\n        \"\"\"\n\n        self.logger.debug(f\"Generating chunk of {count} values\")\n\n        # Use the original generation logic\n        \"\"\"\n        Generate random time values within the specified range.\n\n        Args:\n            count: Number of time values to generate\n\n        Returns:\n            pd.Series: Generated time values\n        \"\"\"\n\n        # Get time generation parameters\n        params = {\n            \"start_time\": self.params[\"start_time\"],\n            \"end_time\": self.params[\"end_time\"],\n        }\n\n        if \"input_format\" in self.params:\n            params[\"input_format\"] = self.params[\"input_format\"]\n        if \"output_format\" in self.params:\n            params[\"output_format\"] = self.params[\"output_format\"]\n\n        # Generate the time values\n        times = []\n        for _ in range(count):\n            # Parse start and end times\n            if \"input_format\" in params:\n                start = datetime.strptime(\n                    params[\"start_time\"], params[\"input_format\"]\n                ).time()\n                end = datetime.strptime(\n                    params[\"end_time\"], params[\"input_format\"]\n                ).time()\n            else:\n                start = datetime.strptime(params[\"start_time\"], \"%H:%M:%S\").time()\n                end = datetime.strptime(params[\"end_time\"], \"%H:%M:%S\").time()\n\n            # Generate random time between start and end\n            start_seconds = start.hour * 3600 + start.minute * 60 + start.second\n            end_seconds = end.hour * 3600 + end.minute * 60 + end.second\n\n            if end_seconds &lt; start_seconds:\n                end_seconds += 24 * 3600  # Add 24 hours if end time is on next day\n\n            random_seconds = random.randint(start_seconds, end_seconds)\n            hours = random_seconds // 3600\n            minutes = (random_seconds % 3600) // 60\n            seconds = random_seconds % 60\n\n            random_time = time(hours % 24, minutes, seconds)\n\n            # Format the time\n            if \"output_format\" in params:\n                time_str = random_time.strftime(params[\"output_format\"])\n            else:\n                time_str = random_time.strftime(\"%H:%M:%S\")\n\n            times.append(time_str)\n\n        return pd.Series(times)\n\n    def reset_state(self):\n        \"\"\"Reset the internal state to initial values\"\"\"\n\n        self.logger.debug(\"Resetting TimeRangeStrategy state\")\n\n        self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/time_range_strategy/#core.strategies.time_range_strategy.TimeRangeStrategy.generate_chunk","title":"<code>generate_chunk(count)</code>","text":"<p>Generate a chunk of data maintaining internal state. This method is stateful and maintains consistent random sequence. Args:     count: Number of values to generate Returns:     pd.Series: Generated values</p> Source code in <code>core/strategies/time_range_strategy.py</code> <pre><code>def generate_chunk(self, count: int) -&gt; pd.Series:\n    \"\"\"\n    Generate a chunk of data maintaining internal state.\n    This method is stateful and maintains consistent random sequence.\n    Args:\n        count: Number of values to generate\n    Returns:\n        pd.Series: Generated values\n    \"\"\"\n\n    self.logger.debug(f\"Generating chunk of {count} values\")\n\n    # Use the original generation logic\n    \"\"\"\n    Generate random time values within the specified range.\n\n    Args:\n        count: Number of time values to generate\n\n    Returns:\n        pd.Series: Generated time values\n    \"\"\"\n\n    # Get time generation parameters\n    params = {\n        \"start_time\": self.params[\"start_time\"],\n        \"end_time\": self.params[\"end_time\"],\n    }\n\n    if \"input_format\" in self.params:\n        params[\"input_format\"] = self.params[\"input_format\"]\n    if \"output_format\" in self.params:\n        params[\"output_format\"] = self.params[\"output_format\"]\n\n    # Generate the time values\n    times = []\n    for _ in range(count):\n        # Parse start and end times\n        if \"input_format\" in params:\n            start = datetime.strptime(\n                params[\"start_time\"], params[\"input_format\"]\n            ).time()\n            end = datetime.strptime(\n                params[\"end_time\"], params[\"input_format\"]\n            ).time()\n        else:\n            start = datetime.strptime(params[\"start_time\"], \"%H:%M:%S\").time()\n            end = datetime.strptime(params[\"end_time\"], \"%H:%M:%S\").time()\n\n        # Generate random time between start and end\n        start_seconds = start.hour * 3600 + start.minute * 60 + start.second\n        end_seconds = end.hour * 3600 + end.minute * 60 + end.second\n\n        if end_seconds &lt; start_seconds:\n            end_seconds += 24 * 3600  # Add 24 hours if end time is on next day\n\n        random_seconds = random.randint(start_seconds, end_seconds)\n        hours = random_seconds // 3600\n        minutes = (random_seconds % 3600) // 60\n        seconds = random_seconds % 60\n\n        random_time = time(hours % 24, minutes, seconds)\n\n        # Format the time\n        if \"output_format\" in params:\n            time_str = random_time.strftime(params[\"output_format\"])\n        else:\n            time_str = random_time.strftime(\"%H:%M:%S\")\n\n        times.append(time_str)\n\n    return pd.Series(times)\n</code></pre>"},{"location":"reference/strategies/time_range_strategy/#core.strategies.time_range_strategy.TimeRangeStrategy.reset_state","title":"<code>reset_state()</code>","text":"<p>Reset the internal state to initial values</p> Source code in <code>core/strategies/time_range_strategy.py</code> <pre><code>def reset_state(self):\n    \"\"\"Reset the internal state to initial values\"\"\"\n\n    self.logger.debug(\"Resetting TimeRangeStrategy state\")\n\n    self._initialize_state()\n</code></pre>"},{"location":"reference/strategies/uuid_strategy/","title":"uuid_strategy","text":""},{"location":"reference/strategies/uuid_strategy/#core.strategies.uuid_strategy","title":"<code>core.strategies.uuid_strategy</code>","text":"<p>UUID strategy: generates UUID strings for identifiers.</p> <p>Formatting options via config: - hyphens (bool): include dashes in the UUID string (default: True) - uppercase (bool): return uppercase hex (default: False) - prefix (str): optional string to prepend (default: \"\")</p> <p>Version: - version (int): 4 or 5. Use UUID4 for random IDs, UUID5 for   deterministic IDs derived from a namespace and counter. Default: 5.</p> <p>Notes on uniqueness: - Both version 4 and 5 produce effectively unique values. This strategy ignores   any 'unique' flag and does not perform additional uniqueness enforcement.</p>"},{"location":"reference/strategies/uuid_strategy/#core.strategies.uuid_strategy.UuidStrategy","title":"<code>UuidStrategy</code>","text":"<p>               Bases: <code>BaseStrategy</code>, <code>StatefulMixin</code>, <code>ValidationMixin</code></p> <p>Strategy for generating UUID values as strings, with optional formatting.</p> Source code in <code>core/strategies/uuid_strategy.py</code> <pre><code>class UuidStrategy(BaseStrategy, StatefulMixin, ValidationMixin):\n    \"\"\"\n    Strategy for generating UUID values as strings, with optional formatting.\n    \"\"\"\n\n    def __init__(self, mode: str, logger=None, **kwargs):\n        super().__init__(mode=mode, logger=logger, **kwargs)\n        # Ignore external uniqueness requests for this strategy\n        if getattr(self, \"unique\", False):\n            self.logger.debug(\n                \"UuidStrategy: ignoring 'unique' flag; both v4 and v5 are unique by design.\"\n            )\n            self.unique = False\n        self._initialize_state()\n\n    def _initialize_state(self) -&gt; None:\n        super()._initialize_state()\n        self._include_hyphens: bool = bool(self.params.get(\"hyphens\", True))\n        self._uppercase: bool = bool(self.params.get(\"uppercase\", False))\n        self._prefix: str = str(self.params.get(\"prefix\", \"\"))\n        self._numbers_only: bool = bool(self.params.get(\"numbers_only\", False))\n        self._letters_only: bool = bool(self.params.get(\"letters_only\", False))\n        self._version: int = int(self.params.get(\"version\", 5))\n\n        # Deterministic mode for version 5\n        self._deterministic: bool = self._version == 5\n        self._counter: int = 0\n\n        # Build a stable namespace using seed (if provided) and column name\n        seed_val = self.params.get(\"seed\")\n        seed_str = str(seed_val) if seed_val is not None else \"no-seed\"\n        # Namespacing by column binds generated IDs to the target column logically\n        ns_name = f\"genxdata:{self.col_name}:{seed_str}\"\n        self._namespace = uuid.uuid5(uuid.NAMESPACE_DNS, ns_name)\n\n        self.logger.debug(\n            f\"UuidStrategy initialized: hyphens={self._include_hyphens}, \"\n            f\"uppercase={self._uppercase}, prefix='{self._prefix}', version={self._version}\"\n        )\n\n    def _format_uuid(self, u: uuid.UUID) -&gt; str:\n        value = str(u)\n\n        if not self._include_hyphens:\n            value = value.replace(\"-\", \"\")\n\n        if self._uppercase:\n            value = value.upper()\n\n        if self._numbers_only:\n            value = str(u.int)\n\n        if self._prefix:\n            value = f\"{self._prefix}{value}\"\n\n        return value\n\n    def generate_chunk(self, count: int) -&gt; pd.Series:\n        self.logger.debug(f\"Generating {count} UUID values\")\n\n        if self._version == 5:\n            start = self._counter\n            values = [\n                self._format_uuid(uuid.uuid5(self._namespace, f\"{start + i}\"))\n                for i in range(count)\n            ]\n            self._counter += count\n            return pd.Series(values, dtype=object)\n        else:\n            # UUID4 (random)\n            values = [self._format_uuid(uuid.uuid4()) for _ in range(count)]\n            return pd.Series(values, dtype=object)\n\n    def reset_state(self) -&gt; None:\n        self.logger.debug(\"Resetting UuidStrategy state\")\n        super().reset_state()\n        self._initialize_state()\n</code></pre>"},{"location":"reference/writers/","title":"Writers","text":"<ul> <li>base_file_writer</li> <li>base_writer</li> <li>batch_writer</li> <li>csv_file_writer</li> <li>excel_file_writer</li> <li>feather_file_writer</li> <li>file_writer_factory</li> <li>generic_file_writer</li> <li>html_file_writer</li> <li>json_file_writer</li> <li>parquet_file_writer</li> <li>sqlite_file_writer</li> <li>stream_writer</li> </ul>"},{"location":"reference/writers/base_file_writer/","title":"base_file_writer","text":""},{"location":"reference/writers/base_file_writer/#core.writers.base_file_writer","title":"<code>core.writers.base_file_writer</code>","text":"<p>Base abstract class for file writers in GenXData.</p> <p>Provides a unified interface for all file format writers.</p>"},{"location":"reference/writers/base_file_writer/#core.writers.base_file_writer.BaseFileWriter","title":"<code>BaseFileWriter</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Abstract base class for all file format writers.</p> <p>Provides common functionality for path handling, validation, and a unified interface for writing DataFrames to files.</p> Source code in <code>core/writers/base_file_writer.py</code> <pre><code>class BaseFileWriter(BaseWriter):\n    \"\"\"\n    Abstract base class for all file format writers.\n\n    Provides common functionality for path handling, validation,\n    and a unified interface for writing DataFrames to files.\n    \"\"\"\n\n    def __init__(self, config: dict[str, Any]):\n        \"\"\"\n        Initialize the file writer with configuration.\n\n        Args:\n            config: Dictionary containing writer-specific parameters\n                   Must include output path via 'output_path' key\n        \"\"\"\n        # Call parent constructor\n        super().__init__(config)\n\n        # For backward compatibility, support both 'config' and direct params\n        self.params = (\n            config if \"output_path\" in config else config.get(\"params\", config)\n        )\n        self.logger = Logger.get_logger(self.__class__.__name__.lower())\n\n        # Validate required parameters\n        self.validate_params()\n\n        # Extract and store the base output path (may contain placeholders)\n        self.base_output_path = self._extract_output_path()\n        self.output_path = self._normalize_path(self.base_output_path)\n        self.last_written_path: str | None = None\n\n        # Ensure output directory exists\n        from utils.file_utils.file_operations import ensure_output_dir\n\n        ensure_output_dir(self.output_path, self.logger)\n\n    def validate_params(self) -&gt; None:\n        \"\"\"\n        Validate that required parameters are present.\n\n        Raises:\n            ValueError: If required parameters are missing\n        \"\"\"\n        if not isinstance(self.params, dict):\n            raise ValueError(\"Parameters must be a dictionary\")\n\n        # Check for output path using various possible keys\n        path_keys = [\"output_path\", \"path_or_buf\", \"path\", \"database\", \"excel_writer\"]\n        if not any(key in self.params for key in path_keys):\n            raise ValueError(\n                f\"Missing output path parameter. Please use 'output_path' \"\n                f\"(preferred) or one of: {path_keys[1:]}\"\n            )\n\n    def _extract_output_path(self) -&gt; str:\n        \"\"\"\n        Extract output path from parameters using various possible keys.\n\n        Note: 'output_path' is the preferred/standard parameter name.\n        Other parameter names are supported for backward compatibility.\n\n        Returns:\n            str: The output path\n        \"\"\"\n        # Parameter name preference order (most preferred first)\n        param_preference = [\n            \"output_path\",  # Preferred standard parameter\n            \"path_or_buf\",  # Pandas standard for some writers\n            \"path\",  # Alternative path parameter\n            \"database\",  # SQLite specific\n            \"excel_writer\",  # Excel specific\n        ]\n\n        for key in param_preference:\n            if key in self.params:\n                # Log deprecation notice for non-preferred parameter names\n                if key != \"output_path\":\n                    self.logger.debug(\n                        f\"Parameter '{key}' is deprecated. Use 'output_path' instead.\"\n                    )\n                return self.params[key]\n\n        raise ValueError(\n            \"No valid output path found in parameters. \"\n            \"Please use 'output_path' parameter.\"\n        )\n\n    def _normalize_path(self, path: str) -&gt; str:\n        \"\"\"\n        Normalize the output path by ensuring proper file extension.\n\n        Args:\n            path: Original file path\n\n        Returns:\n            str: Normalized path with proper extension\n        \"\"\"\n        expected_extensions = self.get_expected_extensions()\n\n        # Check if path already has a valid extension\n        current_ext = os.path.splitext(path)[1].lower()\n        if current_ext in expected_extensions:\n            return path\n\n        # Add the default extension\n        default_ext = expected_extensions[0]\n        return os.path.splitext(path)[0] + default_ext\n\n    def _resolve_output_path(self, metadata: dict[str, Any] = None) -&gt; str:\n        \"\"\"\n        Resolve the output path with metadata substitution.\n\n        Args:\n            metadata: Optional metadata for path substitution\n\n        Returns:\n            str: Resolved output path\n        \"\"\"\n        path = self.base_output_path\n\n        if metadata:\n            # Handle batch_index substitution\n            if \"batch_index\" in metadata:\n                path = path.replace(\"{batch_index}\", str(metadata[\"batch_index\"]))\n\n            # Handle other common placeholders\n            if \"timestamp\" in metadata:\n                path = path.replace(\"{timestamp}\", str(metadata[\"timestamp\"]))\n\n        return self._normalize_path(path)\n\n    def _get_writer_params(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Get parameters for the pandas writer method, excluding path-related keys.\n\n        Returns:\n            dict: Filtered parameters for the writer method\n        \"\"\"\n        excluded_keys = {\n            \"output_path\",\n            \"path_or_buf\",\n            \"path\",\n            \"database\",\n            \"excel_writer\",\n        }\n        return {k: v for k, v in self.params.items() if k not in excluded_keys}\n\n    @abstractmethod\n    def get_expected_extensions(self) -&gt; list[str]:\n        \"\"\"\n        Get list of valid file extensions for this writer.\n\n        Returns:\n            list[str]: List of valid extensions (including the dot)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_default_params(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Get default parameters for this writer type.\n\n        Returns:\n            dict: Default parameters\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def write(\n        self, df: pd.DataFrame, metadata: dict[str, Any] = None\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Write DataFrame to file.\n\n        Args:\n            df: DataFrame to write\n            metadata: Optional metadata (batch info, etc.)\n\n        Returns:\n            dict: Result information including status and file path\n        \"\"\"\n        pass\n\n    def finalize(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Finalize the file writing process.\n\n        For file writers, this typically just returns file information.\n        Subclasses can override for specific cleanup needs.\n\n        Returns:\n            dict: Finalization results and file information\n        \"\"\"\n        return {\n            \"status\": \"finalized\",\n            \"writer_type\": \"file\",\n            \"file_info\": self.get_file_info(),\n        }\n\n    def get_file_info(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Get information about the output file.\n\n        Returns:\n            dict: File information including path, size, etc.\n        \"\"\"\n        path = self.last_written_path or self.output_path\n        info = {\n            \"output_path\": path,\n            \"writer_type\": getattr(\n                self,\n                \"writer_kind\",\n                self.__class__.__name__.replace(\"Writer\", \"\").lower(),\n            ),\n            \"exists\": os.path.exists(path),\n        }\n\n        if info[\"exists\"]:\n            stat = os.stat(path)\n            info.update({\"size_bytes\": stat.st_size, \"modified_time\": stat.st_mtime})\n\n        return info\n</code></pre>"},{"location":"reference/writers/base_file_writer/#core.writers.base_file_writer.BaseFileWriter.finalize","title":"<code>finalize()</code>","text":"<p>Finalize the file writing process.</p> <p>For file writers, this typically just returns file information. Subclasses can override for specific cleanup needs.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Finalization results and file information</p> Source code in <code>core/writers/base_file_writer.py</code> <pre><code>def finalize(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Finalize the file writing process.\n\n    For file writers, this typically just returns file information.\n    Subclasses can override for specific cleanup needs.\n\n    Returns:\n        dict: Finalization results and file information\n    \"\"\"\n    return {\n        \"status\": \"finalized\",\n        \"writer_type\": \"file\",\n        \"file_info\": self.get_file_info(),\n    }\n</code></pre>"},{"location":"reference/writers/base_file_writer/#core.writers.base_file_writer.BaseFileWriter.get_default_params","title":"<code>get_default_params()</code>  <code>abstractmethod</code>","text":"<p>Get default parameters for this writer type.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Default parameters</p> Source code in <code>core/writers/base_file_writer.py</code> <pre><code>@abstractmethod\ndef get_default_params(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get default parameters for this writer type.\n\n    Returns:\n        dict: Default parameters\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/writers/base_file_writer/#core.writers.base_file_writer.BaseFileWriter.get_expected_extensions","title":"<code>get_expected_extensions()</code>  <code>abstractmethod</code>","text":"<p>Get list of valid file extensions for this writer.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of valid extensions (including the dot)</p> Source code in <code>core/writers/base_file_writer.py</code> <pre><code>@abstractmethod\ndef get_expected_extensions(self) -&gt; list[str]:\n    \"\"\"\n    Get list of valid file extensions for this writer.\n\n    Returns:\n        list[str]: List of valid extensions (including the dot)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/writers/base_file_writer/#core.writers.base_file_writer.BaseFileWriter.get_file_info","title":"<code>get_file_info()</code>","text":"<p>Get information about the output file.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>File information including path, size, etc.</p> Source code in <code>core/writers/base_file_writer.py</code> <pre><code>def get_file_info(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get information about the output file.\n\n    Returns:\n        dict: File information including path, size, etc.\n    \"\"\"\n    path = self.last_written_path or self.output_path\n    info = {\n        \"output_path\": path,\n        \"writer_type\": getattr(\n            self,\n            \"writer_kind\",\n            self.__class__.__name__.replace(\"Writer\", \"\").lower(),\n        ),\n        \"exists\": os.path.exists(path),\n    }\n\n    if info[\"exists\"]:\n        stat = os.stat(path)\n        info.update({\"size_bytes\": stat.st_size, \"modified_time\": stat.st_mtime})\n\n    return info\n</code></pre>"},{"location":"reference/writers/base_file_writer/#core.writers.base_file_writer.BaseFileWriter.validate_params","title":"<code>validate_params()</code>","text":"<p>Validate that required parameters are present.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required parameters are missing</p> Source code in <code>core/writers/base_file_writer.py</code> <pre><code>def validate_params(self) -&gt; None:\n    \"\"\"\n    Validate that required parameters are present.\n\n    Raises:\n        ValueError: If required parameters are missing\n    \"\"\"\n    if not isinstance(self.params, dict):\n        raise ValueError(\"Parameters must be a dictionary\")\n\n    # Check for output path using various possible keys\n    path_keys = [\"output_path\", \"path_or_buf\", \"path\", \"database\", \"excel_writer\"]\n    if not any(key in self.params for key in path_keys):\n        raise ValueError(\n            f\"Missing output path parameter. Please use 'output_path' \"\n            f\"(preferred) or one of: {path_keys[1:]}\"\n        )\n</code></pre>"},{"location":"reference/writers/base_file_writer/#core.writers.base_file_writer.BaseFileWriter.write","title":"<code>write(df, metadata=None)</code>  <code>abstractmethod</code>","text":"<p>Write DataFrame to file.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to write</p> required <code>metadata</code> <code>dict[str, Any]</code> <p>Optional metadata (batch info, etc.)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Result information including status and file path</p> Source code in <code>core/writers/base_file_writer.py</code> <pre><code>@abstractmethod\ndef write(\n    self, df: pd.DataFrame, metadata: dict[str, Any] = None\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Write DataFrame to file.\n\n    Args:\n        df: DataFrame to write\n        metadata: Optional metadata (batch info, etc.)\n\n    Returns:\n        dict: Result information including status and file path\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/writers/base_writer/","title":"base_writer","text":""},{"location":"reference/writers/base_writer/#core.writers.base_writer","title":"<code>core.writers.base_writer</code>","text":"<p>Abstract base writer for GenXData output operations.</p>"},{"location":"reference/writers/base_writer/#core.writers.base_writer.BaseWriter","title":"<code>BaseWriter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all writers in GenXData.</p> <p>Writers are responsible for taking DataFrames and outputting them in various formats and destinations.</p> Source code in <code>core/writers/base_writer.py</code> <pre><code>class BaseWriter(ABC):\n    \"\"\"\n    Abstract base class for all writers in GenXData.\n\n    Writers are responsible for taking DataFrames and outputting them\n    in various formats and destinations.\n    \"\"\"\n\n    def __init__(self, config: dict[str, Any]):\n        \"\"\"\n        Initialize the writer with configuration.\n\n        Args:\n            config: Writer configuration dictionary\n        \"\"\"\n        self.config = config\n\n    @abstractmethod\n    def write(\n        self, df: pd.DataFrame, metadata: dict[str, Any] = None\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Write a DataFrame to the output destination.\n\n        Args:\n            df: DataFrame to write\n            metadata: Optional metadata about the data\n\n        Returns:\n            Dictionary with write operation results\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def finalize(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Finalize the writing process and cleanup resources.\n\n        Returns:\n            Dictionary with finalization results and summary\n        \"\"\"\n        pass\n\n    def validate_config(self) -&gt; bool:\n        \"\"\"\n        Validate the writer configuration.\n\n        Returns:\n            True if configuration is valid\n\n        Raises:\n            ValueError: If configuration is invalid\n        \"\"\"\n        if not isinstance(self.config, dict):\n            raise ValueError(\"Writer config must be a dictionary\")\n        return True\n</code></pre>"},{"location":"reference/writers/base_writer/#core.writers.base_writer.BaseWriter.finalize","title":"<code>finalize()</code>  <code>abstractmethod</code>","text":"<p>Finalize the writing process and cleanup resources.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with finalization results and summary</p> Source code in <code>core/writers/base_writer.py</code> <pre><code>@abstractmethod\ndef finalize(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Finalize the writing process and cleanup resources.\n\n    Returns:\n        Dictionary with finalization results and summary\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/writers/base_writer/#core.writers.base_writer.BaseWriter.validate_config","title":"<code>validate_config()</code>","text":"<p>Validate the writer configuration.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if configuration is valid</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration is invalid</p> Source code in <code>core/writers/base_writer.py</code> <pre><code>def validate_config(self) -&gt; bool:\n    \"\"\"\n    Validate the writer configuration.\n\n    Returns:\n        True if configuration is valid\n\n    Raises:\n        ValueError: If configuration is invalid\n    \"\"\"\n    if not isinstance(self.config, dict):\n        raise ValueError(\"Writer config must be a dictionary\")\n    return True\n</code></pre>"},{"location":"reference/writers/base_writer/#core.writers.base_writer.BaseWriter.write","title":"<code>write(df, metadata=None)</code>  <code>abstractmethod</code>","text":"<p>Write a DataFrame to the output destination.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to write</p> required <code>metadata</code> <code>dict[str, Any]</code> <p>Optional metadata about the data</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with write operation results</p> Source code in <code>core/writers/base_writer.py</code> <pre><code>@abstractmethod\ndef write(\n    self, df: pd.DataFrame, metadata: dict[str, Any] = None\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Write a DataFrame to the output destination.\n\n    Args:\n        df: DataFrame to write\n        metadata: Optional metadata about the data\n\n    Returns:\n        Dictionary with write operation results\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/writers/batch_writer/","title":"batch_writer","text":""},{"location":"reference/writers/batch_writer/#core.writers.batch_writer","title":"<code>core.writers.batch_writer</code>","text":"<p>Batch writer implementation for GenXData.</p> <p>This is a compatibility wrapper that implements the BaseWriter interface for use with batch processing scenarios.</p>"},{"location":"reference/writers/batch_writer/#core.writers.batch_writer.BatchWriter","title":"<code>BatchWriter</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer implementation for batch processing scenarios.</p> <p>This is a wrapper that adapts a concrete writer implementation (e.g., file writer or StreamWriter) to the generic BaseWriter interface while adding batch-aware metadata and counters.</p> Source code in <code>core/writers/batch_writer.py</code> <pre><code>class BatchWriter(BaseWriter):\n    \"\"\"\n    Writer implementation for batch processing scenarios.\n\n    This is a wrapper that adapts a concrete writer implementation\n    (e.g., file writer or StreamWriter) to the generic BaseWriter interface\n    while adding batch-aware metadata and counters.\n    \"\"\"\n\n    def __init__(\n        self, config: dict[str, Any], writer_implementation: BaseWriter = None\n    ):\n        \"\"\"\n        Initialize the batch writer.\n\n        Args:\n            config: Batch writer configuration\n            actual_writer: The actual writer to delegate to (FileWriter, StreamWriter, etc.)\n        \"\"\"\n        super().__init__(config)\n        self.logger = Logger.get_logger(\"batch_writer\")\n        self.writer_implementation = writer_implementation\n        self.batches_written = 0\n        self.total_rows_written = 0\n        self.written_paths: list[str] = []\n\n        # If no actual writer provided, default to CSV file writer\n        if not self.writer_implementation:\n            from .file_writer_factory import FileWriterFactory\n\n            factory = FileWriterFactory()\n            file_writer_config = config.get(\"batch\", {}).get(\"file_writer\", {})\n            writer_type = file_writer_config.get(\"type\", \"csv\")\n            writer_params = file_writer_config.get(\"params\", {})\n\n            self.writer_implementation = factory.create_writer(\n                writer_type, writer_params\n            )\n\n        self.logger.debug(\n            f\"BatchWriter initialized with {type(self.writer_implementation).__name__}\"\n        )\n\n    def write(\n        self, df: pd.DataFrame, metadata: dict[str, Any] = None\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Write DataFrame (BaseWriter interface).\n\n        Args:\n            df: DataFrame to write\n            metadata: Optional metadata\n\n        Returns:\n            Dictionary with write operation results\n        \"\"\"\n        # Increment batch counter first (1-based indexing)\n        self.batches_written += 1\n\n        # Convert to batch format and delegate\n        batch_info = {\n            \"batch_index\": self.batches_written,\n            \"batch_size\": len(df),\n            \"timestamp\": pd.Timestamp.now().isoformat(),\n        }\n\n        if metadata:\n            batch_info.update(metadata)\n\n        # Delegate to the actual writer\n        result = self.writer_implementation.write(df, batch_info)\n\n        # Track last written path from underlying writer if available\n        last_path = getattr(self.writer_implementation, \"last_written_path\", None)\n        if last_path:\n            self.written_paths.append(last_path)\n\n        # Update row counter\n        self.total_rows_written += len(df)\n\n        self.logger.debug(f\"Batch write result: {result}\")\n\n        return {\n            \"status\": \"success\",\n            \"rows_written\": len(df),\n            \"batch_index\": self.batches_written,\n            \"metadata\": metadata,\n        }\n\n    def finalize(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Finalize batch writing operations.\n\n        Returns:\n            Dictionary with summary of all write operations\n        \"\"\"\n        self.logger.info(\n            f\"Finalizing batch writer. Total batches: {self.batches_written}, Total rows: {self.total_rows_written}\"\n        )\n\n        # Finalize the actual writer\n        actual_summary = self.writer_implementation.finalize()\n\n        summary = {\n            \"total_rows_written\": self.total_rows_written,\n            \"total_batches_written\": self.batches_written,\n            \"writer_type\": \"batch\",\n            \"writer_implementation_type\": type(self.writer_implementation).__name__,\n            \"writer_implementation_summary\": actual_summary,\n        }\n\n        # If the writer implementation tracked the last path, include it\n        last_path = getattr(self.writer_implementation, \"last_written_path\", None)\n        if last_path:\n            summary[\"last_written_path\"] = last_path\n        if self.written_paths:\n            summary[\"written_paths\"] = list(self.written_paths)\n\n        self.logger.debug(f\"Batch writer summary: {summary}\")\n        return summary\n</code></pre>"},{"location":"reference/writers/batch_writer/#core.writers.batch_writer.BatchWriter.finalize","title":"<code>finalize()</code>","text":"<p>Finalize batch writing operations.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with summary of all write operations</p> Source code in <code>core/writers/batch_writer.py</code> <pre><code>def finalize(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Finalize batch writing operations.\n\n    Returns:\n        Dictionary with summary of all write operations\n    \"\"\"\n    self.logger.info(\n        f\"Finalizing batch writer. Total batches: {self.batches_written}, Total rows: {self.total_rows_written}\"\n    )\n\n    # Finalize the actual writer\n    actual_summary = self.writer_implementation.finalize()\n\n    summary = {\n        \"total_rows_written\": self.total_rows_written,\n        \"total_batches_written\": self.batches_written,\n        \"writer_type\": \"batch\",\n        \"writer_implementation_type\": type(self.writer_implementation).__name__,\n        \"writer_implementation_summary\": actual_summary,\n    }\n\n    # If the writer implementation tracked the last path, include it\n    last_path = getattr(self.writer_implementation, \"last_written_path\", None)\n    if last_path:\n        summary[\"last_written_path\"] = last_path\n    if self.written_paths:\n        summary[\"written_paths\"] = list(self.written_paths)\n\n    self.logger.debug(f\"Batch writer summary: {summary}\")\n    return summary\n</code></pre>"},{"location":"reference/writers/batch_writer/#core.writers.batch_writer.BatchWriter.write","title":"<code>write(df, metadata=None)</code>","text":"<p>Write DataFrame (BaseWriter interface).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to write</p> required <code>metadata</code> <code>dict[str, Any]</code> <p>Optional metadata</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with write operation results</p> Source code in <code>core/writers/batch_writer.py</code> <pre><code>def write(\n    self, df: pd.DataFrame, metadata: dict[str, Any] = None\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Write DataFrame (BaseWriter interface).\n\n    Args:\n        df: DataFrame to write\n        metadata: Optional metadata\n\n    Returns:\n        Dictionary with write operation results\n    \"\"\"\n    # Increment batch counter first (1-based indexing)\n    self.batches_written += 1\n\n    # Convert to batch format and delegate\n    batch_info = {\n        \"batch_index\": self.batches_written,\n        \"batch_size\": len(df),\n        \"timestamp\": pd.Timestamp.now().isoformat(),\n    }\n\n    if metadata:\n        batch_info.update(metadata)\n\n    # Delegate to the actual writer\n    result = self.writer_implementation.write(df, batch_info)\n\n    # Track last written path from underlying writer if available\n    last_path = getattr(self.writer_implementation, \"last_written_path\", None)\n    if last_path:\n        self.written_paths.append(last_path)\n\n    # Update row counter\n    self.total_rows_written += len(df)\n\n    self.logger.debug(f\"Batch write result: {result}\")\n\n    return {\n        \"status\": \"success\",\n        \"rows_written\": len(df),\n        \"batch_index\": self.batches_written,\n        \"metadata\": metadata,\n    }\n</code></pre>"},{"location":"reference/writers/csv_file_writer/","title":"csv_file_writer","text":""},{"location":"reference/writers/csv_file_writer/#core.writers.csv_file_writer","title":"<code>core.writers.csv_file_writer</code>","text":"<p>CSV file writer implementation for GenXData.</p> <p>Refactored to use GenericFileWriter to reduce duplication.</p>"},{"location":"reference/writers/csv_file_writer/#core.writers.csv_file_writer.CsvFileWriter","title":"<code>CsvFileWriter</code>","text":"<p>               Bases: <code>GenericFileWriter</code></p> <p>Writer for CSV file format.</p> <p>Handles writing DataFrames to CSV files with proper defaults and parameter validation.</p> Source code in <code>core/writers/csv_file_writer.py</code> <pre><code>class CsvFileWriter(GenericFileWriter):\n    \"\"\"\n    Writer for CSV file format.\n\n    Handles writing DataFrames to CSV files with proper defaults\n    and parameter validation.\n    \"\"\"\n\n    def __init__(self, config: dict[str, Any]):\n        # Configure the generic writer for CSV format\n        csv_config = {\n            **config,\n            \"writer_kind\": \"csv\",\n            \"pandas_method\": \"to_csv\",\n            \"extensions\": [\".csv\"],\n            \"default_params\": {\n                \"index\": False,  # Don't include DataFrame index by default\n                \"encoding\": \"utf-8\",\n            }\n        }\n        super().__init__(csv_config)\n</code></pre>"},{"location":"reference/writers/excel_file_writer/","title":"excel_file_writer","text":""},{"location":"reference/writers/excel_file_writer/#core.writers.excel_file_writer","title":"<code>core.writers.excel_file_writer</code>","text":"<p>Excel file writer implementation for GenXData.</p> <p>Refactored to use GenericFileWriter to reduce duplication.</p>"},{"location":"reference/writers/excel_file_writer/#core.writers.excel_file_writer.ExcelFileWriter","title":"<code>ExcelFileWriter</code>","text":"<p>               Bases: <code>GenericFileWriter</code></p> <p>Writer for Excel file format.</p> <p>Handles writing DataFrames to Excel files with proper defaults and parameter validation.</p> Source code in <code>core/writers/excel_file_writer.py</code> <pre><code>class ExcelFileWriter(GenericFileWriter):\n    \"\"\"\n    Writer for Excel file format.\n\n    Handles writing DataFrames to Excel files with proper defaults\n    and parameter validation.\n    \"\"\"\n\n    def __init__(self, config: dict[str, Any]):\n        # Configure the generic writer for Excel format\n        excel_config = {\n            **config,\n            \"writer_kind\": \"excel\",\n            \"pandas_method\": \"to_excel\",\n            \"extensions\": [\".xlsx\", \".xls\"],\n            \"default_params\": {\n                \"index\": False,  # Don't include DataFrame index by default\n                \"sheet_name\": \"Sheet1\",  # Default sheet name\n                \"engine\": \"openpyxl\",  # Default engine for xlsx files\n            }\n        }\n        super().__init__(excel_config)\n</code></pre>"},{"location":"reference/writers/feather_file_writer/","title":"feather_file_writer","text":""},{"location":"reference/writers/feather_file_writer/#core.writers.feather_file_writer","title":"<code>core.writers.feather_file_writer</code>","text":"<p>Feather file writer implementation for GenXData.</p> <p>Refactored to use GenericFileWriter to reduce duplication.</p>"},{"location":"reference/writers/feather_file_writer/#core.writers.feather_file_writer.FeatherFileWriter","title":"<code>FeatherFileWriter</code>","text":"<p>               Bases: <code>GenericFileWriter</code></p> <p>Writer for Feather file format.</p> <p>Feather is a fast, lightweight binary columnar format designed for high-performance data interoperability between multiple languages.</p> Source code in <code>core/writers/feather_file_writer.py</code> <pre><code>class FeatherFileWriter(GenericFileWriter):\n    \"\"\"\n    Writer for Feather file format.\n\n    Feather is a fast, lightweight binary columnar format designed for\n    high-performance data interoperability between multiple languages.\n    \"\"\"\n\n    def __init__(self, config: dict[str, Any]):\n        # Configure the generic writer for Feather format\n        feather_config = {\n            **config,\n            \"writer_kind\": \"feather\",\n            \"pandas_method\": \"to_feather\",\n            \"extensions\": [\".feather\"],\n            \"default_params\": {\n                \"compression\": \"zstd\"  # Default compression algorithm\n            }\n        }\n        super().__init__(feather_config)\n</code></pre>"},{"location":"reference/writers/file_writer_factory/","title":"file_writer_factory","text":""},{"location":"reference/writers/file_writer_factory/#core.writers.file_writer_factory","title":"<code>core.writers.file_writer_factory</code>","text":"<p>Factory for creating file writers in GenXData.</p> <p>This factory normalizes writer type strings (case-insensitive, optional \"_WRITER\" suffix) and instantiates concrete <code>BaseFileWriter</code> subclasses.</p> <p>Notes: - Supported types: csv, json, excel/xlsx/xls, parquet, sqlite/db, html/htm, feather - If no parameters are provided, an <code>output_path</code> default is injected   (e.g., \"output.csv\") so that writers can proceed.</p>"},{"location":"reference/writers/file_writer_factory/#core.writers.file_writer_factory.FileWriterFactory","title":"<code>FileWriterFactory</code>","text":"<p>Factory class for creating file writers.</p> <p>Maps writer type strings to their corresponding writer classes and provides helpers for type normalization, support checks, and batch creation.</p> Source code in <code>core/writers/file_writer_factory.py</code> <pre><code>class FileWriterFactory:\n    \"\"\"\n    Factory class for creating file writers.\n\n    Maps writer type strings to their corresponding writer classes and\n    provides helpers for type normalization, support checks, and batch\n    creation.\n    \"\"\"\n\n    # Mapping of writer type strings to writer classes\n    _WRITER_REGISTRY: dict[str, type[BaseFileWriter]] = {\n        \"csv\": CsvFileWriter,\n        \"json\": JsonFileWriter,\n        \"excel\": ExcelFileWriter,\n        \"xlsx\": ExcelFileWriter,  # Alias for Excel\n        \"xls\": ExcelFileWriter,  # Alias for Excel\n        \"parquet\": ParquetFileWriter,\n        \"sqlite\": SqliteFileWriter,\n        \"db\": SqliteFileWriter,  # Alias for SQLite\n        \"html\": HtmlFileWriter,\n        \"htm\": HtmlFileWriter,  # Alias for HTML\n        \"feather\": FeatherFileWriter,\n    }\n\n    def __init__(self):\n        \"\"\"Initialize the factory.\"\"\"\n        self.logger = Logger.get_logger(\"file_writer_factory\")\n\n    @classmethod\n    def get_supported_types(cls) -&gt; list[str]:\n        \"\"\"\n        Get list of supported writer types.\n\n        Returns:\n            list[str]: List of supported writer type strings\n        \"\"\"\n        return list(cls._WRITER_REGISTRY.keys())\n\n    @classmethod\n    def is_supported(cls, writer_type: str) -&gt; bool:\n        \"\"\"\n        Check if a writer type is supported.\n\n        Args:\n            writer_type: Writer type string to check\n\n        Returns:\n            bool: True if the writer type is supported\n        \"\"\"\n        return cls._normalize_type(writer_type) in cls._WRITER_REGISTRY\n\n    @staticmethod\n    def _normalize_type(writer_type: str) -&gt; str:\n        \"\"\"\n        Normalize writer type string.\n\n        Args:\n            writer_type: Original writer type string\n\n        Returns:\n            str: Normalized writer type\n        \"\"\"\n        if not writer_type:\n            raise ValueError(\"Writer type cannot be empty\")\n\n        # Convert to lowercase and remove common suffixes\n        normalized = writer_type.lower().strip()\n\n        # Remove _WRITER suffix if present (for backward compatibility)\n        if normalized.endswith(\"_writer\"):\n            normalized = normalized[:-7]\n\n        return normalized\n\n    def create_writer(self, writer_type: str, params: dict[str, Any]) -&gt; BaseFileWriter:\n        \"\"\"\n        Create a file writer instance.\n\n        Args:\n            writer_type: Type of writer to create (e.g., 'csv', 'json', 'excel')\n            params: Parameters for the writer\n\n        Returns:\n            BaseFileWriter: Writer instance\n\n        Raises:\n            ValueError: If writer type is not supported\n            Exception: If writer creation fails\n        \"\"\"\n        normalized_type = self._normalize_type(writer_type)\n\n        if normalized_type not in self._WRITER_REGISTRY:\n            supported_types = \", \".join(self.get_supported_types())\n            raise ValueError(\n                f\"Unsupported writer type: '{writer_type}'. \"\n                f\"Supported types: {supported_types}\"\n            )\n\n        writer_class = self._WRITER_REGISTRY[normalized_type]\n\n        try:\n            self.logger.debug(f\"Creating {writer_class.__name__} with params: {params}\")\n\n            if not params:\n                params[\"output_path\"] = \"output.csv\"\n\n            writer = writer_class(params)\n            self.logger.info(f\"Successfully created {writer_class.__name__}\")\n            return writer\n\n        except Exception as e:\n            self.logger.error(f\"Failed to create {writer_class.__name__}: {e}\")\n            raise Exception(\n                f\"Failed to create writer for type '{writer_type}': {e}\"\n            ) from e\n\n    @classmethod\n    def register_writer(\n        cls, writer_type: str, writer_class: type[BaseFileWriter]\n    ) -&gt; None:\n        \"\"\"\n        Register a new writer type.\n\n        Args:\n            writer_type: Writer type string\n            writer_class: Writer class that extends BaseFileWriter\n\n        Raises:\n            ValueError: If writer_class doesn't extend BaseFileWriter\n        \"\"\"\n        if not issubclass(writer_class, BaseFileWriter):\n            raise ValueError(\n                f\"Writer class must extend BaseFileWriter, got {writer_class}\"\n            )\n\n        normalized_type = cls._normalize_type(writer_type)\n        cls._WRITER_REGISTRY[normalized_type] = writer_class\n\n    def create_multiple_writers(\n        self, writer_configs: list[dict[str, Any]]\n    ) -&gt; list[BaseFileWriter]:\n        \"\"\"\n        Create multiple writer instances from a list of configurations.\n\n        Args:\n            writer_configs: List of writer configurations, each containing 'type' and 'params'\n\n        Returns:\n            list[BaseFileWriter]: List of writer instances\n\n        Raises:\n            ValueError: If any configuration is invalid\n        \"\"\"\n        writers = []\n\n        for i, config in enumerate(writer_configs):\n            if not isinstance(config, dict):\n                raise ValueError(f\"Writer config {i} must be a dictionary\")\n\n            if \"type\" not in config:\n                raise ValueError(f\"Writer config {i} missing 'type' field\")\n\n            writer_type = config[\"type\"]\n            params = config.get(\"params\", {})\n\n            try:\n                writer = self.create_writer(writer_type, params)\n                writers.append(writer)\n            except Exception as e:\n                self.logger.error(\n                    f\"Failed to create writer {i} (type: {writer_type}): {e}\"\n                )\n                raise\n\n        self.logger.info(f\"Successfully created {len(writers)} writers\")\n        return writers\n</code></pre>"},{"location":"reference/writers/file_writer_factory/#core.writers.file_writer_factory.FileWriterFactory.create_multiple_writers","title":"<code>create_multiple_writers(writer_configs)</code>","text":"<p>Create multiple writer instances from a list of configurations.</p> <p>Parameters:</p> Name Type Description Default <code>writer_configs</code> <code>list[dict[str, Any]]</code> <p>List of writer configurations, each containing 'type' and 'params'</p> required <p>Returns:</p> Type Description <code>list[BaseFileWriter]</code> <p>list[BaseFileWriter]: List of writer instances</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any configuration is invalid</p> Source code in <code>core/writers/file_writer_factory.py</code> <pre><code>def create_multiple_writers(\n    self, writer_configs: list[dict[str, Any]]\n) -&gt; list[BaseFileWriter]:\n    \"\"\"\n    Create multiple writer instances from a list of configurations.\n\n    Args:\n        writer_configs: List of writer configurations, each containing 'type' and 'params'\n\n    Returns:\n        list[BaseFileWriter]: List of writer instances\n\n    Raises:\n        ValueError: If any configuration is invalid\n    \"\"\"\n    writers = []\n\n    for i, config in enumerate(writer_configs):\n        if not isinstance(config, dict):\n            raise ValueError(f\"Writer config {i} must be a dictionary\")\n\n        if \"type\" not in config:\n            raise ValueError(f\"Writer config {i} missing 'type' field\")\n\n        writer_type = config[\"type\"]\n        params = config.get(\"params\", {})\n\n        try:\n            writer = self.create_writer(writer_type, params)\n            writers.append(writer)\n        except Exception as e:\n            self.logger.error(\n                f\"Failed to create writer {i} (type: {writer_type}): {e}\"\n            )\n            raise\n\n    self.logger.info(f\"Successfully created {len(writers)} writers\")\n    return writers\n</code></pre>"},{"location":"reference/writers/file_writer_factory/#core.writers.file_writer_factory.FileWriterFactory.create_writer","title":"<code>create_writer(writer_type, params)</code>","text":"<p>Create a file writer instance.</p> <p>Parameters:</p> Name Type Description Default <code>writer_type</code> <code>str</code> <p>Type of writer to create (e.g., 'csv', 'json', 'excel')</p> required <code>params</code> <code>dict[str, Any]</code> <p>Parameters for the writer</p> required <p>Returns:</p> Name Type Description <code>BaseFileWriter</code> <code>BaseFileWriter</code> <p>Writer instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If writer type is not supported</p> <code>Exception</code> <p>If writer creation fails</p> Source code in <code>core/writers/file_writer_factory.py</code> <pre><code>def create_writer(self, writer_type: str, params: dict[str, Any]) -&gt; BaseFileWriter:\n    \"\"\"\n    Create a file writer instance.\n\n    Args:\n        writer_type: Type of writer to create (e.g., 'csv', 'json', 'excel')\n        params: Parameters for the writer\n\n    Returns:\n        BaseFileWriter: Writer instance\n\n    Raises:\n        ValueError: If writer type is not supported\n        Exception: If writer creation fails\n    \"\"\"\n    normalized_type = self._normalize_type(writer_type)\n\n    if normalized_type not in self._WRITER_REGISTRY:\n        supported_types = \", \".join(self.get_supported_types())\n        raise ValueError(\n            f\"Unsupported writer type: '{writer_type}'. \"\n            f\"Supported types: {supported_types}\"\n        )\n\n    writer_class = self._WRITER_REGISTRY[normalized_type]\n\n    try:\n        self.logger.debug(f\"Creating {writer_class.__name__} with params: {params}\")\n\n        if not params:\n            params[\"output_path\"] = \"output.csv\"\n\n        writer = writer_class(params)\n        self.logger.info(f\"Successfully created {writer_class.__name__}\")\n        return writer\n\n    except Exception as e:\n        self.logger.error(f\"Failed to create {writer_class.__name__}: {e}\")\n        raise Exception(\n            f\"Failed to create writer for type '{writer_type}': {e}\"\n        ) from e\n</code></pre>"},{"location":"reference/writers/file_writer_factory/#core.writers.file_writer_factory.FileWriterFactory.get_supported_types","title":"<code>get_supported_types()</code>  <code>classmethod</code>","text":"<p>Get list of supported writer types.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of supported writer type strings</p> Source code in <code>core/writers/file_writer_factory.py</code> <pre><code>@classmethod\ndef get_supported_types(cls) -&gt; list[str]:\n    \"\"\"\n    Get list of supported writer types.\n\n    Returns:\n        list[str]: List of supported writer type strings\n    \"\"\"\n    return list(cls._WRITER_REGISTRY.keys())\n</code></pre>"},{"location":"reference/writers/file_writer_factory/#core.writers.file_writer_factory.FileWriterFactory.is_supported","title":"<code>is_supported(writer_type)</code>  <code>classmethod</code>","text":"<p>Check if a writer type is supported.</p> <p>Parameters:</p> Name Type Description Default <code>writer_type</code> <code>str</code> <p>Writer type string to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the writer type is supported</p> Source code in <code>core/writers/file_writer_factory.py</code> <pre><code>@classmethod\ndef is_supported(cls, writer_type: str) -&gt; bool:\n    \"\"\"\n    Check if a writer type is supported.\n\n    Args:\n        writer_type: Writer type string to check\n\n    Returns:\n        bool: True if the writer type is supported\n    \"\"\"\n    return cls._normalize_type(writer_type) in cls._WRITER_REGISTRY\n</code></pre>"},{"location":"reference/writers/file_writer_factory/#core.writers.file_writer_factory.FileWriterFactory.register_writer","title":"<code>register_writer(writer_type, writer_class)</code>  <code>classmethod</code>","text":"<p>Register a new writer type.</p> <p>Parameters:</p> Name Type Description Default <code>writer_type</code> <code>str</code> <p>Writer type string</p> required <code>writer_class</code> <code>type[BaseFileWriter]</code> <p>Writer class that extends BaseFileWriter</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If writer_class doesn't extend BaseFileWriter</p> Source code in <code>core/writers/file_writer_factory.py</code> <pre><code>@classmethod\ndef register_writer(\n    cls, writer_type: str, writer_class: type[BaseFileWriter]\n) -&gt; None:\n    \"\"\"\n    Register a new writer type.\n\n    Args:\n        writer_type: Writer type string\n        writer_class: Writer class that extends BaseFileWriter\n\n    Raises:\n        ValueError: If writer_class doesn't extend BaseFileWriter\n    \"\"\"\n    if not issubclass(writer_class, BaseFileWriter):\n        raise ValueError(\n            f\"Writer class must extend BaseFileWriter, got {writer_class}\"\n        )\n\n    normalized_type = cls._normalize_type(writer_type)\n    cls._WRITER_REGISTRY[normalized_type] = writer_class\n</code></pre>"},{"location":"reference/writers/generic_file_writer/","title":"generic_file_writer","text":""},{"location":"reference/writers/generic_file_writer/#core.writers.generic_file_writer","title":"<code>core.writers.generic_file_writer</code>","text":"<p>Generic file writer implementation for GenXData.</p> <p>This module provides a generic file writer that reduces duplication across different file format writers by handling common patterns while allowing format-specific customization.</p>"},{"location":"reference/writers/generic_file_writer/#core.writers.generic_file_writer.GenericFileWriter","title":"<code>GenericFileWriter</code>","text":"<p>               Bases: <code>BaseFileWriter</code></p> <p>Generic file writer that handles common patterns for pandas DataFrame output.</p> <p>This writer reduces duplication by providing a common implementation that can be configured for different file formats through the pandas to_* methods.</p> Usage Source code in <code>core/writers/generic_file_writer.py</code> <pre><code>class GenericFileWriter(BaseFileWriter):\n    \"\"\"\n    Generic file writer that handles common patterns for pandas DataFrame output.\n\n    This writer reduces duplication by providing a common implementation\n    that can be configured for different file formats through the pandas\n    to_* methods.\n\n    Usage:\n        # For simple formats that map directly to pandas methods\n        writer = GenericFileWriter({\n            \"output_path\": \"output.csv\",\n            \"writer_kind\": \"csv\",\n            \"pandas_method\": \"to_csv\",\n            \"extensions\": [\".csv\"],\n            \"default_params\": {\"index\": False, \"encoding\": \"utf-8\"}\n        })\n\n        # For formats requiring custom handling\n        writer = GenericFileWriter({\n            \"output_path\": \"output.html\", \n            \"writer_kind\": \"html\",\n            \"custom_write_func\": custom_html_writer,\n            \"extensions\": [\".html\", \".htm\"]\n        })\n    \"\"\"\n\n    def __init__(self, config: dict[str, Any]):\n        \"\"\"\n        Initialize the generic file writer.\n\n        Args:\n            config: Configuration dictionary containing:\n                - output_path: Output file path\n                - writer_kind: Type identifier for the writer\n                - pandas_method: Name of pandas method to use (e.g., 'to_csv')\n                - extensions: List of valid file extensions\n                - default_params: Default parameters for the pandas method\n                - custom_write_func: Optional custom write function\n                - custom_params_extractor: Optional function to extract custom params\n        \"\"\"\n        # Extract configuration first\n        self.writer_kind = config.get(\"writer_kind\", \"generic\")\n        self.pandas_method = config.get(\"pandas_method\")\n        self.extensions = config.get(\"extensions\", [])\n        self.default_params = config.get(\"default_params\", {})\n        self.custom_write_func = config.get(\"custom_write_func\")\n        self.custom_params_extractor = config.get(\"custom_params_extractor\")\n\n        # Validate configuration\n        if not self.custom_write_func and not self.pandas_method:\n            raise ValueError(\"Either 'pandas_method' or 'custom_write_func' must be specified\")\n\n        if not self.extensions:\n            raise ValueError(\"'extensions' must be specified\")\n\n        # Now call parent constructor\n        super().__init__(config)\n\n    def get_expected_extensions(self) -&gt; list[str]:\n        \"\"\"Get valid file extensions for this writer.\"\"\"\n        return self.extensions\n\n    def get_default_params(self) -&gt; dict[str, Any]:\n        \"\"\"Get default parameters for this writer.\"\"\"\n        return self.default_params.copy()\n\n    def write(self, df: pd.DataFrame, metadata: dict[str, Any] = None) -&gt; dict[str, Any]:\n        \"\"\"\n        Write DataFrame to file using the configured method.\n\n        Args:\n            df: DataFrame to write\n            metadata: Optional metadata (batch info, etc.)\n\n        Returns:\n            dict: Result information\n        \"\"\"\n        try:\n            # Get writer parameters, excluding path-related keys\n            writer_params = self._get_writer_params()\n\n            # Apply defaults for missing parameters\n            defaults = self.get_default_params()\n            for key, value in defaults.items():\n                if key not in writer_params:\n                    writer_params[key] = value\n\n            # Resolve output path with metadata substitution\n            resolved_path = self._resolve_output_path(metadata)\n\n            # Ensure the directory exists for the resolved path\n            output_dir = os.path.dirname(resolved_path)\n            if output_dir and not os.path.exists(output_dir):\n                os.makedirs(output_dir, exist_ok=True)\n\n            # Use custom write function if provided\n            if self.custom_write_func:\n                result = self._write_with_custom_func(df, resolved_path, writer_params, metadata)\n            else:\n                result = self._write_with_pandas_method(df, resolved_path, writer_params, metadata)\n\n            self.last_written_path = resolved_path\n            return result\n\n        except Exception as e:\n            self.logger.error(f\"Error writing DataFrame to {self.writer_kind}: {e}\")\n            return {\"status\": \"error\", \"error\": str(e), \"output_path\": self.output_path}\n\n    def _write_with_pandas_method(\n        self, df: pd.DataFrame, resolved_path: str, writer_params: dict[str, Any], metadata: dict[str, Any] = None\n    ) -&gt; dict[str, Any]:\n        \"\"\"Write using pandas method.\"\"\"\n        # Get the pandas method\n        pandas_write_method = getattr(df, self.pandas_method)\n\n        # Filter out non-pandas parameters\n        meta_keys = {\n            \"writer_kind\",\n            \"extensions\",\n            \"custom_write_func\",\n            \"custom_params_extractor\",\n            \"pandas_method\",\n            \"default_params\",\n        }\n        pandas_params = {k: v for k, v in writer_params.items() if k not in meta_keys}\n\n        # Add path parameter (different methods use different parameter names)\n        if self.pandas_method in [\"to_csv\", \"to_json\"]:\n            pandas_params[\"path_or_buf\"] = resolved_path\n        elif self.pandas_method == \"to_excel\":\n            pandas_params[\"excel_writer\"] = resolved_path\n        elif self.pandas_method in [\"to_parquet\", \"to_feather\"]:\n            pandas_params[\"path\"] = resolved_path\n        else:\n            pandas_params[\"path\"] = resolved_path\n\n        # Write the file\n        self.logger.debug(f\"Writing DataFrame to {self.writer_kind}: {resolved_path}\")\n\n        pandas_write_method(**pandas_params)\n\n        self.logger.info(f\"Successfully wrote {len(df)} rows to {self.writer_kind}: {resolved_path}\")\n\n        return {\n            \"status\": \"success\",\n            \"output_path\": resolved_path,\n            \"rows_written\": len(df),\n            \"file_info\": self.get_file_info(),\n        }\n\n    def _write_with_custom_func(\n        self, df: pd.DataFrame, resolved_path: str, writer_params: dict[str, Any], metadata: dict[str, Any] = None\n    ) -&gt; dict[str, Any]:\n        \"\"\"Write using custom function.\"\"\"\n        self.logger.debug(f\"Writing DataFrame to {self.writer_kind}: {resolved_path}\")\n\n        # Extract custom parameters if extractor is provided\n        if self.custom_params_extractor:\n            custom_params = self.custom_params_extractor(writer_params)\n        else:\n            custom_params = {}\n\n        # Build pandas-compatible params by excluding meta and custom keys\n        meta_keys = {\n            \"writer_kind\",\n            \"extensions\",\n            \"custom_write_func\",\n            \"custom_params_extractor\",\n            \"pandas_method\",\n            \"default_params\",\n        }\n        pandas_params = {k: v for k, v in writer_params.items() if k not in meta_keys and k not in custom_params}\n\n        # Call custom write function\n        result = self.custom_write_func(df, resolved_path, pandas_params, custom_params, metadata)\n\n        self.logger.info(f\"Successfully wrote {len(df)} rows to {self.writer_kind}: {resolved_path}\")\n\n        return {\n            \"status\": \"success\",\n            \"output_path\": resolved_path,\n            \"rows_written\": len(df),\n            \"file_info\": self.get_file_info(),\n            **result  # Include any additional result data from custom function\n        }\n</code></pre>"},{"location":"reference/writers/generic_file_writer/#core.writers.generic_file_writer.GenericFileWriter--for-simple-formats-that-map-directly-to-pandas-methods","title":"For simple formats that map directly to pandas methods","text":"<p>writer = GenericFileWriter({     \"output_path\": \"output.csv\",     \"writer_kind\": \"csv\",     \"pandas_method\": \"to_csv\",     \"extensions\": [\".csv\"],     \"default_params\": {\"index\": False, \"encoding\": \"utf-8\"} })</p>"},{"location":"reference/writers/generic_file_writer/#core.writers.generic_file_writer.GenericFileWriter--for-formats-requiring-custom-handling","title":"For formats requiring custom handling","text":"<p>writer = GenericFileWriter({     \"output_path\": \"output.html\",      \"writer_kind\": \"html\",     \"custom_write_func\": custom_html_writer,     \"extensions\": [\".html\", \".htm\"] })</p>"},{"location":"reference/writers/generic_file_writer/#core.writers.generic_file_writer.GenericFileWriter.get_default_params","title":"<code>get_default_params()</code>","text":"<p>Get default parameters for this writer.</p> Source code in <code>core/writers/generic_file_writer.py</code> <pre><code>def get_default_params(self) -&gt; dict[str, Any]:\n    \"\"\"Get default parameters for this writer.\"\"\"\n    return self.default_params.copy()\n</code></pre>"},{"location":"reference/writers/generic_file_writer/#core.writers.generic_file_writer.GenericFileWriter.get_expected_extensions","title":"<code>get_expected_extensions()</code>","text":"<p>Get valid file extensions for this writer.</p> Source code in <code>core/writers/generic_file_writer.py</code> <pre><code>def get_expected_extensions(self) -&gt; list[str]:\n    \"\"\"Get valid file extensions for this writer.\"\"\"\n    return self.extensions\n</code></pre>"},{"location":"reference/writers/generic_file_writer/#core.writers.generic_file_writer.GenericFileWriter.write","title":"<code>write(df, metadata=None)</code>","text":"<p>Write DataFrame to file using the configured method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to write</p> required <code>metadata</code> <code>dict[str, Any]</code> <p>Optional metadata (batch info, etc.)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Result information</p> Source code in <code>core/writers/generic_file_writer.py</code> <pre><code>def write(self, df: pd.DataFrame, metadata: dict[str, Any] = None) -&gt; dict[str, Any]:\n    \"\"\"\n    Write DataFrame to file using the configured method.\n\n    Args:\n        df: DataFrame to write\n        metadata: Optional metadata (batch info, etc.)\n\n    Returns:\n        dict: Result information\n    \"\"\"\n    try:\n        # Get writer parameters, excluding path-related keys\n        writer_params = self._get_writer_params()\n\n        # Apply defaults for missing parameters\n        defaults = self.get_default_params()\n        for key, value in defaults.items():\n            if key not in writer_params:\n                writer_params[key] = value\n\n        # Resolve output path with metadata substitution\n        resolved_path = self._resolve_output_path(metadata)\n\n        # Ensure the directory exists for the resolved path\n        output_dir = os.path.dirname(resolved_path)\n        if output_dir and not os.path.exists(output_dir):\n            os.makedirs(output_dir, exist_ok=True)\n\n        # Use custom write function if provided\n        if self.custom_write_func:\n            result = self._write_with_custom_func(df, resolved_path, writer_params, metadata)\n        else:\n            result = self._write_with_pandas_method(df, resolved_path, writer_params, metadata)\n\n        self.last_written_path = resolved_path\n        return result\n\n    except Exception as e:\n        self.logger.error(f\"Error writing DataFrame to {self.writer_kind}: {e}\")\n        return {\"status\": \"error\", \"error\": str(e), \"output_path\": self.output_path}\n</code></pre>"},{"location":"reference/writers/html_file_writer/","title":"html_file_writer","text":""},{"location":"reference/writers/html_file_writer/#core.writers.html_file_writer","title":"<code>core.writers.html_file_writer</code>","text":"<p>HTML file writer implementation for GenXData.</p> <p>Refactored to use GenericFileWriter to reduce duplication.</p>"},{"location":"reference/writers/html_file_writer/#core.writers.html_file_writer.HtmlFileWriter","title":"<code>HtmlFileWriter</code>","text":"<p>               Bases: <code>GenericFileWriter</code></p> <p>Writer for HTML file format.</p> <p>Handles writing DataFrames to HTML files with Bootstrap styling and proper defaults.</p> Source code in <code>core/writers/html_file_writer.py</code> <pre><code>class HtmlFileWriter(GenericFileWriter):\n    \"\"\"\n    Writer for HTML file format.\n\n    Handles writing DataFrames to HTML files with Bootstrap styling\n    and proper defaults.\n    \"\"\"\n\n    def __init__(self, config: dict[str, Any]):\n        # Configure the generic writer for HTML format\n        html_config = {\n            **config,\n            \"writer_kind\": \"html\",\n            \"extensions\": [\".html\", \".htm\"],\n            \"default_params\": {\n                \"title\": \"Data Generator Output\",\n                \"classes\": \"table table-striped table-hover\",\n                \"index\": False,\n                \"border\": 0,\n                \"escape\": True,\n                \"na_rep\": \"N/A\",\n                \"include_bootstrap\": True,\n                \"render_links\": True,\n            },\n            \"custom_write_func\": _html_custom_write_func,\n            \"custom_params_extractor\": _html_params_extractor,\n        }\n        super().__init__(html_config)\n</code></pre>"},{"location":"reference/writers/json_file_writer/","title":"json_file_writer","text":""},{"location":"reference/writers/json_file_writer/#core.writers.json_file_writer","title":"<code>core.writers.json_file_writer</code>","text":"<p>JSON file writer implementation for GenXData.</p> <p>Refactored to use GenericFileWriter to reduce duplication.</p>"},{"location":"reference/writers/json_file_writer/#core.writers.json_file_writer.JsonFileWriter","title":"<code>JsonFileWriter</code>","text":"<p>               Bases: <code>GenericFileWriter</code></p> <p>Writer for JSON file format.</p> <p>Handles writing DataFrames to JSON files with proper defaults and parameter validation.</p> Source code in <code>core/writers/json_file_writer.py</code> <pre><code>class JsonFileWriter(GenericFileWriter):\n    \"\"\"\n    Writer for JSON file format.\n\n    Handles writing DataFrames to JSON files with proper defaults\n    and parameter validation.\n    \"\"\"\n\n    def __init__(self, config: dict[str, Any]):\n        # Configure the generic writer for JSON format\n        json_config = {\n            **config,\n            \"writer_kind\": \"json\",\n            \"pandas_method\": \"to_json\",\n            \"extensions\": [\".json\"],\n            \"default_params\": {\n                \"orient\": \"records\",  # Write as array of objects\n                \"date_format\": \"iso\",  # ISO format for dates\n                \"indent\": 2,  # Pretty print JSON\n            }\n        }\n        super().__init__(json_config)\n</code></pre>"},{"location":"reference/writers/parquet_file_writer/","title":"parquet_file_writer","text":""},{"location":"reference/writers/parquet_file_writer/#core.writers.parquet_file_writer","title":"<code>core.writers.parquet_file_writer</code>","text":"<p>Parquet file writer implementation for GenXData.</p> <p>Refactored to use GenericFileWriter to reduce duplication.</p>"},{"location":"reference/writers/parquet_file_writer/#core.writers.parquet_file_writer.ParquetFileWriter","title":"<code>ParquetFileWriter</code>","text":"<p>               Bases: <code>GenericFileWriter</code></p> <p>Writer for Parquet file format.</p> <p>Handles writing DataFrames to Parquet files with proper defaults and parameter validation.</p> Source code in <code>core/writers/parquet_file_writer.py</code> <pre><code>class ParquetFileWriter(GenericFileWriter):\n    \"\"\"\n    Writer for Parquet file format.\n\n    Handles writing DataFrames to Parquet files with proper defaults\n    and parameter validation.\n    \"\"\"\n\n    def __init__(self, config: dict[str, Any]):\n        # Configure the generic writer for Parquet format\n        parquet_config = {\n            **config,\n            \"writer_kind\": \"parquet\",\n            \"pandas_method\": \"to_parquet\",\n            \"extensions\": [\".parquet\"],\n            \"default_params\": {\n                \"compression\": \"snappy\",  # Default compression\n                \"index\": False,  # Don't include DataFrame index by default\n            }\n        }\n        super().__init__(parquet_config)\n</code></pre>"},{"location":"reference/writers/sqlite_file_writer/","title":"sqlite_file_writer","text":""},{"location":"reference/writers/sqlite_file_writer/#core.writers.sqlite_file_writer","title":"<code>core.writers.sqlite_file_writer</code>","text":"<p>SQLite file writer implementation for GenXData.</p> <p>Refactored to use GenericFileWriter to reduce duplication.</p>"},{"location":"reference/writers/sqlite_file_writer/#core.writers.sqlite_file_writer.SqliteFileWriter","title":"<code>SqliteFileWriter</code>","text":"<p>               Bases: <code>GenericFileWriter</code></p> <p>Writer for SQLite database format.</p> <p>Handles writing DataFrames to SQLite database files with proper defaults and parameter validation.</p> Source code in <code>core/writers/sqlite_file_writer.py</code> <pre><code>class SqliteFileWriter(GenericFileWriter):\n    \"\"\"\n    Writer for SQLite database format.\n\n    Handles writing DataFrames to SQLite database files with proper defaults\n    and parameter validation.\n    \"\"\"\n\n    def __init__(self, config: dict[str, Any]):\n        # Configure the generic writer for SQLite format\n        sqlite_config = {\n            **config,\n            \"writer_kind\": \"sqlite\",\n            \"extensions\": [\".db\", \".sqlite\", \".sqlite3\"],\n            \"default_params\": {\n                \"table\": \"data\",  # Default table name\n                \"if_exists\": \"replace\",  # Replace table if it exists\n                \"index\": False,  # Don't include DataFrame index by default\n            },\n            \"custom_write_func\": _sqlite_custom_write_func,\n            \"custom_params_extractor\": _sqlite_params_extractor,\n        }\n        super().__init__(sqlite_config)\n</code></pre>"},{"location":"reference/writers/stream_writer/","title":"stream_writer","text":""},{"location":"reference/writers/stream_writer/#core.writers.stream_writer","title":"<code>core.writers.stream_writer</code>","text":"<p>Stream writer implementation for GenXData.</p> <p>Handles writing DataFrames to message queues (AMQP, Kafka, etc.).</p>"},{"location":"reference/writers/stream_writer/#core.writers.stream_writer.StreamWriter","title":"<code>StreamWriter</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer implementation for streaming/message queue outputs.</p> <p>Uses the messaging module to send data to various message queue systems.</p> Source code in <code>core/writers/stream_writer.py</code> <pre><code>class StreamWriter(BaseWriter):\n    \"\"\"\n    Writer implementation for streaming/message queue outputs.\n\n    Uses the messaging module to send data to various message queue systems.\n    \"\"\"\n\n    def __init__(self, config: dict[str, Any]):\n        \"\"\"\n        Initialize the stream writer.\n\n        Args:\n            config: Stream writer configuration containing queue settings\n        \"\"\"\n        super().__init__(config)\n        self.logger = Logger.get_logger(\"stream_writer\")\n        self.queue_producer = None\n        self.total_rows_written = 0\n        self.total_batches_sent = 0\n        self.queue_meta: dict[str, Any] = {}\n        self.normalized_queue_config: dict[str, Any] | None = None\n\n        # Validate configuration and normalize queue settings\n        self.validate_config()\n        # Initialize queue producer\n        self._initialize_producer()\n\n        self.logger.debug(\"StreamWriter initialized with config\")\n\n    def _extract_queue_section(self) -&gt; tuple[str, dict[str, Any]] | None:\n        \"\"\"Return (queue_type, queue_section) from config supporting nested or flat forms.\"\"\"\n        # Nested form: {\"amqp\": {...}} or {\"kafka\": {...}}\n        for key in (\"amqp\", \"kafka\"):\n            if key in self.config and isinstance(self.config[key], dict):\n                return key, self.config[key]\n\n        # Flat form: {\"type\": \"amqp\", \"host\": ..., \"port\": ..., \"queue\": ...}\n        qtype = self.config.get(\"type\")\n        if isinstance(qtype, str) and qtype.lower() in {\"amqp\", \"kafka\"}:\n            return qtype.lower(), self.config\n        return None\n\n    def validate_config(self) -&gt; bool:\n        \"\"\"\n        Validate stream writer configuration.\n\n        Returns:\n            True if configuration is valid\n\n        Raises:\n            ValueError: If configuration is invalid\n        \"\"\"\n        super().validate_config()\n\n        # Accept nested or flat queue config and normalize\n        extracted = self._extract_queue_section()\n        if not extracted:\n            raise ValueError(\n                \"Stream writer config must include a supported queue section (e.g., nested 'amqp'/'kafka' or flat 'type').\"\n            )\n\n        queue_type, section = extracted\n        # Validate minimal required fields for convenience (messaging also validates)\n        # Accept either a single URL or host+port; 'queue' is always required\n        if \"queue\" not in section:\n            raise ValueError(\n                f\"Stream writer config missing required fields for {queue_type}: queue\"\n            )\n        has_url = \"url\" in section and isinstance(section[\"url\"], str)\n        has_host_port = \"host\" in section and \"port\" in section\n        if not (has_url or has_host_port):\n            raise ValueError(\n                f\"Stream writer config missing required fields for {queue_type}: provide either 'url' or 'host' and 'port'\"\n            )\n\n        # Store normalized meta and config\n        self.queue_meta = {\n            \"queue_type\": queue_type,\n            \"host\": section.get(\"host\") or section.get(\"url\"),\n            \"port\": section.get(\"port\"),\n            \"queue\": section.get(\"queue\"),\n        }\n        self.normalized_queue_config = {queue_type: section}\n\n        return True\n\n    def _initialize_producer(self):\n        \"\"\"Initialize the message queue producer.\"\"\"\n        try:\n            self.logger.debug(\"Initializing queue producer\")\n            cfg = (\n                self.normalized_queue_config\n                if self.normalized_queue_config\n                else self.config\n            )\n            self.queue_producer = QueueFactory.create_from_config(cfg)\n            self.queue_producer.connect()\n            # Attempt to log masked connection info if available\n            try:\n                amqp_conf = getattr(self.queue_producer, \"config\", None)\n                masked = (\n                    amqp_conf.get_connection_url_masked() if amqp_conf else \"(unknown)\"\n                )\n                self.logger.info(f\"Successfully connected to message queue: {masked}\")\n            except Exception:\n                self.logger.info(\"Successfully connected to message queue\")\n        except Exception as e:\n            self.logger.error(f\"Failed to initialize queue producer: {e}\")\n            raise\n\n    def write(\n        self, df: pd.DataFrame, metadata: dict[str, Any] = None\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Write DataFrame to message queue.\n\n        Args:\n            df: DataFrame to write\n            metadata: Optional metadata (batch info, etc.)\n\n        Returns:\n            Dictionary with write operation results\n        \"\"\"\n        if df.empty:\n            self.logger.warning(\"Received empty DataFrame, skipping write\")\n            return {\"status\": \"skipped\", \"reason\": \"empty_dataframe\"}\n\n        if not self.queue_producer:\n            self.logger.error(\"Queue producer not initialized\")\n            return {\"status\": \"error\", \"error\": \"Queue producer not initialized\"}\n\n        self.logger.info(f\"Sending DataFrame with {len(df)} rows to message queue\")\n\n        try:\n            # Prepare batch information\n            batch_info = {\n                \"rows\": len(df),\n                \"columns\": list(df.columns),\n                \"timestamp\": pd.Timestamp.now().isoformat(),\n            }\n\n            # Add metadata if provided\n            if metadata:\n                batch_info.update(metadata)\n\n            # Send DataFrame to queue\n            self.queue_producer.send_dataframe(df, batch_info)\n\n            # Update counters\n            self.total_rows_written += len(df)\n            self.total_batches_sent += 1\n\n            self.logger.info(f\"Successfully sent {len(df)} rows to message queue\")\n\n            return {\n                \"status\": \"success\",\n                \"rows_written\": len(df),\n                \"batch_info\": batch_info,\n                \"metadata\": metadata,\n            }\n\n        except Exception as e:\n            self.logger.error(f\"Error sending DataFrame to message queue: {e}\")\n            return {\"status\": \"error\", \"error\": str(e), \"metadata\": metadata}\n\n    def finalize(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Finalize stream writing operations and cleanup.\n\n        Returns:\n            Dictionary with summary of all write operations\n        \"\"\"\n        self.logger.info(\n            f\"Finalizing stream writer. Total rows sent: {self.total_rows_written}\"\n        )\n\n        # Disconnect from queue\n        if self.queue_producer:\n            try:\n                self.queue_producer.disconnect()\n                self.logger.info(\"Disconnected from message queue\")\n            except Exception as e:\n                self.logger.warning(f\"Error disconnecting from queue: {e}\")\n\n        summary = {\n            \"total_rows_written\": self.total_rows_written,\n            \"total_batches_sent\": self.total_batches_sent,\n            \"queue_type\": self.queue_meta.get(\"queue_type\", \"unknown\"),\n            \"queue_host\": self.queue_meta.get(\"host\", \"unknown\"),\n            \"queue_name\": self.queue_meta.get(\"queue\", \"unknown\"),\n            \"writer_type\": \"stream\",\n        }\n\n        self.logger.debug(f\"Stream writer summary: {summary}\")\n        return summary\n</code></pre>"},{"location":"reference/writers/stream_writer/#core.writers.stream_writer.StreamWriter.finalize","title":"<code>finalize()</code>","text":"<p>Finalize stream writing operations and cleanup.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with summary of all write operations</p> Source code in <code>core/writers/stream_writer.py</code> <pre><code>def finalize(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Finalize stream writing operations and cleanup.\n\n    Returns:\n        Dictionary with summary of all write operations\n    \"\"\"\n    self.logger.info(\n        f\"Finalizing stream writer. Total rows sent: {self.total_rows_written}\"\n    )\n\n    # Disconnect from queue\n    if self.queue_producer:\n        try:\n            self.queue_producer.disconnect()\n            self.logger.info(\"Disconnected from message queue\")\n        except Exception as e:\n            self.logger.warning(f\"Error disconnecting from queue: {e}\")\n\n    summary = {\n        \"total_rows_written\": self.total_rows_written,\n        \"total_batches_sent\": self.total_batches_sent,\n        \"queue_type\": self.queue_meta.get(\"queue_type\", \"unknown\"),\n        \"queue_host\": self.queue_meta.get(\"host\", \"unknown\"),\n        \"queue_name\": self.queue_meta.get(\"queue\", \"unknown\"),\n        \"writer_type\": \"stream\",\n    }\n\n    self.logger.debug(f\"Stream writer summary: {summary}\")\n    return summary\n</code></pre>"},{"location":"reference/writers/stream_writer/#core.writers.stream_writer.StreamWriter.validate_config","title":"<code>validate_config()</code>","text":"<p>Validate stream writer configuration.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if configuration is valid</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration is invalid</p> Source code in <code>core/writers/stream_writer.py</code> <pre><code>def validate_config(self) -&gt; bool:\n    \"\"\"\n    Validate stream writer configuration.\n\n    Returns:\n        True if configuration is valid\n\n    Raises:\n        ValueError: If configuration is invalid\n    \"\"\"\n    super().validate_config()\n\n    # Accept nested or flat queue config and normalize\n    extracted = self._extract_queue_section()\n    if not extracted:\n        raise ValueError(\n            \"Stream writer config must include a supported queue section (e.g., nested 'amqp'/'kafka' or flat 'type').\"\n        )\n\n    queue_type, section = extracted\n    # Validate minimal required fields for convenience (messaging also validates)\n    # Accept either a single URL or host+port; 'queue' is always required\n    if \"queue\" not in section:\n        raise ValueError(\n            f\"Stream writer config missing required fields for {queue_type}: queue\"\n        )\n    has_url = \"url\" in section and isinstance(section[\"url\"], str)\n    has_host_port = \"host\" in section and \"port\" in section\n    if not (has_url or has_host_port):\n        raise ValueError(\n            f\"Stream writer config missing required fields for {queue_type}: provide either 'url' or 'host' and 'port'\"\n        )\n\n    # Store normalized meta and config\n    self.queue_meta = {\n        \"queue_type\": queue_type,\n        \"host\": section.get(\"host\") or section.get(\"url\"),\n        \"port\": section.get(\"port\"),\n        \"queue\": section.get(\"queue\"),\n    }\n    self.normalized_queue_config = {queue_type: section}\n\n    return True\n</code></pre>"},{"location":"reference/writers/stream_writer/#core.writers.stream_writer.StreamWriter.write","title":"<code>write(df, metadata=None)</code>","text":"<p>Write DataFrame to message queue.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to write</p> required <code>metadata</code> <code>dict[str, Any]</code> <p>Optional metadata (batch info, etc.)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with write operation results</p> Source code in <code>core/writers/stream_writer.py</code> <pre><code>def write(\n    self, df: pd.DataFrame, metadata: dict[str, Any] = None\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Write DataFrame to message queue.\n\n    Args:\n        df: DataFrame to write\n        metadata: Optional metadata (batch info, etc.)\n\n    Returns:\n        Dictionary with write operation results\n    \"\"\"\n    if df.empty:\n        self.logger.warning(\"Received empty DataFrame, skipping write\")\n        return {\"status\": \"skipped\", \"reason\": \"empty_dataframe\"}\n\n    if not self.queue_producer:\n        self.logger.error(\"Queue producer not initialized\")\n        return {\"status\": \"error\", \"error\": \"Queue producer not initialized\"}\n\n    self.logger.info(f\"Sending DataFrame with {len(df)} rows to message queue\")\n\n    try:\n        # Prepare batch information\n        batch_info = {\n            \"rows\": len(df),\n            \"columns\": list(df.columns),\n            \"timestamp\": pd.Timestamp.now().isoformat(),\n        }\n\n        # Add metadata if provided\n        if metadata:\n            batch_info.update(metadata)\n\n        # Send DataFrame to queue\n        self.queue_producer.send_dataframe(df, batch_info)\n\n        # Update counters\n        self.total_rows_written += len(df)\n        self.total_batches_sent += 1\n\n        self.logger.info(f\"Successfully sent {len(df)} rows to message queue\")\n\n        return {\n            \"status\": \"success\",\n            \"rows_written\": len(df),\n            \"batch_info\": batch_info,\n            \"metadata\": metadata,\n        }\n\n    except Exception as e:\n        self.logger.error(f\"Error sending DataFrame to message queue: {e}\")\n        return {\"status\": \"error\", \"error\": str(e), \"metadata\": metadata}\n</code></pre>"},{"location":"tutorials/quickstart-batch/","title":"Quickstart (Batch)","text":"<p>This guide runs a batch generation using an example YAML.</p>"},{"location":"tutorials/quickstart-batch/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python and Poetry installed</li> <li>Repository cloned: <code>git clone ... &amp;&amp; cd GenXData</code></li> <li>Dependencies installed:</li> </ul> <pre><code>poetry install\n</code></pre>"},{"location":"tutorials/quickstart-batch/#run-the-batch-example","title":"Run the batch example","text":"<p>Use the provided example config to generate data to <code>output/</code>.</p> <pre><code>poetry run python main.py \\\n  --config examples/batch_configs/batch_example.yaml \\\n  --mode batch\n</code></pre> <p>Validate the output files under <code>output/</code>.</p>"},{"location":"tutorials/quickstart-batch/#next-steps","title":"Next steps","text":"<ul> <li>Explore more examples in <code>examples/</code></li> <li>Review strategies in <code>core/strategies/</code></li> <li>See how to write configs in the How\u2011to guides EOF</li> </ul>"},{"location":"tutorials/quickstart-frontend/","title":"Quickstart (Frontend)","text":"<p>This guide launches the web UI and explains basic usage.</p>"},{"location":"tutorials/quickstart-frontend/#install-and-run","title":"Install and run","text":"<pre><code>cd frontend\nnpm install\nnpm run dev\n</code></pre> <p>Open the displayed local URL and follow the UI to load or author configs, start generation, and view results.</p>"},{"location":"tutorials/quickstart-frontend/#next-steps","title":"Next steps","text":"<ul> <li>See How\u2011to guides to write configs and extend the platform</li> <li>Use Docker/Compose for consistent local dev EOF</li> </ul>"},{"location":"tutorials/quickstart-streaming/","title":"Quickstart (Streaming)","text":"<p>This guide runs GenXData in streaming mode to a message broker.</p>"},{"location":"tutorials/quickstart-streaming/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python and Poetry installed</li> <li>Broker locally (Kafka or AMQP) via Docker Compose</li> </ul> <pre><code>docker compose up -d\n</code></pre>"},{"location":"tutorials/quickstart-streaming/#run-streaming","title":"Run streaming","text":"<pre><code>poetry run python main.py \\\n  --config examples/stream_configs/streaming_example.yaml \\\n  --mode streaming\n</code></pre> <p>Check the target topic/queue with the tools in <code>tools/</code>.</p>"},{"location":"tutorials/quickstart-streaming/#next-steps","title":"Next steps","text":"<ul> <li>See messaging guides for Kafka/AMQP configuration</li> <li>Validate continuity and ordering with the tests and tools provided EOF</li> </ul>"}]}